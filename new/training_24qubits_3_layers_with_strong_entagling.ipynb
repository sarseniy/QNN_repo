{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd6e9a6",
   "metadata": {
    "id": "4fd6e9a6"
   },
   "source": [
    "# Satifsying requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25b8a9e",
   "metadata": {
    "id": "a25b8a9e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc1b4e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dc1b4e5",
    "outputId": "4ba9778d-35b6-457e-d8b4-e9c59969fc68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653bc14",
   "metadata": {
    "id": "e653bc14"
   },
   "source": [
    "# Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33a22a6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467,
     "referenced_widgets": [
      "df69f8986a7b46df869d78996f9a92bf",
      "3c55b6cc69fe4e71bfefdfafe172e1f4",
      "53264b907c324fd2903f6a33df4024eb",
      "fe3e249b5f1947ec9fc496ab71992860",
      "5f557bc2ab954fcdbb3145c5ae1cd1fe",
      "ca79710ed42844eda53edd54ad1cce9e",
      "3629620f44dc4eec928e8285f4e292c7",
      "d8d21603da7146628c7025db753adeb3",
      "f4bb69ed5dcf45cabca0294b04062d68",
      "4b9f9045a6a64724b75bed3bdf4825e5",
      "831588c036b84ba5919552e1c0ffaf95",
      "c7dfb28ce1ec490bbbc71bbf8f8bc7bb",
      "fc29fb7181a64867aa764553a94327cf",
      "4d29ed3d913f4c3dadd8aa28ffdf60ab",
      "739c6da6cd64446686e87fd499fc4e8c",
      "2454c06e04cc44a59af6a54edf771dcf",
      "7598f22b00ca4188abd0e988d87c8717",
      "1fb4ea8439054fbeaab29cbbdd8d8c62",
      "ae7d8308ff8541d082c143732f92bce6",
      "1784bfcf80e54eb1a3b7873cd36494b1",
      "f1cb6810b0f0425c925092fd2c564460",
      "d31e57f44fef4cd99cbda543609a5363",
      "42c3a0dfb96d4d0a9cbcfbefdb88634a",
      "fb3c5d28cc434035936450f03eaab7d4",
      "b7381bd873134277869b9bf86f7f6cde",
      "92a84a7a4c1a49fca9c649540174de1a",
      "973a9057772b4a33bebe1c71b2548322",
      "a2b522fb50d54cecaead8906ecc2c88f",
      "4ee572a2f86844b38f1475756d831d7e",
      "a5767d4185754f1f82e22dc55b9b659e",
      "2bae88a771624fce8a7195ab2d3790e1",
      "656d789dd50d4d7c95121ee911a0315e",
      "bcaf3aab2df24777bb896394049144d2",
      "7192858941cb47649969779dc0e63c91",
      "0de84b8c5dde45b68b6fe495e3978524",
      "664e26cb5e91441d8360e652dc6f3f13",
      "ba90ae57f0bf4101b5306b32401f5ca6",
      "585d33ea2f6c4c6f866ab5eaf500a486",
      "611e4230f7284f56a36e03b6cab128bb",
      "dbeac195f2cf4776b6c95dedda39513c",
      "dc04c3799392479f84746241eac0c9bc",
      "9a0d229daebc428d8d0bea6db980845f",
      "00cd50cc19624ebf946c288dcf96ca77",
      "4472460dcaf349e282e8505097ac56db"
     ]
    },
    "id": "33a22a6f",
    "outputId": "d99650b6-68ff-4c1e-bf7c-3ac0dc41c7c3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True,            \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce5a95",
   "metadata": {
    "id": "09ce5a95"
   },
   "source": [
    "# Preparing data with DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cd1531f",
   "metadata": {
    "id": "0cd1531f"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_data, \n",
    "                                          batch_size=80, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1,\n",
    "                                          pin_memory=True),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size=80, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1,\n",
    "                                          pin_memory=True,\n",
    "                                          drop_last=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dada402",
   "metadata": {
    "id": "3dada402"
   },
   "source": [
    "# Defining a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "121f2af3",
   "metadata": {
    "id": "121f2af3"
   },
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def qnode(inputs, weights):\n",
    "    qml.templates.AngleEmbedding(features=inputs, wires=range(n_qubits))\n",
    "    \n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    \n",
    "    return [qml.expval(qml.PauliY(wires=i)) for i in range(n_qubits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e8604a",
   "metadata": {
    "id": "f8e8604a"
   },
   "outputs": [],
   "source": [
    "n_layers = 4\n",
    "weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67b30b7",
   "metadata": {
    "id": "a67b30b7"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class HybridNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=16,              \n",
    "                out_channels=32,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,    \n",
    "            ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),                \n",
    "        )\n",
    "        self.fc_1 = nn.Sequential(\n",
    "            nn.Linear(32 * 7 * 7, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # LIST USAGE?\n",
    "        self.qlayer_1 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        self.qlayer_2 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        self.qlayer_3 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        self.qlayer_4 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        \n",
    "        self.qlayer_1.to(device)\n",
    "        self.qlayer_2.to(device)\n",
    "        self.qlayer_3.to(device)\n",
    "        self.qlayer_4.to(device)\n",
    "        \n",
    "        self.after_q = nn.Sequential(\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc_2 = nn.Linear(16, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1) \n",
    "\n",
    "        x = self.fc_1(x)\n",
    "        #print('Before split')\n",
    "        x_1, x_2, x_3, x_4 = torch.split(x, 4, dim=1) # second argument is number of elements in one new tensor\n",
    "        #print('After split')\n",
    "        #x = torch.Tensor(0)\n",
    "        \n",
    "        x_1 = self.qlayer_1(x_1)\n",
    "        x_2 = self.qlayer_2(x_2)\n",
    "        x_3 = self.qlayer_3(x_3)\n",
    "        x_4 = self.qlayer_4(x_4)\n",
    "        \n",
    "        #print(x.device)\n",
    "        \n",
    "        x = torch.cat([x_1, x_2, x_3, x_4], axis=1)\n",
    "        x = x.to(device)\n",
    "        x = self.after_q(x)\n",
    "        logits = self.fc_2(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db3df4eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db3df4eb",
    "outputId": "9eddce9b-8bf3-4fb1-8d8f-12fbb55c6b1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_1): Sequential(\n",
      "    (0): Linear(in_features=1568, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (qlayer_1): <Quantum Torch Layer: func=qnode>\n",
      "  (qlayer_2): <Quantum Torch Layer: func=qnode>\n",
      "  (qlayer_3): <Quantum Torch Layer: func=qnode>\n",
      "  (qlayer_4): <Quantum Torch Layer: func=qnode>\n",
      "  (after_q): Sequential(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc_2): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hnn = HybridNN()\n",
    "hnn = hnn.to(device)\n",
    "print(hnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809cbe6",
   "metadata": {
    "id": "6809cbe6"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a415a96",
   "metadata": {
    "id": "1a415a96"
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df9aafa9",
   "metadata": {
    "id": "df9aafa9"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.AdamW(hnn.parameters(), lr = 0.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cc5c6d9",
   "metadata": {
    "id": "7cc5c6d9"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def train(num_epochs, model, loaders):\n",
    "    \n",
    "    model.train()\n",
    "    history = []\n",
    "    \n",
    "    avg_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "        \n",
    "    for epoch in trange(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            b_x, b_y = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()           \n",
    "            \n",
    "            output = model(b_x)             \n",
    "            loss = loss_func(output, b_y)\n",
    "            \n",
    "            loss.backward()               \n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(loaders['train'])\n",
    "            \n",
    "            if (i+1) % 10 >= 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "        \n",
    "        model.eval()        \n",
    "        for X_batch, Y_batch in loaders['test']:\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(X_batch)\n",
    "                loss = loss_func(outputs, Y_batch)\n",
    "                val_loss += loss.item() / len(loaders['test'])\n",
    "            \n",
    "        history.append((avg_loss, val_loss))\n",
    "        print('Epoch [{}/{}], Tr. loss: {:.4f}. Test loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, avg_loss, val_loss))\n",
    "        print('\\n')\n",
    "        torch.save(hnn.state_dict(), 'trained_model_24qubits_3_layers_with_strong_entagling_' + str(i+1) + 'ep.pt')\n",
    "        with open('data_' + str(i+1) + 'ep.txt', 'w') as outfile:\n",
    "            json.dump(history, outfile)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20f9ed6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "a2db74a18ce8412399aba11b3bab930b",
      "9002f4b1af6d4c03b8f53fecdffaa3b0",
      "aa56b1689edb4a75aa4af965407467e6",
      "8b99aefdd9a247678de2cefd312207bb",
      "0a0c3aa1494d454b85ce6083f2c98234",
      "0aa6d7245fc747b58457668a840cd66b",
      "df312d8125694913844f2856ee295e67",
      "70629c93466c490cb4d10c7ba2e41d62",
      "7081d0216eb6473fae87f7331becfb53",
      "979724284c094071b6c3143666a2a8d5",
      "7ca85bfb648a4586ab15f601f45bd7c2"
     ]
    },
    "id": "20f9ed6c",
    "outputId": "3b1078fe-8904-4227-b481-0d9b43a61f33",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db17be91dcb04e79ac1d18b566da0d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OLEG\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:147: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  ..\\aten\\src\\ATen\\native\\Copy.cpp:240.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Step [1/750], Loss: 2.3979\n",
      "Epoch [1/8], Step [2/750], Loss: 2.2228\n",
      "Epoch [1/8], Step [3/750], Loss: 1.9520\n",
      "Epoch [1/8], Step [4/750], Loss: 1.9574\n",
      "Epoch [1/8], Step [5/750], Loss: 1.8181\n",
      "Epoch [1/8], Step [6/750], Loss: 1.8627\n",
      "Epoch [1/8], Step [7/750], Loss: 1.7431\n",
      "Epoch [1/8], Step [8/750], Loss: 1.7685\n",
      "Epoch [1/8], Step [9/750], Loss: 1.7052\n",
      "Epoch [1/8], Step [10/750], Loss: 1.6744\n",
      "Epoch [1/8], Step [11/750], Loss: 1.5159\n",
      "Epoch [1/8], Step [12/750], Loss: 1.4878\n",
      "Epoch [1/8], Step [13/750], Loss: 1.4814\n",
      "Epoch [1/8], Step [14/750], Loss: 1.4064\n",
      "Epoch [1/8], Step [15/750], Loss: 1.3037\n",
      "Epoch [1/8], Step [16/750], Loss: 1.3518\n",
      "Epoch [1/8], Step [17/750], Loss: 1.1915\n",
      "Epoch [1/8], Step [18/750], Loss: 1.0946\n",
      "Epoch [1/8], Step [19/750], Loss: 1.3056\n",
      "Epoch [1/8], Step [20/750], Loss: 1.0981\n",
      "Epoch [1/8], Step [21/750], Loss: 0.9593\n",
      "Epoch [1/8], Step [22/750], Loss: 0.9579\n",
      "Epoch [1/8], Step [23/750], Loss: 0.9217\n",
      "Epoch [1/8], Step [24/750], Loss: 0.9114\n",
      "Epoch [1/8], Step [25/750], Loss: 0.8694\n",
      "Epoch [1/8], Step [26/750], Loss: 0.7786\n",
      "Epoch [1/8], Step [27/750], Loss: 0.8603\n",
      "Epoch [1/8], Step [28/750], Loss: 0.7813\n",
      "Epoch [1/8], Step [29/750], Loss: 0.7067\n",
      "Epoch [1/8], Step [30/750], Loss: 0.7212\n",
      "Epoch [1/8], Step [31/750], Loss: 0.7054\n",
      "Epoch [1/8], Step [32/750], Loss: 0.5865\n",
      "Epoch [1/8], Step [33/750], Loss: 0.5697\n",
      "Epoch [1/8], Step [34/750], Loss: 0.5641\n",
      "Epoch [1/8], Step [35/750], Loss: 0.5451\n",
      "Epoch [1/8], Step [36/750], Loss: 0.3545\n",
      "Epoch [1/8], Step [37/750], Loss: 0.4964\n",
      "Epoch [1/8], Step [38/750], Loss: 0.4906\n",
      "Epoch [1/8], Step [39/750], Loss: 0.4697\n",
      "Epoch [1/8], Step [40/750], Loss: 0.5433\n",
      "Epoch [1/8], Step [41/750], Loss: 0.3698\n",
      "Epoch [1/8], Step [42/750], Loss: 0.4169\n",
      "Epoch [1/8], Step [43/750], Loss: 0.3788\n",
      "Epoch [1/8], Step [44/750], Loss: 0.4558\n",
      "Epoch [1/8], Step [45/750], Loss: 0.3541\n",
      "Epoch [1/8], Step [46/750], Loss: 0.3111\n",
      "Epoch [1/8], Step [47/750], Loss: 0.3003\n",
      "Epoch [1/8], Step [48/750], Loss: 0.2303\n",
      "Epoch [1/8], Step [49/750], Loss: 0.2298\n",
      "Epoch [1/8], Step [50/750], Loss: 0.2690\n",
      "Epoch [1/8], Step [51/750], Loss: 0.1903\n",
      "Epoch [1/8], Step [52/750], Loss: 0.2692\n",
      "Epoch [1/8], Step [53/750], Loss: 0.3697\n",
      "Epoch [1/8], Step [54/750], Loss: 0.3605\n",
      "Epoch [1/8], Step [55/750], Loss: 0.2192\n",
      "Epoch [1/8], Step [56/750], Loss: 0.2636\n",
      "Epoch [1/8], Step [57/750], Loss: 0.1490\n",
      "Epoch [1/8], Step [58/750], Loss: 0.2303\n",
      "Epoch [1/8], Step [59/750], Loss: 0.2515\n",
      "Epoch [1/8], Step [60/750], Loss: 0.1585\n",
      "Epoch [1/8], Step [61/750], Loss: 0.1405\n",
      "Epoch [1/8], Step [62/750], Loss: 0.1970\n",
      "Epoch [1/8], Step [63/750], Loss: 0.4177\n",
      "Epoch [1/8], Step [64/750], Loss: 0.2038\n",
      "Epoch [1/8], Step [65/750], Loss: 0.2162\n",
      "Epoch [1/8], Step [66/750], Loss: 0.1783\n",
      "Epoch [1/8], Step [67/750], Loss: 0.2098\n",
      "Epoch [1/8], Step [68/750], Loss: 0.2466\n",
      "Epoch [1/8], Step [69/750], Loss: 0.1804\n",
      "Epoch [1/8], Step [70/750], Loss: 0.1233\n",
      "Epoch [1/8], Step [71/750], Loss: 0.1050\n",
      "Epoch [1/8], Step [72/750], Loss: 0.1537\n",
      "Epoch [1/8], Step [73/750], Loss: 0.2478\n",
      "Epoch [1/8], Step [74/750], Loss: 0.1257\n",
      "Epoch [1/8], Step [75/750], Loss: 0.0879\n",
      "Epoch [1/8], Step [76/750], Loss: 0.2237\n",
      "Epoch [1/8], Step [77/750], Loss: 0.1895\n",
      "Epoch [1/8], Step [78/750], Loss: 0.1759\n",
      "Epoch [1/8], Step [79/750], Loss: 0.3550\n",
      "Epoch [1/8], Step [80/750], Loss: 0.2184\n",
      "Epoch [1/8], Step [81/750], Loss: 0.1981\n",
      "Epoch [1/8], Step [82/750], Loss: 0.1590\n",
      "Epoch [1/8], Step [83/750], Loss: 0.1197\n",
      "Epoch [1/8], Step [84/750], Loss: 0.1978\n",
      "Epoch [1/8], Step [85/750], Loss: 0.2200\n",
      "Epoch [1/8], Step [86/750], Loss: 0.3004\n",
      "Epoch [1/8], Step [87/750], Loss: 0.3055\n",
      "Epoch [1/8], Step [88/750], Loss: 0.1721\n",
      "Epoch [1/8], Step [89/750], Loss: 0.2901\n",
      "Epoch [1/8], Step [90/750], Loss: 0.1848\n",
      "Epoch [1/8], Step [91/750], Loss: 0.2261\n",
      "Epoch [1/8], Step [92/750], Loss: 0.2072\n",
      "Epoch [1/8], Step [93/750], Loss: 0.2764\n",
      "Epoch [1/8], Step [94/750], Loss: 0.1472\n",
      "Epoch [1/8], Step [95/750], Loss: 0.1246\n",
      "Epoch [1/8], Step [96/750], Loss: 0.1122\n",
      "Epoch [1/8], Step [97/750], Loss: 0.2170\n",
      "Epoch [1/8], Step [98/750], Loss: 0.1929\n",
      "Epoch [1/8], Step [99/750], Loss: 0.1844\n",
      "Epoch [1/8], Step [100/750], Loss: 0.2362\n",
      "Epoch [1/8], Step [101/750], Loss: 0.1903\n",
      "Epoch [1/8], Step [102/750], Loss: 0.1843\n",
      "Epoch [1/8], Step [103/750], Loss: 0.1527\n",
      "Epoch [1/8], Step [104/750], Loss: 0.1437\n",
      "Epoch [1/8], Step [105/750], Loss: 0.3567\n",
      "Epoch [1/8], Step [106/750], Loss: 0.1842\n",
      "Epoch [1/8], Step [107/750], Loss: 0.1843\n",
      "Epoch [1/8], Step [108/750], Loss: 0.2267\n",
      "Epoch [1/8], Step [109/750], Loss: 0.1863\n",
      "Epoch [1/8], Step [110/750], Loss: 0.1685\n",
      "Epoch [1/8], Step [111/750], Loss: 0.1221\n",
      "Epoch [1/8], Step [112/750], Loss: 0.2237\n",
      "Epoch [1/8], Step [113/750], Loss: 0.3921\n",
      "Epoch [1/8], Step [114/750], Loss: 0.1237\n",
      "Epoch [1/8], Step [115/750], Loss: 0.4034\n",
      "Epoch [1/8], Step [116/750], Loss: 0.1540\n",
      "Epoch [1/8], Step [117/750], Loss: 0.1871\n",
      "Epoch [1/8], Step [118/750], Loss: 0.2159\n",
      "Epoch [1/8], Step [119/750], Loss: 0.1627\n",
      "Epoch [1/8], Step [120/750], Loss: 0.2841\n",
      "Epoch [1/8], Step [121/750], Loss: 0.2488\n",
      "Epoch [1/8], Step [122/750], Loss: 0.0993\n",
      "Epoch [1/8], Step [123/750], Loss: 0.1776\n",
      "Epoch [1/8], Step [124/750], Loss: 0.2469\n",
      "Epoch [1/8], Step [125/750], Loss: 0.1972\n",
      "Epoch [1/8], Step [126/750], Loss: 0.1090\n",
      "Epoch [1/8], Step [127/750], Loss: 0.1061\n",
      "Epoch [1/8], Step [128/750], Loss: 0.1382\n",
      "Epoch [1/8], Step [129/750], Loss: 0.2365\n",
      "Epoch [1/8], Step [130/750], Loss: 0.1542\n",
      "Epoch [1/8], Step [131/750], Loss: 0.1886\n",
      "Epoch [1/8], Step [132/750], Loss: 0.1335\n",
      "Epoch [1/8], Step [133/750], Loss: 0.1143\n",
      "Epoch [1/8], Step [134/750], Loss: 0.1212\n",
      "Epoch [1/8], Step [135/750], Loss: 0.1952\n",
      "Epoch [1/8], Step [136/750], Loss: 0.1552\n",
      "Epoch [1/8], Step [137/750], Loss: 0.0837\n",
      "Epoch [1/8], Step [138/750], Loss: 0.1576\n",
      "Epoch [1/8], Step [139/750], Loss: 0.0963\n",
      "Epoch [1/8], Step [140/750], Loss: 0.0828\n",
      "Epoch [1/8], Step [141/750], Loss: 0.0821\n",
      "Epoch [1/8], Step [142/750], Loss: 0.0829\n",
      "Epoch [1/8], Step [143/750], Loss: 0.1380\n",
      "Epoch [1/8], Step [144/750], Loss: 0.1167\n",
      "Epoch [1/8], Step [145/750], Loss: 0.1542\n",
      "Epoch [1/8], Step [146/750], Loss: 0.0865\n",
      "Epoch [1/8], Step [147/750], Loss: 0.1382\n",
      "Epoch [1/8], Step [148/750], Loss: 0.1915\n",
      "Epoch [1/8], Step [149/750], Loss: 0.0278\n",
      "Epoch [1/8], Step [150/750], Loss: 0.2134\n",
      "Epoch [1/8], Step [151/750], Loss: 0.1738\n",
      "Epoch [1/8], Step [152/750], Loss: 0.2480\n",
      "Epoch [1/8], Step [153/750], Loss: 0.1467\n",
      "Epoch [1/8], Step [154/750], Loss: 0.0797\n",
      "Epoch [1/8], Step [155/750], Loss: 0.1304\n",
      "Epoch [1/8], Step [156/750], Loss: 0.0528\n",
      "Epoch [1/8], Step [157/750], Loss: 0.0541\n",
      "Epoch [1/8], Step [158/750], Loss: 0.1406\n",
      "Epoch [1/8], Step [159/750], Loss: 0.1862\n",
      "Epoch [1/8], Step [160/750], Loss: 0.1353\n",
      "Epoch [1/8], Step [161/750], Loss: 0.1072\n",
      "Epoch [1/8], Step [162/750], Loss: 0.0734\n",
      "Epoch [1/8], Step [163/750], Loss: 0.1161\n",
      "Epoch [1/8], Step [164/750], Loss: 0.2627\n",
      "Epoch [1/8], Step [165/750], Loss: 0.0988\n",
      "Epoch [1/8], Step [166/750], Loss: 0.1003\n",
      "Epoch [1/8], Step [167/750], Loss: 0.2008\n",
      "Epoch [1/8], Step [168/750], Loss: 0.0878\n",
      "Epoch [1/8], Step [169/750], Loss: 0.0987\n",
      "Epoch [1/8], Step [170/750], Loss: 0.1188\n",
      "Epoch [1/8], Step [171/750], Loss: 0.0975\n",
      "Epoch [1/8], Step [172/750], Loss: 0.1107\n",
      "Epoch [1/8], Step [173/750], Loss: 0.1109\n",
      "Epoch [1/8], Step [174/750], Loss: 0.1349\n",
      "Epoch [1/8], Step [175/750], Loss: 0.1162\n",
      "Epoch [1/8], Step [176/750], Loss: 0.1009\n",
      "Epoch [1/8], Step [177/750], Loss: 0.0659\n",
      "Epoch [1/8], Step [178/750], Loss: 0.1864\n",
      "Epoch [1/8], Step [179/750], Loss: 0.0601\n",
      "Epoch [1/8], Step [180/750], Loss: 0.0824\n",
      "Epoch [1/8], Step [181/750], Loss: 0.0555\n",
      "Epoch [1/8], Step [182/750], Loss: 0.1778\n",
      "Epoch [1/8], Step [183/750], Loss: 0.0298\n",
      "Epoch [1/8], Step [184/750], Loss: 0.0655\n",
      "Epoch [1/8], Step [185/750], Loss: 0.0695\n",
      "Epoch [1/8], Step [186/750], Loss: 0.1632\n",
      "Epoch [1/8], Step [187/750], Loss: 0.0536\n",
      "Epoch [1/8], Step [188/750], Loss: 0.0890\n",
      "Epoch [1/8], Step [189/750], Loss: 0.1414\n",
      "Epoch [1/8], Step [190/750], Loss: 0.0549\n",
      "Epoch [1/8], Step [191/750], Loss: 0.0960\n",
      "Epoch [1/8], Step [192/750], Loss: 0.2994\n",
      "Epoch [1/8], Step [193/750], Loss: 0.0448\n",
      "Epoch [1/8], Step [194/750], Loss: 0.0414\n",
      "Epoch [1/8], Step [195/750], Loss: 0.1597\n",
      "Epoch [1/8], Step [196/750], Loss: 0.0788\n",
      "Epoch [1/8], Step [197/750], Loss: 0.0168\n",
      "Epoch [1/8], Step [198/750], Loss: 0.0415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Step [199/750], Loss: 0.1086\n",
      "Epoch [1/8], Step [200/750], Loss: 0.0826\n",
      "Epoch [1/8], Step [201/750], Loss: 0.0814\n",
      "Epoch [1/8], Step [202/750], Loss: 0.1621\n",
      "Epoch [1/8], Step [203/750], Loss: 0.1900\n",
      "Epoch [1/8], Step [204/750], Loss: 0.0765\n",
      "Epoch [1/8], Step [205/750], Loss: 0.1721\n",
      "Epoch [1/8], Step [206/750], Loss: 0.0381\n",
      "Epoch [1/8], Step [207/750], Loss: 0.1520\n",
      "Epoch [1/8], Step [208/750], Loss: 0.1146\n",
      "Epoch [1/8], Step [209/750], Loss: 0.0724\n",
      "Epoch [1/8], Step [210/750], Loss: 0.0965\n",
      "Epoch [1/8], Step [211/750], Loss: 0.2166\n",
      "Epoch [1/8], Step [212/750], Loss: 0.0263\n",
      "Epoch [1/8], Step [213/750], Loss: 0.1330\n",
      "Epoch [1/8], Step [214/750], Loss: 0.1819\n",
      "Epoch [1/8], Step [215/750], Loss: 0.0830\n",
      "Epoch [1/8], Step [216/750], Loss: 0.1117\n",
      "Epoch [1/8], Step [217/750], Loss: 0.0384\n",
      "Epoch [1/8], Step [218/750], Loss: 0.2119\n",
      "Epoch [1/8], Step [219/750], Loss: 0.0760\n",
      "Epoch [1/8], Step [220/750], Loss: 0.0402\n",
      "Epoch [1/8], Step [221/750], Loss: 0.0845\n",
      "Epoch [1/8], Step [222/750], Loss: 0.0379\n",
      "Epoch [1/8], Step [223/750], Loss: 0.0977\n",
      "Epoch [1/8], Step [224/750], Loss: 0.1663\n",
      "Epoch [1/8], Step [225/750], Loss: 0.0602\n",
      "Epoch [1/8], Step [226/750], Loss: 0.1508\n",
      "Epoch [1/8], Step [227/750], Loss: 0.1592\n",
      "Epoch [1/8], Step [228/750], Loss: 0.0550\n",
      "Epoch [1/8], Step [229/750], Loss: 0.2196\n",
      "Epoch [1/8], Step [230/750], Loss: 0.1422\n",
      "Epoch [1/8], Step [231/750], Loss: 0.0674\n",
      "Epoch [1/8], Step [232/750], Loss: 0.1921\n",
      "Epoch [1/8], Step [233/750], Loss: 0.0440\n",
      "Epoch [1/8], Step [234/750], Loss: 0.1390\n",
      "Epoch [1/8], Step [235/750], Loss: 0.1736\n",
      "Epoch [1/8], Step [236/750], Loss: 0.0996\n",
      "Epoch [1/8], Step [237/750], Loss: 0.0427\n",
      "Epoch [1/8], Step [238/750], Loss: 0.1169\n",
      "Epoch [1/8], Step [239/750], Loss: 0.0642\n",
      "Epoch [1/8], Step [240/750], Loss: 0.1092\n",
      "Epoch [1/8], Step [241/750], Loss: 0.0664\n",
      "Epoch [1/8], Step [242/750], Loss: 0.0477\n",
      "Epoch [1/8], Step [243/750], Loss: 0.0457\n",
      "Epoch [1/8], Step [244/750], Loss: 0.1359\n",
      "Epoch [1/8], Step [245/750], Loss: 0.0932\n",
      "Epoch [1/8], Step [246/750], Loss: 0.0472\n",
      "Epoch [1/8], Step [247/750], Loss: 0.1549\n",
      "Epoch [1/8], Step [248/750], Loss: 0.0685\n",
      "Epoch [1/8], Step [249/750], Loss: 0.1588\n",
      "Epoch [1/8], Step [250/750], Loss: 0.0820\n",
      "Epoch [1/8], Step [251/750], Loss: 0.0589\n",
      "Epoch [1/8], Step [252/750], Loss: 0.0310\n",
      "Epoch [1/8], Step [253/750], Loss: 0.1572\n",
      "Epoch [1/8], Step [254/750], Loss: 0.3041\n",
      "Epoch [1/8], Step [255/750], Loss: 0.0788\n",
      "Epoch [1/8], Step [256/750], Loss: 0.0941\n",
      "Epoch [1/8], Step [257/750], Loss: 0.1332\n",
      "Epoch [1/8], Step [258/750], Loss: 0.0553\n",
      "Epoch [1/8], Step [259/750], Loss: 0.0314\n",
      "Epoch [1/8], Step [260/750], Loss: 0.1453\n",
      "Epoch [1/8], Step [261/750], Loss: 0.1016\n",
      "Epoch [1/8], Step [262/750], Loss: 0.1132\n",
      "Epoch [1/8], Step [263/750], Loss: 0.1181\n",
      "Epoch [1/8], Step [264/750], Loss: 0.1279\n",
      "Epoch [1/8], Step [265/750], Loss: 0.1681\n",
      "Epoch [1/8], Step [266/750], Loss: 0.0897\n",
      "Epoch [1/8], Step [267/750], Loss: 0.1106\n",
      "Epoch [1/8], Step [268/750], Loss: 0.1256\n",
      "Epoch [1/8], Step [269/750], Loss: 0.1003\n",
      "Epoch [1/8], Step [270/750], Loss: 0.1920\n",
      "Epoch [1/8], Step [271/750], Loss: 0.1111\n",
      "Epoch [1/8], Step [272/750], Loss: 0.1232\n",
      "Epoch [1/8], Step [273/750], Loss: 0.0829\n",
      "Epoch [1/8], Step [274/750], Loss: 0.2492\n",
      "Epoch [1/8], Step [275/750], Loss: 0.0933\n",
      "Epoch [1/8], Step [276/750], Loss: 0.0592\n",
      "Epoch [1/8], Step [277/750], Loss: 0.0803\n",
      "Epoch [1/8], Step [278/750], Loss: 0.1373\n",
      "Epoch [1/8], Step [279/750], Loss: 0.1320\n",
      "Epoch [1/8], Step [280/750], Loss: 0.1036\n",
      "Epoch [1/8], Step [281/750], Loss: 0.0640\n",
      "Epoch [1/8], Step [282/750], Loss: 0.0553\n",
      "Epoch [1/8], Step [283/750], Loss: 0.0796\n",
      "Epoch [1/8], Step [284/750], Loss: 0.0727\n",
      "Epoch [1/8], Step [285/750], Loss: 0.0541\n",
      "Epoch [1/8], Step [286/750], Loss: 0.1717\n",
      "Epoch [1/8], Step [287/750], Loss: 0.1264\n",
      "Epoch [1/8], Step [288/750], Loss: 0.0835\n",
      "Epoch [1/8], Step [289/750], Loss: 0.1106\n",
      "Epoch [1/8], Step [290/750], Loss: 0.0629\n",
      "Epoch [1/8], Step [291/750], Loss: 0.0728\n",
      "Epoch [1/8], Step [292/750], Loss: 0.0425\n",
      "Epoch [1/8], Step [293/750], Loss: 0.1505\n",
      "Epoch [1/8], Step [294/750], Loss: 0.0380\n",
      "Epoch [1/8], Step [295/750], Loss: 0.0649\n",
      "Epoch [1/8], Step [296/750], Loss: 0.0889\n",
      "Epoch [1/8], Step [297/750], Loss: 0.1334\n",
      "Epoch [1/8], Step [298/750], Loss: 0.1358\n",
      "Epoch [1/8], Step [299/750], Loss: 0.0588\n",
      "Epoch [1/8], Step [300/750], Loss: 0.0328\n",
      "Epoch [1/8], Step [301/750], Loss: 0.0865\n",
      "Epoch [1/8], Step [302/750], Loss: 0.0632\n",
      "Epoch [1/8], Step [303/750], Loss: 0.1220\n",
      "Epoch [1/8], Step [304/750], Loss: 0.1153\n",
      "Epoch [1/8], Step [305/750], Loss: 0.0741\n",
      "Epoch [1/8], Step [306/750], Loss: 0.1017\n",
      "Epoch [1/8], Step [307/750], Loss: 0.1173\n",
      "Epoch [1/8], Step [308/750], Loss: 0.0986\n",
      "Epoch [1/8], Step [309/750], Loss: 0.0303\n",
      "Epoch [1/8], Step [310/750], Loss: 0.0948\n",
      "Epoch [1/8], Step [311/750], Loss: 0.0491\n",
      "Epoch [1/8], Step [312/750], Loss: 0.1730\n",
      "Epoch [1/8], Step [313/750], Loss: 0.0527\n",
      "Epoch [1/8], Step [314/750], Loss: 0.1374\n",
      "Epoch [1/8], Step [315/750], Loss: 0.0765\n",
      "Epoch [1/8], Step [316/750], Loss: 0.0824\n",
      "Epoch [1/8], Step [317/750], Loss: 0.0607\n",
      "Epoch [1/8], Step [318/750], Loss: 0.0460\n",
      "Epoch [1/8], Step [319/750], Loss: 0.1466\n",
      "Epoch [1/8], Step [320/750], Loss: 0.0306\n",
      "Epoch [1/8], Step [321/750], Loss: 0.1436\n",
      "Epoch [1/8], Step [322/750], Loss: 0.0408\n",
      "Epoch [1/8], Step [323/750], Loss: 0.1044\n",
      "Epoch [1/8], Step [324/750], Loss: 0.0843\n",
      "Epoch [1/8], Step [325/750], Loss: 0.1056\n",
      "Epoch [1/8], Step [326/750], Loss: 0.1281\n",
      "Epoch [1/8], Step [327/750], Loss: 0.0703\n",
      "Epoch [1/8], Step [328/750], Loss: 0.1040\n",
      "Epoch [1/8], Step [329/750], Loss: 0.1851\n",
      "Epoch [1/8], Step [330/750], Loss: 0.0957\n",
      "Epoch [1/8], Step [331/750], Loss: 0.0230\n",
      "Epoch [1/8], Step [332/750], Loss: 0.0573\n",
      "Epoch [1/8], Step [333/750], Loss: 0.1557\n",
      "Epoch [1/8], Step [334/750], Loss: 0.1021\n",
      "Epoch [1/8], Step [335/750], Loss: 0.2910\n",
      "Epoch [1/8], Step [336/750], Loss: 0.1225\n",
      "Epoch [1/8], Step [337/750], Loss: 0.0864\n",
      "Epoch [1/8], Step [338/750], Loss: 0.1371\n",
      "Epoch [1/8], Step [339/750], Loss: 0.0895\n",
      "Epoch [1/8], Step [340/750], Loss: 0.0154\n",
      "Epoch [1/8], Step [341/750], Loss: 0.0664\n",
      "Epoch [1/8], Step [342/750], Loss: 0.0450\n",
      "Epoch [1/8], Step [343/750], Loss: 0.1694\n",
      "Epoch [1/8], Step [344/750], Loss: 0.0847\n",
      "Epoch [1/8], Step [345/750], Loss: 0.0400\n",
      "Epoch [1/8], Step [346/750], Loss: 0.0269\n",
      "Epoch [1/8], Step [347/750], Loss: 0.0428\n",
      "Epoch [1/8], Step [348/750], Loss: 0.0299\n",
      "Epoch [1/8], Step [349/750], Loss: 0.0574\n",
      "Epoch [1/8], Step [350/750], Loss: 0.0542\n",
      "Epoch [1/8], Step [351/750], Loss: 0.1123\n",
      "Epoch [1/8], Step [352/750], Loss: 0.1108\n",
      "Epoch [1/8], Step [353/750], Loss: 0.0277\n",
      "Epoch [1/8], Step [354/750], Loss: 0.0773\n",
      "Epoch [1/8], Step [355/750], Loss: 0.0827\n",
      "Epoch [1/8], Step [356/750], Loss: 0.0340\n",
      "Epoch [1/8], Step [357/750], Loss: 0.0373\n",
      "Epoch [1/8], Step [358/750], Loss: 0.0717\n",
      "Epoch [1/8], Step [359/750], Loss: 0.0202\n",
      "Epoch [1/8], Step [360/750], Loss: 0.0351\n",
      "Epoch [1/8], Step [361/750], Loss: 0.0264\n",
      "Epoch [1/8], Step [362/750], Loss: 0.0257\n",
      "Epoch [1/8], Step [363/750], Loss: 0.0182\n",
      "Epoch [1/8], Step [364/750], Loss: 0.0683\n",
      "Epoch [1/8], Step [365/750], Loss: 0.0547\n",
      "Epoch [1/8], Step [366/750], Loss: 0.1057\n",
      "Epoch [1/8], Step [367/750], Loss: 0.0981\n",
      "Epoch [1/8], Step [368/750], Loss: 0.1008\n",
      "Epoch [1/8], Step [369/750], Loss: 0.0502\n",
      "Epoch [1/8], Step [370/750], Loss: 0.1238\n",
      "Epoch [1/8], Step [371/750], Loss: 0.0646\n",
      "Epoch [1/8], Step [372/750], Loss: 0.0505\n",
      "Epoch [1/8], Step [373/750], Loss: 0.0646\n",
      "Epoch [1/8], Step [374/750], Loss: 0.0221\n",
      "Epoch [1/8], Step [375/750], Loss: 0.0771\n",
      "Epoch [1/8], Step [376/750], Loss: 0.0526\n",
      "Epoch [1/8], Step [377/750], Loss: 0.0139\n",
      "Epoch [1/8], Step [378/750], Loss: 0.0721\n",
      "Epoch [1/8], Step [379/750], Loss: 0.2038\n",
      "Epoch [1/8], Step [380/750], Loss: 0.0410\n",
      "Epoch [1/8], Step [381/750], Loss: 0.0248\n",
      "Epoch [1/8], Step [382/750], Loss: 0.0558\n",
      "Epoch [1/8], Step [383/750], Loss: 0.0417\n",
      "Epoch [1/8], Step [384/750], Loss: 0.1683\n",
      "Epoch [1/8], Step [385/750], Loss: 0.0595\n",
      "Epoch [1/8], Step [386/750], Loss: 0.0244\n",
      "Epoch [1/8], Step [387/750], Loss: 0.1943\n",
      "Epoch [1/8], Step [388/750], Loss: 0.1129\n",
      "Epoch [1/8], Step [389/750], Loss: 0.0227\n",
      "Epoch [1/8], Step [390/750], Loss: 0.0226\n",
      "Epoch [1/8], Step [391/750], Loss: 0.0565\n",
      "Epoch [1/8], Step [392/750], Loss: 0.1567\n",
      "Epoch [1/8], Step [393/750], Loss: 0.0881\n",
      "Epoch [1/8], Step [394/750], Loss: 0.1027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Step [395/750], Loss: 0.1565\n",
      "Epoch [1/8], Step [396/750], Loss: 0.1244\n",
      "Epoch [1/8], Step [397/750], Loss: 0.0389\n",
      "Epoch [1/8], Step [398/750], Loss: 0.0436\n",
      "Epoch [1/8], Step [399/750], Loss: 0.0464\n",
      "Epoch [1/8], Step [400/750], Loss: 0.0576\n",
      "Epoch [1/8], Step [401/750], Loss: 0.0395\n",
      "Epoch [1/8], Step [402/750], Loss: 0.0450\n",
      "Epoch [1/8], Step [403/750], Loss: 0.0890\n",
      "Epoch [1/8], Step [404/750], Loss: 0.0926\n",
      "Epoch [1/8], Step [405/750], Loss: 0.0257\n",
      "Epoch [1/8], Step [406/750], Loss: 0.0426\n",
      "Epoch [1/8], Step [407/750], Loss: 0.0406\n",
      "Epoch [1/8], Step [408/750], Loss: 0.0269\n",
      "Epoch [1/8], Step [409/750], Loss: 0.1083\n",
      "Epoch [1/8], Step [410/750], Loss: 0.2277\n",
      "Epoch [1/8], Step [411/750], Loss: 0.0561\n",
      "Epoch [1/8], Step [412/750], Loss: 0.1747\n",
      "Epoch [1/8], Step [413/750], Loss: 0.1088\n",
      "Epoch [1/8], Step [414/750], Loss: 0.0171\n",
      "Epoch [1/8], Step [415/750], Loss: 0.1855\n",
      "Epoch [1/8], Step [416/750], Loss: 0.0613\n",
      "Epoch [1/8], Step [417/750], Loss: 0.0672\n",
      "Epoch [1/8], Step [418/750], Loss: 0.0249\n",
      "Epoch [1/8], Step [419/750], Loss: 0.1789\n",
      "Epoch [1/8], Step [420/750], Loss: 0.0262\n",
      "Epoch [1/8], Step [421/750], Loss: 0.0381\n",
      "Epoch [1/8], Step [422/750], Loss: 0.3352\n",
      "Epoch [1/8], Step [423/750], Loss: 0.1230\n",
      "Epoch [1/8], Step [424/750], Loss: 0.0961\n",
      "Epoch [1/8], Step [425/750], Loss: 0.0519\n",
      "Epoch [1/8], Step [426/750], Loss: 0.0329\n",
      "Epoch [1/8], Step [427/750], Loss: 0.1489\n",
      "Epoch [1/8], Step [428/750], Loss: 0.0789\n",
      "Epoch [1/8], Step [429/750], Loss: 0.0180\n",
      "Epoch [1/8], Step [430/750], Loss: 0.1021\n",
      "Epoch [1/8], Step [431/750], Loss: 0.0905\n",
      "Epoch [1/8], Step [432/750], Loss: 0.0592\n",
      "Epoch [1/8], Step [433/750], Loss: 0.0837\n",
      "Epoch [1/8], Step [434/750], Loss: 0.0229\n",
      "Epoch [1/8], Step [435/750], Loss: 0.0329\n",
      "Epoch [1/8], Step [436/750], Loss: 0.0755\n",
      "Epoch [1/8], Step [437/750], Loss: 0.0806\n",
      "Epoch [1/8], Step [438/750], Loss: 0.0377\n",
      "Epoch [1/8], Step [439/750], Loss: 0.0803\n",
      "Epoch [1/8], Step [440/750], Loss: 0.0783\n",
      "Epoch [1/8], Step [441/750], Loss: 0.1013\n",
      "Epoch [1/8], Step [442/750], Loss: 0.0838\n",
      "Epoch [1/8], Step [443/750], Loss: 0.0516\n",
      "Epoch [1/8], Step [444/750], Loss: 0.1236\n",
      "Epoch [1/8], Step [445/750], Loss: 0.1633\n",
      "Epoch [1/8], Step [446/750], Loss: 0.1148\n",
      "Epoch [1/8], Step [447/750], Loss: 0.0324\n",
      "Epoch [1/8], Step [448/750], Loss: 0.0242\n",
      "Epoch [1/8], Step [449/750], Loss: 0.0750\n",
      "Epoch [1/8], Step [450/750], Loss: 0.0299\n",
      "Epoch [1/8], Step [451/750], Loss: 0.0938\n",
      "Epoch [1/8], Step [452/750], Loss: 0.1675\n",
      "Epoch [1/8], Step [453/750], Loss: 0.0611\n",
      "Epoch [1/8], Step [454/750], Loss: 0.0287\n",
      "Epoch [1/8], Step [455/750], Loss: 0.0156\n",
      "Epoch [1/8], Step [456/750], Loss: 0.0764\n",
      "Epoch [1/8], Step [457/750], Loss: 0.0635\n",
      "Epoch [1/8], Step [458/750], Loss: 0.1872\n",
      "Epoch [1/8], Step [459/750], Loss: 0.0317\n",
      "Epoch [1/8], Step [460/750], Loss: 0.0483\n",
      "Epoch [1/8], Step [461/750], Loss: 0.0428\n",
      "Epoch [1/8], Step [462/750], Loss: 0.0162\n",
      "Epoch [1/8], Step [463/750], Loss: 0.1392\n",
      "Epoch [1/8], Step [464/750], Loss: 0.0223\n",
      "Epoch [1/8], Step [465/750], Loss: 0.0479\n",
      "Epoch [1/8], Step [466/750], Loss: 0.0987\n",
      "Epoch [1/8], Step [467/750], Loss: 0.0648\n",
      "Epoch [1/8], Step [468/750], Loss: 0.0297\n",
      "Epoch [1/8], Step [469/750], Loss: 0.0219\n",
      "Epoch [1/8], Step [470/750], Loss: 0.0457\n",
      "Epoch [1/8], Step [471/750], Loss: 0.1417\n",
      "Epoch [1/8], Step [472/750], Loss: 0.0420\n",
      "Epoch [1/8], Step [473/750], Loss: 0.0599\n",
      "Epoch [1/8], Step [474/750], Loss: 0.0561\n",
      "Epoch [1/8], Step [475/750], Loss: 0.0452\n",
      "Epoch [1/8], Step [476/750], Loss: 0.0850\n",
      "Epoch [1/8], Step [477/750], Loss: 0.0472\n",
      "Epoch [1/8], Step [478/750], Loss: 0.0436\n",
      "Epoch [1/8], Step [479/750], Loss: 0.1243\n",
      "Epoch [1/8], Step [480/750], Loss: 0.1947\n",
      "Epoch [1/8], Step [481/750], Loss: 0.0826\n",
      "Epoch [1/8], Step [482/750], Loss: 0.0521\n",
      "Epoch [1/8], Step [483/750], Loss: 0.0276\n",
      "Epoch [1/8], Step [484/750], Loss: 0.1365\n",
      "Epoch [1/8], Step [485/750], Loss: 0.1441\n",
      "Epoch [1/8], Step [486/750], Loss: 0.0757\n",
      "Epoch [1/8], Step [487/750], Loss: 0.0807\n",
      "Epoch [1/8], Step [488/750], Loss: 0.2107\n",
      "Epoch [1/8], Step [489/750], Loss: 0.1088\n",
      "Epoch [1/8], Step [490/750], Loss: 0.0964\n",
      "Epoch [1/8], Step [491/750], Loss: 0.0780\n",
      "Epoch [1/8], Step [492/750], Loss: 0.0869\n",
      "Epoch [1/8], Step [493/750], Loss: 0.0968\n",
      "Epoch [1/8], Step [494/750], Loss: 0.2659\n",
      "Epoch [1/8], Step [495/750], Loss: 0.0999\n",
      "Epoch [1/8], Step [496/750], Loss: 0.0862\n",
      "Epoch [1/8], Step [497/750], Loss: 0.1285\n",
      "Epoch [1/8], Step [498/750], Loss: 0.0688\n",
      "Epoch [1/8], Step [499/750], Loss: 0.0265\n",
      "Epoch [1/8], Step [500/750], Loss: 0.0320\n",
      "Epoch [1/8], Step [501/750], Loss: 0.0355\n",
      "Epoch [1/8], Step [502/750], Loss: 0.0276\n",
      "Epoch [1/8], Step [503/750], Loss: 0.1740\n",
      "Epoch [1/8], Step [504/750], Loss: 0.1075\n",
      "Epoch [1/8], Step [505/750], Loss: 0.1210\n",
      "Epoch [1/8], Step [506/750], Loss: 0.0835\n",
      "Epoch [1/8], Step [507/750], Loss: 0.1384\n",
      "Epoch [1/8], Step [508/750], Loss: 0.1337\n",
      "Epoch [1/8], Step [509/750], Loss: 0.0637\n",
      "Epoch [1/8], Step [510/750], Loss: 0.0750\n",
      "Epoch [1/8], Step [511/750], Loss: 0.0635\n",
      "Epoch [1/8], Step [512/750], Loss: 0.1128\n",
      "Epoch [1/8], Step [513/750], Loss: 0.0143\n",
      "Epoch [1/8], Step [514/750], Loss: 0.0209\n",
      "Epoch [1/8], Step [515/750], Loss: 0.1249\n",
      "Epoch [1/8], Step [516/750], Loss: 0.1024\n",
      "Epoch [1/8], Step [517/750], Loss: 0.0633\n",
      "Epoch [1/8], Step [518/750], Loss: 0.1246\n",
      "Epoch [1/8], Step [519/750], Loss: 0.2324\n",
      "Epoch [1/8], Step [520/750], Loss: 0.0205\n",
      "Epoch [1/8], Step [521/750], Loss: 0.0904\n",
      "Epoch [1/8], Step [522/750], Loss: 0.0432\n",
      "Epoch [1/8], Step [523/750], Loss: 0.0866\n",
      "Epoch [1/8], Step [524/750], Loss: 0.0182\n",
      "Epoch [1/8], Step [525/750], Loss: 0.0577\n",
      "Epoch [1/8], Step [526/750], Loss: 0.0915\n",
      "Epoch [1/8], Step [527/750], Loss: 0.0378\n",
      "Epoch [1/8], Step [528/750], Loss: 0.1036\n",
      "Epoch [1/8], Step [529/750], Loss: 0.1564\n",
      "Epoch [1/8], Step [530/750], Loss: 0.1155\n",
      "Epoch [1/8], Step [531/750], Loss: 0.1766\n",
      "Epoch [1/8], Step [532/750], Loss: 0.0523\n",
      "Epoch [1/8], Step [533/750], Loss: 0.0848\n",
      "Epoch [1/8], Step [534/750], Loss: 0.0869\n",
      "Epoch [1/8], Step [535/750], Loss: 0.0405\n",
      "Epoch [1/8], Step [536/750], Loss: 0.1198\n",
      "Epoch [1/8], Step [537/750], Loss: 0.1261\n",
      "Epoch [1/8], Step [538/750], Loss: 0.1454\n",
      "Epoch [1/8], Step [539/750], Loss: 0.0683\n",
      "Epoch [1/8], Step [540/750], Loss: 0.0448\n",
      "Epoch [1/8], Step [541/750], Loss: 0.0275\n",
      "Epoch [1/8], Step [542/750], Loss: 0.0329\n",
      "Epoch [1/8], Step [543/750], Loss: 0.0658\n",
      "Epoch [1/8], Step [544/750], Loss: 0.0820\n",
      "Epoch [1/8], Step [545/750], Loss: 0.0870\n",
      "Epoch [1/8], Step [546/750], Loss: 0.0227\n",
      "Epoch [1/8], Step [547/750], Loss: 0.0175\n",
      "Epoch [1/8], Step [548/750], Loss: 0.0875\n",
      "Epoch [1/8], Step [549/750], Loss: 0.0131\n",
      "Epoch [1/8], Step [550/750], Loss: 0.1496\n",
      "Epoch [1/8], Step [551/750], Loss: 0.0171\n",
      "Epoch [1/8], Step [552/750], Loss: 0.0577\n",
      "Epoch [1/8], Step [553/750], Loss: 0.0866\n",
      "Epoch [1/8], Step [554/750], Loss: 0.1117\n",
      "Epoch [1/8], Step [555/750], Loss: 0.1683\n",
      "Epoch [1/8], Step [556/750], Loss: 0.0938\n",
      "Epoch [1/8], Step [557/750], Loss: 0.0149\n",
      "Epoch [1/8], Step [558/750], Loss: 0.0156\n",
      "Epoch [1/8], Step [559/750], Loss: 0.0077\n",
      "Epoch [1/8], Step [560/750], Loss: 0.2723\n",
      "Epoch [1/8], Step [561/750], Loss: 0.0122\n",
      "Epoch [1/8], Step [562/750], Loss: 0.0139\n",
      "Epoch [1/8], Step [563/750], Loss: 0.0564\n",
      "Epoch [1/8], Step [564/750], Loss: 0.0477\n",
      "Epoch [1/8], Step [565/750], Loss: 0.0279\n",
      "Epoch [1/8], Step [566/750], Loss: 0.0102\n",
      "Epoch [1/8], Step [567/750], Loss: 0.0566\n",
      "Epoch [1/8], Step [568/750], Loss: 0.2083\n",
      "Epoch [1/8], Step [569/750], Loss: 0.0282\n",
      "Epoch [1/8], Step [570/750], Loss: 0.0553\n",
      "Epoch [1/8], Step [571/750], Loss: 0.0910\n",
      "Epoch [1/8], Step [572/750], Loss: 0.0647\n",
      "Epoch [1/8], Step [573/750], Loss: 0.0902\n",
      "Epoch [1/8], Step [574/750], Loss: 0.0316\n",
      "Epoch [1/8], Step [575/750], Loss: 0.1543\n",
      "Epoch [1/8], Step [576/750], Loss: 0.0258\n",
      "Epoch [1/8], Step [577/750], Loss: 0.0375\n",
      "Epoch [1/8], Step [578/750], Loss: 0.0398\n",
      "Epoch [1/8], Step [579/750], Loss: 0.0381\n",
      "Epoch [1/8], Step [580/750], Loss: 0.0347\n",
      "Epoch [1/8], Step [581/750], Loss: 0.0218\n",
      "Epoch [1/8], Step [582/750], Loss: 0.0356\n",
      "Epoch [1/8], Step [583/750], Loss: 0.0993\n",
      "Epoch [1/8], Step [584/750], Loss: 0.1377\n",
      "Epoch [1/8], Step [585/750], Loss: 0.0156\n",
      "Epoch [1/8], Step [586/750], Loss: 0.0483\n",
      "Epoch [1/8], Step [587/750], Loss: 0.1069\n",
      "Epoch [1/8], Step [588/750], Loss: 0.0583\n",
      "Epoch [1/8], Step [589/750], Loss: 0.0196\n",
      "Epoch [1/8], Step [590/750], Loss: 0.0740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Step [591/750], Loss: 0.0336\n",
      "Epoch [1/8], Step [592/750], Loss: 0.0404\n",
      "Epoch [1/8], Step [593/750], Loss: 0.0649\n",
      "Epoch [1/8], Step [594/750], Loss: 0.0379\n",
      "Epoch [1/8], Step [595/750], Loss: 0.2648\n",
      "Epoch [1/8], Step [596/750], Loss: 0.0359\n",
      "Epoch [1/8], Step [597/750], Loss: 0.0971\n",
      "Epoch [1/8], Step [598/750], Loss: 0.0137\n",
      "Epoch [1/8], Step [599/750], Loss: 0.0607\n",
      "Epoch [1/8], Step [600/750], Loss: 0.0374\n",
      "Epoch [1/8], Step [601/750], Loss: 0.0231\n",
      "Epoch [1/8], Step [602/750], Loss: 0.0838\n",
      "Epoch [1/8], Step [603/750], Loss: 0.1036\n",
      "Epoch [1/8], Step [604/750], Loss: 0.0333\n",
      "Epoch [1/8], Step [605/750], Loss: 0.0904\n",
      "Epoch [1/8], Step [606/750], Loss: 0.0179\n",
      "Epoch [1/8], Step [607/750], Loss: 0.0402\n",
      "Epoch [1/8], Step [608/750], Loss: 0.0536\n",
      "Epoch [1/8], Step [609/750], Loss: 0.1216\n",
      "Epoch [1/8], Step [610/750], Loss: 0.0479\n",
      "Epoch [1/8], Step [611/750], Loss: 0.1219\n",
      "Epoch [1/8], Step [612/750], Loss: 0.1039\n",
      "Epoch [1/8], Step [613/750], Loss: 0.0473\n",
      "Epoch [1/8], Step [614/750], Loss: 0.0164\n",
      "Epoch [1/8], Step [615/750], Loss: 0.0708\n",
      "Epoch [1/8], Step [616/750], Loss: 0.0593\n",
      "Epoch [1/8], Step [617/750], Loss: 0.0852\n",
      "Epoch [1/8], Step [618/750], Loss: 0.0228\n",
      "Epoch [1/8], Step [619/750], Loss: 0.0556\n",
      "Epoch [1/8], Step [620/750], Loss: 0.0397\n",
      "Epoch [1/8], Step [621/750], Loss: 0.1051\n",
      "Epoch [1/8], Step [622/750], Loss: 0.0535\n",
      "Epoch [1/8], Step [623/750], Loss: 0.1047\n",
      "Epoch [1/8], Step [624/750], Loss: 0.0682\n",
      "Epoch [1/8], Step [625/750], Loss: 0.1169\n",
      "Epoch [1/8], Step [626/750], Loss: 0.0955\n",
      "Epoch [1/8], Step [627/750], Loss: 0.0550\n",
      "Epoch [1/8], Step [628/750], Loss: 0.1139\n",
      "Epoch [1/8], Step [629/750], Loss: 0.1182\n",
      "Epoch [1/8], Step [630/750], Loss: 0.0700\n",
      "Epoch [1/8], Step [631/750], Loss: 0.1355\n",
      "Epoch [1/8], Step [632/750], Loss: 0.0335\n",
      "Epoch [1/8], Step [633/750], Loss: 0.0768\n",
      "Epoch [1/8], Step [634/750], Loss: 0.0526\n",
      "Epoch [1/8], Step [635/750], Loss: 0.0315\n",
      "Epoch [1/8], Step [636/750], Loss: 0.0513\n",
      "Epoch [1/8], Step [637/750], Loss: 0.0176\n",
      "Epoch [1/8], Step [638/750], Loss: 0.0469\n",
      "Epoch [1/8], Step [639/750], Loss: 0.0592\n",
      "Epoch [1/8], Step [640/750], Loss: 0.1177\n",
      "Epoch [1/8], Step [641/750], Loss: 0.1367\n",
      "Epoch [1/8], Step [642/750], Loss: 0.0704\n",
      "Epoch [1/8], Step [643/750], Loss: 0.0173\n",
      "Epoch [1/8], Step [644/750], Loss: 0.0284\n",
      "Epoch [1/8], Step [645/750], Loss: 0.1122\n",
      "Epoch [1/8], Step [646/750], Loss: 0.0290\n",
      "Epoch [1/8], Step [647/750], Loss: 0.0137\n",
      "Epoch [1/8], Step [648/750], Loss: 0.0240\n",
      "Epoch [1/8], Step [649/750], Loss: 0.1613\n",
      "Epoch [1/8], Step [650/750], Loss: 0.0689\n",
      "Epoch [1/8], Step [651/750], Loss: 0.0068\n",
      "Epoch [1/8], Step [652/750], Loss: 0.0533\n",
      "Epoch [1/8], Step [653/750], Loss: 0.1703\n",
      "Epoch [1/8], Step [654/750], Loss: 0.1802\n",
      "Epoch [1/8], Step [655/750], Loss: 0.0151\n",
      "Epoch [1/8], Step [656/750], Loss: 0.0781\n",
      "Epoch [1/8], Step [657/750], Loss: 0.0211\n",
      "Epoch [1/8], Step [658/750], Loss: 0.1559\n",
      "Epoch [1/8], Step [659/750], Loss: 0.0126\n",
      "Epoch [1/8], Step [660/750], Loss: 0.1454\n",
      "Epoch [1/8], Step [661/750], Loss: 0.1237\n",
      "Epoch [1/8], Step [662/750], Loss: 0.0593\n",
      "Epoch [1/8], Step [663/750], Loss: 0.0943\n",
      "Epoch [1/8], Step [664/750], Loss: 0.0110\n",
      "Epoch [1/8], Step [665/750], Loss: 0.0892\n",
      "Epoch [1/8], Step [666/750], Loss: 0.0224\n",
      "Epoch [1/8], Step [667/750], Loss: 0.0348\n",
      "Epoch [1/8], Step [668/750], Loss: 0.0744\n",
      "Epoch [1/8], Step [669/750], Loss: 0.0176\n",
      "Epoch [1/8], Step [670/750], Loss: 0.0163\n",
      "Epoch [1/8], Step [671/750], Loss: 0.2209\n",
      "Epoch [1/8], Step [672/750], Loss: 0.1247\n",
      "Epoch [1/8], Step [673/750], Loss: 0.0865\n",
      "Epoch [1/8], Step [674/750], Loss: 0.0703\n",
      "Epoch [1/8], Step [675/750], Loss: 0.0694\n",
      "Epoch [1/8], Step [676/750], Loss: 0.0721\n",
      "Epoch [1/8], Step [677/750], Loss: 0.1038\n",
      "Epoch [1/8], Step [678/750], Loss: 0.0260\n",
      "Epoch [1/8], Step [679/750], Loss: 0.0782\n",
      "Epoch [1/8], Step [680/750], Loss: 0.0496\n",
      "Epoch [1/8], Step [681/750], Loss: 0.0771\n",
      "Epoch [1/8], Step [682/750], Loss: 0.1668\n",
      "Epoch [1/8], Step [683/750], Loss: 0.0433\n",
      "Epoch [1/8], Step [684/750], Loss: 0.0422\n",
      "Epoch [1/8], Step [685/750], Loss: 0.0197\n",
      "Epoch [1/8], Step [686/750], Loss: 0.1754\n",
      "Epoch [1/8], Step [687/750], Loss: 0.2761\n",
      "Epoch [1/8], Step [688/750], Loss: 0.0223\n",
      "Epoch [1/8], Step [689/750], Loss: 0.0895\n",
      "Epoch [1/8], Step [690/750], Loss: 0.0298\n",
      "Epoch [1/8], Step [691/750], Loss: 0.0373\n",
      "Epoch [1/8], Step [692/750], Loss: 0.1046\n",
      "Epoch [1/8], Step [693/750], Loss: 0.0280\n",
      "Epoch [1/8], Step [694/750], Loss: 0.0459\n",
      "Epoch [1/8], Step [695/750], Loss: 0.0223\n",
      "Epoch [1/8], Step [696/750], Loss: 0.0872\n",
      "Epoch [1/8], Step [697/750], Loss: 0.0463\n",
      "Epoch [1/8], Step [698/750], Loss: 0.1000\n",
      "Epoch [1/8], Step [699/750], Loss: 0.1009\n",
      "Epoch [1/8], Step [700/750], Loss: 0.0837\n",
      "Epoch [1/8], Step [701/750], Loss: 0.0259\n",
      "Epoch [1/8], Step [702/750], Loss: 0.0381\n",
      "Epoch [1/8], Step [703/750], Loss: 0.0430\n",
      "Epoch [1/8], Step [704/750], Loss: 0.0584\n",
      "Epoch [1/8], Step [705/750], Loss: 0.0995\n",
      "Epoch [1/8], Step [706/750], Loss: 0.0302\n",
      "Epoch [1/8], Step [707/750], Loss: 0.0540\n",
      "Epoch [1/8], Step [708/750], Loss: 0.0229\n",
      "Epoch [1/8], Step [709/750], Loss: 0.0239\n",
      "Epoch [1/8], Step [710/750], Loss: 0.0255\n",
      "Epoch [1/8], Step [711/750], Loss: 0.1421\n",
      "Epoch [1/8], Step [712/750], Loss: 0.1319\n",
      "Epoch [1/8], Step [713/750], Loss: 0.0228\n",
      "Epoch [1/8], Step [714/750], Loss: 0.0219\n",
      "Epoch [1/8], Step [715/750], Loss: 0.0158\n",
      "Epoch [1/8], Step [716/750], Loss: 0.1239\n",
      "Epoch [1/8], Step [717/750], Loss: 0.1231\n",
      "Epoch [1/8], Step [718/750], Loss: 0.0522\n",
      "Epoch [1/8], Step [719/750], Loss: 0.0490\n",
      "Epoch [1/8], Step [720/750], Loss: 0.0765\n",
      "Epoch [1/8], Step [721/750], Loss: 0.1716\n",
      "Epoch [1/8], Step [722/750], Loss: 0.0948\n",
      "Epoch [1/8], Step [723/750], Loss: 0.0242\n",
      "Epoch [1/8], Step [724/750], Loss: 0.0246\n",
      "Epoch [1/8], Step [725/750], Loss: 0.0250\n",
      "Epoch [1/8], Step [726/750], Loss: 0.0209\n",
      "Epoch [1/8], Step [727/750], Loss: 0.0614\n",
      "Epoch [1/8], Step [728/750], Loss: 0.0572\n",
      "Epoch [1/8], Step [729/750], Loss: 0.1085\n",
      "Epoch [1/8], Step [730/750], Loss: 0.1485\n",
      "Epoch [1/8], Step [731/750], Loss: 0.0509\n",
      "Epoch [1/8], Step [732/750], Loss: 0.0280\n",
      "Epoch [1/8], Step [733/750], Loss: 0.0524\n",
      "Epoch [1/8], Step [734/750], Loss: 0.0210\n",
      "Epoch [1/8], Step [735/750], Loss: 0.0311\n",
      "Epoch [1/8], Step [736/750], Loss: 0.0101\n",
      "Epoch [1/8], Step [737/750], Loss: 0.0711\n",
      "Epoch [1/8], Step [738/750], Loss: 0.0139\n",
      "Epoch [1/8], Step [739/750], Loss: 0.1446\n",
      "Epoch [1/8], Step [740/750], Loss: 0.0690\n",
      "Epoch [1/8], Step [741/750], Loss: 0.0174\n",
      "Epoch [1/8], Step [742/750], Loss: 0.0818\n",
      "Epoch [1/8], Step [743/750], Loss: 0.0434\n",
      "Epoch [1/8], Step [744/750], Loss: 0.0873\n",
      "Epoch [1/8], Step [745/750], Loss: 0.0527\n",
      "Epoch [1/8], Step [746/750], Loss: 0.0478\n",
      "Epoch [1/8], Step [747/750], Loss: 0.1314\n",
      "Epoch [1/8], Step [748/750], Loss: 0.0825\n",
      "Epoch [1/8], Step [749/750], Loss: 0.1327\n",
      "Epoch [1/8], Step [750/750], Loss: 0.1402\n",
      "Epoch [1/8], Tr. loss: 0.1586. Test loss: 0.0654\n",
      "\n",
      "\n",
      "Epoch [2/8], Step [1/750], Loss: 0.0650\n",
      "Epoch [2/8], Step [2/750], Loss: 0.0201\n",
      "Epoch [2/8], Step [3/750], Loss: 0.0136\n",
      "Epoch [2/8], Step [4/750], Loss: 0.0414\n",
      "Epoch [2/8], Step [5/750], Loss: 0.0616\n",
      "Epoch [2/8], Step [6/750], Loss: 0.0996\n",
      "Epoch [2/8], Step [7/750], Loss: 0.0952\n",
      "Epoch [2/8], Step [8/750], Loss: 0.0660\n",
      "Epoch [2/8], Step [9/750], Loss: 0.0506\n",
      "Epoch [2/8], Step [10/750], Loss: 0.1737\n",
      "Epoch [2/8], Step [11/750], Loss: 0.0733\n",
      "Epoch [2/8], Step [12/750], Loss: 0.0350\n",
      "Epoch [2/8], Step [13/750], Loss: 0.0457\n",
      "Epoch [2/8], Step [14/750], Loss: 0.0299\n",
      "Epoch [2/8], Step [15/750], Loss: 0.0856\n",
      "Epoch [2/8], Step [16/750], Loss: 0.0660\n",
      "Epoch [2/8], Step [17/750], Loss: 0.0603\n",
      "Epoch [2/8], Step [18/750], Loss: 0.0854\n",
      "Epoch [2/8], Step [19/750], Loss: 0.0765\n",
      "Epoch [2/8], Step [20/750], Loss: 0.0232\n",
      "Epoch [2/8], Step [21/750], Loss: 0.1111\n",
      "Epoch [2/8], Step [22/750], Loss: 0.1344\n",
      "Epoch [2/8], Step [23/750], Loss: 0.0676\n",
      "Epoch [2/8], Step [24/750], Loss: 0.1213\n",
      "Epoch [2/8], Step [25/750], Loss: 0.1023\n",
      "Epoch [2/8], Step [26/750], Loss: 0.2275\n",
      "Epoch [2/8], Step [27/750], Loss: 0.0734\n",
      "Epoch [2/8], Step [28/750], Loss: 0.1511\n",
      "Epoch [2/8], Step [29/750], Loss: 0.0628\n",
      "Epoch [2/8], Step [30/750], Loss: 0.1976\n",
      "Epoch [2/8], Step [31/750], Loss: 0.0245\n",
      "Epoch [2/8], Step [32/750], Loss: 0.1349\n",
      "Epoch [2/8], Step [33/750], Loss: 0.1419\n",
      "Epoch [2/8], Step [34/750], Loss: 0.1173\n",
      "Epoch [2/8], Step [35/750], Loss: 0.0338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Step [36/750], Loss: 0.1421\n",
      "Epoch [2/8], Step [37/750], Loss: 0.1456\n",
      "Epoch [2/8], Step [38/750], Loss: 0.0879\n",
      "Epoch [2/8], Step [39/750], Loss: 0.0955\n",
      "Epoch [2/8], Step [40/750], Loss: 0.0344\n",
      "Epoch [2/8], Step [41/750], Loss: 0.0210\n",
      "Epoch [2/8], Step [42/750], Loss: 0.1007\n",
      "Epoch [2/8], Step [43/750], Loss: 0.0370\n",
      "Epoch [2/8], Step [44/750], Loss: 0.1449\n",
      "Epoch [2/8], Step [45/750], Loss: 0.2204\n",
      "Epoch [2/8], Step [46/750], Loss: 0.2811\n",
      "Epoch [2/8], Step [47/750], Loss: 0.0901\n",
      "Epoch [2/8], Step [48/750], Loss: 0.2572\n",
      "Epoch [2/8], Step [49/750], Loss: 0.1206\n",
      "Epoch [2/8], Step [50/750], Loss: 0.1133\n",
      "Epoch [2/8], Step [51/750], Loss: 0.0475\n",
      "Epoch [2/8], Step [52/750], Loss: 0.0638\n",
      "Epoch [2/8], Step [53/750], Loss: 0.0364\n",
      "Epoch [2/8], Step [54/750], Loss: 0.0947\n",
      "Epoch [2/8], Step [55/750], Loss: 0.0256\n",
      "Epoch [2/8], Step [56/750], Loss: 0.0124\n",
      "Epoch [2/8], Step [57/750], Loss: 0.1709\n",
      "Epoch [2/8], Step [58/750], Loss: 0.1388\n",
      "Epoch [2/8], Step [59/750], Loss: 0.0543\n",
      "Epoch [2/8], Step [60/750], Loss: 0.0286\n",
      "Epoch [2/8], Step [61/750], Loss: 0.1096\n",
      "Epoch [2/8], Step [62/750], Loss: 0.1781\n",
      "Epoch [2/8], Step [63/750], Loss: 0.1187\n",
      "Epoch [2/8], Step [64/750], Loss: 0.0140\n",
      "Epoch [2/8], Step [65/750], Loss: 0.1195\n",
      "Epoch [2/8], Step [66/750], Loss: 0.1398\n",
      "Epoch [2/8], Step [67/750], Loss: 0.1947\n",
      "Epoch [2/8], Step [68/750], Loss: 0.0487\n",
      "Epoch [2/8], Step [69/750], Loss: 0.1359\n",
      "Epoch [2/8], Step [70/750], Loss: 0.1371\n",
      "Epoch [2/8], Step [71/750], Loss: 0.1912\n",
      "Epoch [2/8], Step [72/750], Loss: 0.1280\n",
      "Epoch [2/8], Step [73/750], Loss: 0.1031\n",
      "Epoch [2/8], Step [74/750], Loss: 0.0459\n",
      "Epoch [2/8], Step [75/750], Loss: 0.0363\n",
      "Epoch [2/8], Step [76/750], Loss: 0.0718\n",
      "Epoch [2/8], Step [77/750], Loss: 0.0589\n",
      "Epoch [2/8], Step [78/750], Loss: 0.0446\n",
      "Epoch [2/8], Step [79/750], Loss: 0.0235\n",
      "Epoch [2/8], Step [80/750], Loss: 0.0565\n",
      "Epoch [2/8], Step [81/750], Loss: 0.1808\n",
      "Epoch [2/8], Step [82/750], Loss: 0.0679\n",
      "Epoch [2/8], Step [83/750], Loss: 0.0480\n",
      "Epoch [2/8], Step [84/750], Loss: 0.0315\n",
      "Epoch [2/8], Step [85/750], Loss: 0.0402\n",
      "Epoch [2/8], Step [86/750], Loss: 0.0392\n",
      "Epoch [2/8], Step [87/750], Loss: 0.0287\n",
      "Epoch [2/8], Step [88/750], Loss: 0.0565\n",
      "Epoch [2/8], Step [89/750], Loss: 0.1312\n",
      "Epoch [2/8], Step [90/750], Loss: 0.1407\n",
      "Epoch [2/8], Step [91/750], Loss: 0.0450\n",
      "Epoch [2/8], Step [92/750], Loss: 0.0144\n",
      "Epoch [2/8], Step [93/750], Loss: 0.0383\n",
      "Epoch [2/8], Step [94/750], Loss: 0.0458\n",
      "Epoch [2/8], Step [95/750], Loss: 0.0335\n",
      "Epoch [2/8], Step [96/750], Loss: 0.2044\n",
      "Epoch [2/8], Step [97/750], Loss: 0.1203\n",
      "Epoch [2/8], Step [98/750], Loss: 0.0981\n",
      "Epoch [2/8], Step [99/750], Loss: 0.0204\n",
      "Epoch [2/8], Step [100/750], Loss: 0.0752\n",
      "Epoch [2/8], Step [101/750], Loss: 0.0610\n",
      "Epoch [2/8], Step [102/750], Loss: 0.2538\n",
      "Epoch [2/8], Step [103/750], Loss: 0.0085\n",
      "Epoch [2/8], Step [104/750], Loss: 0.1140\n",
      "Epoch [2/8], Step [105/750], Loss: 0.1404\n",
      "Epoch [2/8], Step [106/750], Loss: 0.0946\n",
      "Epoch [2/8], Step [107/750], Loss: 0.0547\n",
      "Epoch [2/8], Step [108/750], Loss: 0.0129\n",
      "Epoch [2/8], Step [109/750], Loss: 0.1801\n",
      "Epoch [2/8], Step [110/750], Loss: 0.0889\n",
      "Epoch [2/8], Step [111/750], Loss: 0.0930\n",
      "Epoch [2/8], Step [112/750], Loss: 0.0230\n",
      "Epoch [2/8], Step [113/750], Loss: 0.0422\n",
      "Epoch [2/8], Step [114/750], Loss: 0.2875\n",
      "Epoch [2/8], Step [115/750], Loss: 0.0424\n",
      "Epoch [2/8], Step [116/750], Loss: 0.0207\n",
      "Epoch [2/8], Step [117/750], Loss: 0.1036\n",
      "Epoch [2/8], Step [118/750], Loss: 0.0485\n",
      "Epoch [2/8], Step [119/750], Loss: 0.0230\n",
      "Epoch [2/8], Step [120/750], Loss: 0.1107\n",
      "Epoch [2/8], Step [121/750], Loss: 0.1005\n",
      "Epoch [2/8], Step [122/750], Loss: 0.1004\n",
      "Epoch [2/8], Step [123/750], Loss: 0.1580\n",
      "Epoch [2/8], Step [124/750], Loss: 0.0529\n",
      "Epoch [2/8], Step [125/750], Loss: 0.0882\n",
      "Epoch [2/8], Step [126/750], Loss: 0.1445\n",
      "Epoch [2/8], Step [127/750], Loss: 0.2086\n",
      "Epoch [2/8], Step [128/750], Loss: 0.0931\n",
      "Epoch [2/8], Step [129/750], Loss: 0.0724\n",
      "Epoch [2/8], Step [130/750], Loss: 0.1131\n",
      "Epoch [2/8], Step [131/750], Loss: 0.1115\n",
      "Epoch [2/8], Step [132/750], Loss: 0.0574\n",
      "Epoch [2/8], Step [133/750], Loss: 0.0683\n",
      "Epoch [2/8], Step [134/750], Loss: 0.1289\n",
      "Epoch [2/8], Step [135/750], Loss: 0.1806\n",
      "Epoch [2/8], Step [136/750], Loss: 0.1494\n",
      "Epoch [2/8], Step [137/750], Loss: 0.0926\n",
      "Epoch [2/8], Step [138/750], Loss: 0.0885\n",
      "Epoch [2/8], Step [139/750], Loss: 0.1359\n",
      "Epoch [2/8], Step [140/750], Loss: 0.0381\n",
      "Epoch [2/8], Step [141/750], Loss: 0.0685\n",
      "Epoch [2/8], Step [142/750], Loss: 0.0098\n",
      "Epoch [2/8], Step [143/750], Loss: 0.0837\n",
      "Epoch [2/8], Step [144/750], Loss: 0.0238\n",
      "Epoch [2/8], Step [145/750], Loss: 0.1557\n",
      "Epoch [2/8], Step [146/750], Loss: 0.0210\n",
      "Epoch [2/8], Step [147/750], Loss: 0.0326\n",
      "Epoch [2/8], Step [148/750], Loss: 0.0177\n",
      "Epoch [2/8], Step [149/750], Loss: 0.0131\n",
      "Epoch [2/8], Step [150/750], Loss: 0.0713\n",
      "Epoch [2/8], Step [151/750], Loss: 0.0112\n",
      "Epoch [2/8], Step [152/750], Loss: 0.0099\n",
      "Epoch [2/8], Step [153/750], Loss: 0.0610\n",
      "Epoch [2/8], Step [154/750], Loss: 0.0211\n",
      "Epoch [2/8], Step [155/750], Loss: 0.0166\n",
      "Epoch [2/8], Step [156/750], Loss: 0.0452\n",
      "Epoch [2/8], Step [157/750], Loss: 0.0252\n",
      "Epoch [2/8], Step [158/750], Loss: 0.0167\n",
      "Epoch [2/8], Step [159/750], Loss: 0.0191\n",
      "Epoch [2/8], Step [160/750], Loss: 0.0405\n",
      "Epoch [2/8], Step [161/750], Loss: 0.0808\n",
      "Epoch [2/8], Step [162/750], Loss: 0.0199\n",
      "Epoch [2/8], Step [163/750], Loss: 0.0853\n",
      "Epoch [2/8], Step [164/750], Loss: 0.0257\n",
      "Epoch [2/8], Step [165/750], Loss: 0.0579\n",
      "Epoch [2/8], Step [166/750], Loss: 0.0887\n",
      "Epoch [2/8], Step [167/750], Loss: 0.0577\n",
      "Epoch [2/8], Step [168/750], Loss: 0.0207\n",
      "Epoch [2/8], Step [169/750], Loss: 0.0879\n",
      "Epoch [2/8], Step [170/750], Loss: 0.0194\n",
      "Epoch [2/8], Step [171/750], Loss: 0.1157\n",
      "Epoch [2/8], Step [172/750], Loss: 0.0613\n",
      "Epoch [2/8], Step [173/750], Loss: 0.0992\n",
      "Epoch [2/8], Step [174/750], Loss: 0.0469\n",
      "Epoch [2/8], Step [175/750], Loss: 0.1454\n",
      "Epoch [2/8], Step [176/750], Loss: 0.1174\n",
      "Epoch [2/8], Step [177/750], Loss: 0.0338\n",
      "Epoch [2/8], Step [178/750], Loss: 0.1703\n",
      "Epoch [2/8], Step [179/750], Loss: 0.1096\n",
      "Epoch [2/8], Step [180/750], Loss: 0.0775\n",
      "Epoch [2/8], Step [181/750], Loss: 0.0411\n",
      "Epoch [2/8], Step [182/750], Loss: 0.0784\n",
      "Epoch [2/8], Step [183/750], Loss: 0.1445\n",
      "Epoch [2/8], Step [184/750], Loss: 0.0437\n",
      "Epoch [2/8], Step [185/750], Loss: 0.1188\n",
      "Epoch [2/8], Step [186/750], Loss: 0.0180\n",
      "Epoch [2/8], Step [187/750], Loss: 0.1106\n",
      "Epoch [2/8], Step [188/750], Loss: 0.0534\n",
      "Epoch [2/8], Step [189/750], Loss: 0.0749\n",
      "Epoch [2/8], Step [190/750], Loss: 0.0290\n",
      "Epoch [2/8], Step [191/750], Loss: 0.0492\n",
      "Epoch [2/8], Step [192/750], Loss: 0.0568\n",
      "Epoch [2/8], Step [193/750], Loss: 0.0462\n",
      "Epoch [2/8], Step [194/750], Loss: 0.1178\n",
      "Epoch [2/8], Step [195/750], Loss: 0.1254\n",
      "Epoch [2/8], Step [196/750], Loss: 0.0918\n",
      "Epoch [2/8], Step [197/750], Loss: 0.0137\n",
      "Epoch [2/8], Step [198/750], Loss: 0.1019\n",
      "Epoch [2/8], Step [199/750], Loss: 0.0181\n",
      "Epoch [2/8], Step [200/750], Loss: 0.0038\n",
      "Epoch [2/8], Step [201/750], Loss: 0.1146\n",
      "Epoch [2/8], Step [202/750], Loss: 0.1021\n",
      "Epoch [2/8], Step [203/750], Loss: 0.0573\n",
      "Epoch [2/8], Step [204/750], Loss: 0.0094\n",
      "Epoch [2/8], Step [205/750], Loss: 0.0278\n",
      "Epoch [2/8], Step [206/750], Loss: 0.0577\n",
      "Epoch [2/8], Step [207/750], Loss: 0.0214\n",
      "Epoch [2/8], Step [208/750], Loss: 0.0281\n",
      "Epoch [2/8], Step [209/750], Loss: 0.0049\n",
      "Epoch [2/8], Step [210/750], Loss: 0.0719\n",
      "Epoch [2/8], Step [211/750], Loss: 0.0236\n",
      "Epoch [2/8], Step [212/750], Loss: 0.0344\n",
      "Epoch [2/8], Step [213/750], Loss: 0.0184\n",
      "Epoch [2/8], Step [214/750], Loss: 0.0098\n",
      "Epoch [2/8], Step [215/750], Loss: 0.1970\n",
      "Epoch [2/8], Step [216/750], Loss: 0.0539\n",
      "Epoch [2/8], Step [217/750], Loss: 0.0170\n",
      "Epoch [2/8], Step [218/750], Loss: 0.0690\n",
      "Epoch [2/8], Step [219/750], Loss: 0.0226\n",
      "Epoch [2/8], Step [220/750], Loss: 0.0232\n",
      "Epoch [2/8], Step [221/750], Loss: 0.0298\n",
      "Epoch [2/8], Step [222/750], Loss: 0.0429\n",
      "Epoch [2/8], Step [223/750], Loss: 0.0612\n",
      "Epoch [2/8], Step [224/750], Loss: 0.2006\n",
      "Epoch [2/8], Step [225/750], Loss: 0.0193\n",
      "Epoch [2/8], Step [226/750], Loss: 0.0632\n",
      "Epoch [2/8], Step [227/750], Loss: 0.0300\n",
      "Epoch [2/8], Step [228/750], Loss: 0.0523\n",
      "Epoch [2/8], Step [229/750], Loss: 0.0430\n",
      "Epoch [2/8], Step [230/750], Loss: 0.1086\n",
      "Epoch [2/8], Step [231/750], Loss: 0.0115\n",
      "Epoch [2/8], Step [232/750], Loss: 0.0149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Step [233/750], Loss: 0.0263\n",
      "Epoch [2/8], Step [234/750], Loss: 0.0653\n",
      "Epoch [2/8], Step [235/750], Loss: 0.0434\n",
      "Epoch [2/8], Step [236/750], Loss: 0.0644\n",
      "Epoch [2/8], Step [237/750], Loss: 0.0369\n",
      "Epoch [2/8], Step [238/750], Loss: 0.0574\n",
      "Epoch [2/8], Step [239/750], Loss: 0.0256\n",
      "Epoch [2/8], Step [240/750], Loss: 0.1826\n",
      "Epoch [2/8], Step [241/750], Loss: 0.0393\n",
      "Epoch [2/8], Step [242/750], Loss: 0.0924\n",
      "Epoch [2/8], Step [243/750], Loss: 0.0999\n",
      "Epoch [2/8], Step [244/750], Loss: 0.1127\n",
      "Epoch [2/8], Step [245/750], Loss: 0.0322\n",
      "Epoch [2/8], Step [246/750], Loss: 0.0303\n",
      "Epoch [2/8], Step [247/750], Loss: 0.0034\n",
      "Epoch [2/8], Step [248/750], Loss: 0.0302\n",
      "Epoch [2/8], Step [249/750], Loss: 0.0450\n",
      "Epoch [2/8], Step [250/750], Loss: 0.0068\n",
      "Epoch [2/8], Step [251/750], Loss: 0.0208\n",
      "Epoch [2/8], Step [252/750], Loss: 0.1730\n",
      "Epoch [2/8], Step [253/750], Loss: 0.1017\n",
      "Epoch [2/8], Step [254/750], Loss: 0.0644\n",
      "Epoch [2/8], Step [255/750], Loss: 0.1254\n",
      "Epoch [2/8], Step [256/750], Loss: 0.0361\n",
      "Epoch [2/8], Step [257/750], Loss: 0.0646\n",
      "Epoch [2/8], Step [258/750], Loss: 0.0061\n",
      "Epoch [2/8], Step [259/750], Loss: 0.0182\n",
      "Epoch [2/8], Step [260/750], Loss: 0.0886\n",
      "Epoch [2/8], Step [261/750], Loss: 0.0096\n",
      "Epoch [2/8], Step [262/750], Loss: 0.1395\n",
      "Epoch [2/8], Step [263/750], Loss: 0.0269\n",
      "Epoch [2/8], Step [264/750], Loss: 0.0382\n",
      "Epoch [2/8], Step [265/750], Loss: 0.0801\n",
      "Epoch [2/8], Step [266/750], Loss: 0.0825\n",
      "Epoch [2/8], Step [267/750], Loss: 0.0194\n",
      "Epoch [2/8], Step [268/750], Loss: 0.0474\n",
      "Epoch [2/8], Step [269/750], Loss: 0.1133\n",
      "Epoch [2/8], Step [270/750], Loss: 0.0538\n",
      "Epoch [2/8], Step [271/750], Loss: 0.0192\n",
      "Epoch [2/8], Step [272/750], Loss: 0.0781\n",
      "Epoch [2/8], Step [273/750], Loss: 0.0089\n",
      "Epoch [2/8], Step [274/750], Loss: 0.0093\n",
      "Epoch [2/8], Step [275/750], Loss: 0.0501\n",
      "Epoch [2/8], Step [276/750], Loss: 0.0089\n",
      "Epoch [2/8], Step [277/750], Loss: 0.0161\n",
      "Epoch [2/8], Step [278/750], Loss: 0.0191\n",
      "Epoch [2/8], Step [279/750], Loss: 0.0056\n",
      "Epoch [2/8], Step [280/750], Loss: 0.1216\n",
      "Epoch [2/8], Step [281/750], Loss: 0.1592\n",
      "Epoch [2/8], Step [282/750], Loss: 0.0384\n",
      "Epoch [2/8], Step [283/750], Loss: 0.0647\n",
      "Epoch [2/8], Step [284/750], Loss: 0.0601\n",
      "Epoch [2/8], Step [285/750], Loss: 0.0910\n",
      "Epoch [2/8], Step [286/750], Loss: 0.0600\n",
      "Epoch [2/8], Step [287/750], Loss: 0.0441\n",
      "Epoch [2/8], Step [288/750], Loss: 0.0465\n",
      "Epoch [2/8], Step [289/750], Loss: 0.0883\n",
      "Epoch [2/8], Step [290/750], Loss: 0.0436\n",
      "Epoch [2/8], Step [291/750], Loss: 0.0175\n",
      "Epoch [2/8], Step [292/750], Loss: 0.0614\n",
      "Epoch [2/8], Step [293/750], Loss: 0.0030\n",
      "Epoch [2/8], Step [294/750], Loss: 0.1446\n",
      "Epoch [2/8], Step [295/750], Loss: 0.0297\n",
      "Epoch [2/8], Step [296/750], Loss: 0.0223\n",
      "Epoch [2/8], Step [297/750], Loss: 0.0136\n",
      "Epoch [2/8], Step [298/750], Loss: 0.0492\n",
      "Epoch [2/8], Step [299/750], Loss: 0.0056\n",
      "Epoch [2/8], Step [300/750], Loss: 0.1180\n",
      "Epoch [2/8], Step [301/750], Loss: 0.0224\n",
      "Epoch [2/8], Step [302/750], Loss: 0.0030\n",
      "Epoch [2/8], Step [303/750], Loss: 0.1443\n",
      "Epoch [2/8], Step [304/750], Loss: 0.0376\n",
      "Epoch [2/8], Step [305/750], Loss: 0.0331\n",
      "Epoch [2/8], Step [306/750], Loss: 0.0373\n",
      "Epoch [2/8], Step [307/750], Loss: 0.0227\n",
      "Epoch [2/8], Step [308/750], Loss: 0.0095\n",
      "Epoch [2/8], Step [309/750], Loss: 0.0223\n",
      "Epoch [2/8], Step [310/750], Loss: 0.0144\n",
      "Epoch [2/8], Step [311/750], Loss: 0.0970\n",
      "Epoch [2/8], Step [312/750], Loss: 0.0473\n",
      "Epoch [2/8], Step [313/750], Loss: 0.0128\n",
      "Epoch [2/8], Step [314/750], Loss: 0.1132\n",
      "Epoch [2/8], Step [315/750], Loss: 0.0292\n",
      "Epoch [2/8], Step [316/750], Loss: 0.0506\n",
      "Epoch [2/8], Step [317/750], Loss: 0.0035\n",
      "Epoch [2/8], Step [318/750], Loss: 0.0520\n",
      "Epoch [2/8], Step [319/750], Loss: 0.0529\n",
      "Epoch [2/8], Step [320/750], Loss: 0.0362\n",
      "Epoch [2/8], Step [321/750], Loss: 0.0068\n",
      "Epoch [2/8], Step [322/750], Loss: 0.1310\n",
      "Epoch [2/8], Step [323/750], Loss: 0.0222\n",
      "Epoch [2/8], Step [324/750], Loss: 0.0245\n",
      "Epoch [2/8], Step [325/750], Loss: 0.0159\n",
      "Epoch [2/8], Step [326/750], Loss: 0.0869\n",
      "Epoch [2/8], Step [327/750], Loss: 0.1729\n",
      "Epoch [2/8], Step [328/750], Loss: 0.0878\n",
      "Epoch [2/8], Step [329/750], Loss: 0.0347\n",
      "Epoch [2/8], Step [330/750], Loss: 0.1598\n",
      "Epoch [2/8], Step [331/750], Loss: 0.0413\n",
      "Epoch [2/8], Step [332/750], Loss: 0.1048\n",
      "Epoch [2/8], Step [333/750], Loss: 0.1188\n",
      "Epoch [2/8], Step [334/750], Loss: 0.0805\n",
      "Epoch [2/8], Step [335/750], Loss: 0.0322\n",
      "Epoch [2/8], Step [336/750], Loss: 0.0041\n",
      "Epoch [2/8], Step [337/750], Loss: 0.1617\n",
      "Epoch [2/8], Step [338/750], Loss: 0.0264\n",
      "Epoch [2/8], Step [339/750], Loss: 0.0059\n",
      "Epoch [2/8], Step [340/750], Loss: 0.0618\n",
      "Epoch [2/8], Step [341/750], Loss: 0.1153\n",
      "Epoch [2/8], Step [342/750], Loss: 0.0423\n",
      "Epoch [2/8], Step [343/750], Loss: 0.0581\n",
      "Epoch [2/8], Step [344/750], Loss: 0.0673\n",
      "Epoch [2/8], Step [345/750], Loss: 0.0649\n",
      "Epoch [2/8], Step [346/750], Loss: 0.0387\n",
      "Epoch [2/8], Step [347/750], Loss: 0.0482\n",
      "Epoch [2/8], Step [348/750], Loss: 0.0643\n",
      "Epoch [2/8], Step [349/750], Loss: 0.0302\n",
      "Epoch [2/8], Step [350/750], Loss: 0.1431\n",
      "Epoch [2/8], Step [351/750], Loss: 0.0997\n",
      "Epoch [2/8], Step [352/750], Loss: 0.0077\n",
      "Epoch [2/8], Step [353/750], Loss: 0.0414\n",
      "Epoch [2/8], Step [354/750], Loss: 0.0524\n",
      "Epoch [2/8], Step [355/750], Loss: 0.0311\n",
      "Epoch [2/8], Step [356/750], Loss: 0.0631\n",
      "Epoch [2/8], Step [357/750], Loss: 0.0256\n",
      "Epoch [2/8], Step [358/750], Loss: 0.0622\n",
      "Epoch [2/8], Step [359/750], Loss: 0.0183\n",
      "Epoch [2/8], Step [360/750], Loss: 0.1228\n",
      "Epoch [2/8], Step [361/750], Loss: 0.0260\n",
      "Epoch [2/8], Step [362/750], Loss: 0.0201\n",
      "Epoch [2/8], Step [363/750], Loss: 0.0613\n",
      "Epoch [2/8], Step [364/750], Loss: 0.0923\n",
      "Epoch [2/8], Step [365/750], Loss: 0.0576\n",
      "Epoch [2/8], Step [366/750], Loss: 0.0874\n",
      "Epoch [2/8], Step [367/750], Loss: 0.0768\n",
      "Epoch [2/8], Step [368/750], Loss: 0.0064\n",
      "Epoch [2/8], Step [369/750], Loss: 0.0122\n",
      "Epoch [2/8], Step [370/750], Loss: 0.0582\n",
      "Epoch [2/8], Step [371/750], Loss: 0.1412\n",
      "Epoch [2/8], Step [372/750], Loss: 0.1748\n",
      "Epoch [2/8], Step [373/750], Loss: 0.1569\n",
      "Epoch [2/8], Step [374/750], Loss: 0.0227\n",
      "Epoch [2/8], Step [375/750], Loss: 0.0158\n",
      "Epoch [2/8], Step [376/750], Loss: 0.0825\n",
      "Epoch [2/8], Step [377/750], Loss: 0.0087\n",
      "Epoch [2/8], Step [378/750], Loss: 0.0193\n",
      "Epoch [2/8], Step [379/750], Loss: 0.0684\n",
      "Epoch [2/8], Step [380/750], Loss: 0.0549\n",
      "Epoch [2/8], Step [381/750], Loss: 0.0368\n",
      "Epoch [2/8], Step [382/750], Loss: 0.0273\n",
      "Epoch [2/8], Step [383/750], Loss: 0.0198\n",
      "Epoch [2/8], Step [384/750], Loss: 0.1572\n",
      "Epoch [2/8], Step [385/750], Loss: 0.1108\n",
      "Epoch [2/8], Step [386/750], Loss: 0.1430\n",
      "Epoch [2/8], Step [387/750], Loss: 0.0187\n",
      "Epoch [2/8], Step [388/750], Loss: 0.0191\n",
      "Epoch [2/8], Step [389/750], Loss: 0.0242\n",
      "Epoch [2/8], Step [390/750], Loss: 0.1015\n",
      "Epoch [2/8], Step [391/750], Loss: 0.1652\n",
      "Epoch [2/8], Step [392/750], Loss: 0.0180\n",
      "Epoch [2/8], Step [393/750], Loss: 0.1103\n",
      "Epoch [2/8], Step [394/750], Loss: 0.1185\n",
      "Epoch [2/8], Step [395/750], Loss: 0.0162\n",
      "Epoch [2/8], Step [396/750], Loss: 0.1254\n",
      "Epoch [2/8], Step [397/750], Loss: 0.1008\n",
      "Epoch [2/8], Step [398/750], Loss: 0.0306\n",
      "Epoch [2/8], Step [399/750], Loss: 0.1069\n",
      "Epoch [2/8], Step [400/750], Loss: 0.0494\n",
      "Epoch [2/8], Step [401/750], Loss: 0.1392\n",
      "Epoch [2/8], Step [402/750], Loss: 0.0026\n",
      "Epoch [2/8], Step [403/750], Loss: 0.0678\n",
      "Epoch [2/8], Step [404/750], Loss: 0.0869\n",
      "Epoch [2/8], Step [405/750], Loss: 0.0241\n",
      "Epoch [2/8], Step [406/750], Loss: 0.0177\n",
      "Epoch [2/8], Step [407/750], Loss: 0.0743\n",
      "Epoch [2/8], Step [408/750], Loss: 0.0440\n",
      "Epoch [2/8], Step [409/750], Loss: 0.0083\n",
      "Epoch [2/8], Step [410/750], Loss: 0.0600\n",
      "Epoch [2/8], Step [411/750], Loss: 0.0370\n",
      "Epoch [2/8], Step [412/750], Loss: 0.0461\n",
      "Epoch [2/8], Step [413/750], Loss: 0.0231\n",
      "Epoch [2/8], Step [414/750], Loss: 0.0382\n",
      "Epoch [2/8], Step [415/750], Loss: 0.0233\n",
      "Epoch [2/8], Step [416/750], Loss: 0.0959\n",
      "Epoch [2/8], Step [417/750], Loss: 0.0180\n",
      "Epoch [2/8], Step [418/750], Loss: 0.0615\n",
      "Epoch [2/8], Step [419/750], Loss: 0.0378\n",
      "Epoch [2/8], Step [420/750], Loss: 0.0738\n",
      "Epoch [2/8], Step [421/750], Loss: 0.0493\n",
      "Epoch [2/8], Step [422/750], Loss: 0.0388\n",
      "Epoch [2/8], Step [423/750], Loss: 0.0406\n",
      "Epoch [2/8], Step [424/750], Loss: 0.0654\n",
      "Epoch [2/8], Step [425/750], Loss: 0.0056\n",
      "Epoch [2/8], Step [426/750], Loss: 0.0509\n",
      "Epoch [2/8], Step [427/750], Loss: 0.0045\n",
      "Epoch [2/8], Step [428/750], Loss: 0.0881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Step [429/750], Loss: 0.0119\n",
      "Epoch [2/8], Step [430/750], Loss: 0.0222\n",
      "Epoch [2/8], Step [431/750], Loss: 0.0658\n",
      "Epoch [2/8], Step [432/750], Loss: 0.0964\n",
      "Epoch [2/8], Step [433/750], Loss: 0.0998\n",
      "Epoch [2/8], Step [434/750], Loss: 0.2350\n",
      "Epoch [2/8], Step [435/750], Loss: 0.0044\n",
      "Epoch [2/8], Step [436/750], Loss: 0.0966\n",
      "Epoch [2/8], Step [437/750], Loss: 0.0525\n",
      "Epoch [2/8], Step [438/750], Loss: 0.0972\n",
      "Epoch [2/8], Step [439/750], Loss: 0.1060\n",
      "Epoch [2/8], Step [440/750], Loss: 0.0561\n",
      "Epoch [2/8], Step [441/750], Loss: 0.1072\n",
      "Epoch [2/8], Step [442/750], Loss: 0.1281\n",
      "Epoch [2/8], Step [443/750], Loss: 0.0036\n",
      "Epoch [2/8], Step [444/750], Loss: 0.1059\n",
      "Epoch [2/8], Step [445/750], Loss: 0.0241\n",
      "Epoch [2/8], Step [446/750], Loss: 0.1104\n",
      "Epoch [2/8], Step [447/750], Loss: 0.0309\n",
      "Epoch [2/8], Step [448/750], Loss: 0.1081\n",
      "Epoch [2/8], Step [449/750], Loss: 0.0215\n",
      "Epoch [2/8], Step [450/750], Loss: 0.1337\n",
      "Epoch [2/8], Step [451/750], Loss: 0.1360\n",
      "Epoch [2/8], Step [452/750], Loss: 0.0093\n",
      "Epoch [2/8], Step [453/750], Loss: 0.0275\n",
      "Epoch [2/8], Step [454/750], Loss: 0.0749\n",
      "Epoch [2/8], Step [455/750], Loss: 0.0449\n",
      "Epoch [2/8], Step [456/750], Loss: 0.0609\n",
      "Epoch [2/8], Step [457/750], Loss: 0.0363\n",
      "Epoch [2/8], Step [458/750], Loss: 0.1135\n",
      "Epoch [2/8], Step [459/750], Loss: 0.0057\n",
      "Epoch [2/8], Step [460/750], Loss: 0.0702\n",
      "Epoch [2/8], Step [461/750], Loss: 0.0493\n",
      "Epoch [2/8], Step [462/750], Loss: 0.0117\n",
      "Epoch [2/8], Step [463/750], Loss: 0.0180\n",
      "Epoch [2/8], Step [464/750], Loss: 0.0855\n",
      "Epoch [2/8], Step [465/750], Loss: 0.0175\n",
      "Epoch [2/8], Step [466/750], Loss: 0.0936\n",
      "Epoch [2/8], Step [467/750], Loss: 0.0373\n",
      "Epoch [2/8], Step [468/750], Loss: 0.0563\n",
      "Epoch [2/8], Step [469/750], Loss: 0.0921\n",
      "Epoch [2/8], Step [470/750], Loss: 0.0808\n",
      "Epoch [2/8], Step [471/750], Loss: 0.1242\n",
      "Epoch [2/8], Step [472/750], Loss: 0.0059\n",
      "Epoch [2/8], Step [473/750], Loss: 0.0257\n",
      "Epoch [2/8], Step [474/750], Loss: 0.0233\n",
      "Epoch [2/8], Step [475/750], Loss: 0.0465\n",
      "Epoch [2/8], Step [476/750], Loss: 0.0409\n",
      "Epoch [2/8], Step [477/750], Loss: 0.0855\n",
      "Epoch [2/8], Step [478/750], Loss: 0.0022\n",
      "Epoch [2/8], Step [479/750], Loss: 0.0047\n",
      "Epoch [2/8], Step [480/750], Loss: 0.0874\n",
      "Epoch [2/8], Step [481/750], Loss: 0.0144\n",
      "Epoch [2/8], Step [482/750], Loss: 0.0827\n",
      "Epoch [2/8], Step [483/750], Loss: 0.0684\n",
      "Epoch [2/8], Step [484/750], Loss: 0.0182\n",
      "Epoch [2/8], Step [485/750], Loss: 0.0592\n",
      "Epoch [2/8], Step [486/750], Loss: 0.0046\n",
      "Epoch [2/8], Step [487/750], Loss: 0.1641\n",
      "Epoch [2/8], Step [488/750], Loss: 0.0756\n",
      "Epoch [2/8], Step [489/750], Loss: 0.0738\n",
      "Epoch [2/8], Step [490/750], Loss: 0.0212\n",
      "Epoch [2/8], Step [491/750], Loss: 0.0230\n",
      "Epoch [2/8], Step [492/750], Loss: 0.0213\n",
      "Epoch [2/8], Step [493/750], Loss: 0.0566\n",
      "Epoch [2/8], Step [494/750], Loss: 0.0109\n",
      "Epoch [2/8], Step [495/750], Loss: 0.0724\n",
      "Epoch [2/8], Step [496/750], Loss: 0.1235\n",
      "Epoch [2/8], Step [497/750], Loss: 0.1035\n",
      "Epoch [2/8], Step [498/750], Loss: 0.0970\n",
      "Epoch [2/8], Step [499/750], Loss: 0.0068\n",
      "Epoch [2/8], Step [500/750], Loss: 0.0086\n",
      "Epoch [2/8], Step [501/750], Loss: 0.0298\n",
      "Epoch [2/8], Step [502/750], Loss: 0.0036\n",
      "Epoch [2/8], Step [503/750], Loss: 0.0133\n",
      "Epoch [2/8], Step [504/750], Loss: 0.0466\n",
      "Epoch [2/8], Step [505/750], Loss: 0.0604\n",
      "Epoch [2/8], Step [506/750], Loss: 0.1160\n",
      "Epoch [2/8], Step [507/750], Loss: 0.0373\n",
      "Epoch [2/8], Step [508/750], Loss: 0.1068\n",
      "Epoch [2/8], Step [509/750], Loss: 0.0549\n",
      "Epoch [2/8], Step [510/750], Loss: 0.0758\n",
      "Epoch [2/8], Step [511/750], Loss: 0.1371\n",
      "Epoch [2/8], Step [512/750], Loss: 0.0039\n",
      "Epoch [2/8], Step [513/750], Loss: 0.0092\n",
      "Epoch [2/8], Step [514/750], Loss: 0.1120\n",
      "Epoch [2/8], Step [515/750], Loss: 0.1681\n",
      "Epoch [2/8], Step [516/750], Loss: 0.0371\n",
      "Epoch [2/8], Step [517/750], Loss: 0.0676\n",
      "Epoch [2/8], Step [518/750], Loss: 0.0330\n",
      "Epoch [2/8], Step [519/750], Loss: 0.0448\n",
      "Epoch [2/8], Step [520/750], Loss: 0.1385\n",
      "Epoch [2/8], Step [521/750], Loss: 0.1537\n",
      "Epoch [2/8], Step [522/750], Loss: 0.1660\n",
      "Epoch [2/8], Step [523/750], Loss: 0.0073\n",
      "Epoch [2/8], Step [524/750], Loss: 0.0536\n",
      "Epoch [2/8], Step [525/750], Loss: 0.0117\n",
      "Epoch [2/8], Step [526/750], Loss: 0.0119\n",
      "Epoch [2/8], Step [527/750], Loss: 0.1522\n",
      "Epoch [2/8], Step [528/750], Loss: 0.1063\n",
      "Epoch [2/8], Step [529/750], Loss: 0.0394\n",
      "Epoch [2/8], Step [530/750], Loss: 0.0414\n",
      "Epoch [2/8], Step [531/750], Loss: 0.0221\n",
      "Epoch [2/8], Step [532/750], Loss: 0.1042\n",
      "Epoch [2/8], Step [533/750], Loss: 0.0230\n",
      "Epoch [2/8], Step [534/750], Loss: 0.0541\n",
      "Epoch [2/8], Step [535/750], Loss: 0.0428\n",
      "Epoch [2/8], Step [536/750], Loss: 0.0642\n",
      "Epoch [2/8], Step [537/750], Loss: 0.1613\n",
      "Epoch [2/8], Step [538/750], Loss: 0.1137\n",
      "Epoch [2/8], Step [539/750], Loss: 0.0500\n",
      "Epoch [2/8], Step [540/750], Loss: 0.0284\n",
      "Epoch [2/8], Step [541/750], Loss: 0.1075\n",
      "Epoch [2/8], Step [542/750], Loss: 0.0756\n",
      "Epoch [2/8], Step [543/750], Loss: 0.1027\n",
      "Epoch [2/8], Step [544/750], Loss: 0.0279\n",
      "Epoch [2/8], Step [545/750], Loss: 0.0087\n",
      "Epoch [2/8], Step [546/750], Loss: 0.0235\n",
      "Epoch [2/8], Step [547/750], Loss: 0.0538\n",
      "Epoch [2/8], Step [548/750], Loss: 0.0766\n",
      "Epoch [2/8], Step [549/750], Loss: 0.1334\n",
      "Epoch [2/8], Step [550/750], Loss: 0.0378\n",
      "Epoch [2/8], Step [551/750], Loss: 0.0258\n",
      "Epoch [2/8], Step [552/750], Loss: 0.0953\n",
      "Epoch [2/8], Step [553/750], Loss: 0.0539\n",
      "Epoch [2/8], Step [554/750], Loss: 0.1018\n",
      "Epoch [2/8], Step [555/750], Loss: 0.0245\n",
      "Epoch [2/8], Step [556/750], Loss: 0.0129\n",
      "Epoch [2/8], Step [557/750], Loss: 0.0267\n",
      "Epoch [2/8], Step [558/750], Loss: 0.0317\n",
      "Epoch [2/8], Step [559/750], Loss: 0.0121\n",
      "Epoch [2/8], Step [560/750], Loss: 0.0203\n",
      "Epoch [2/8], Step [561/750], Loss: 0.0539\n",
      "Epoch [2/8], Step [562/750], Loss: 0.0075\n",
      "Epoch [2/8], Step [563/750], Loss: 0.0899\n",
      "Epoch [2/8], Step [564/750], Loss: 0.0600\n",
      "Epoch [2/8], Step [565/750], Loss: 0.0326\n",
      "Epoch [2/8], Step [566/750], Loss: 0.0539\n",
      "Epoch [2/8], Step [567/750], Loss: 0.0199\n",
      "Epoch [2/8], Step [568/750], Loss: 0.0436\n",
      "Epoch [2/8], Step [569/750], Loss: 0.0260\n",
      "Epoch [2/8], Step [570/750], Loss: 0.0104\n",
      "Epoch [2/8], Step [571/750], Loss: 0.0118\n",
      "Epoch [2/8], Step [572/750], Loss: 0.0728\n",
      "Epoch [2/8], Step [573/750], Loss: 0.0830\n",
      "Epoch [2/8], Step [574/750], Loss: 0.0104\n",
      "Epoch [2/8], Step [575/750], Loss: 0.0469\n",
      "Epoch [2/8], Step [576/750], Loss: 0.0627\n",
      "Epoch [2/8], Step [577/750], Loss: 0.1367\n",
      "Epoch [2/8], Step [578/750], Loss: 0.0250\n",
      "Epoch [2/8], Step [579/750], Loss: 0.0710\n",
      "Epoch [2/8], Step [580/750], Loss: 0.0093\n",
      "Epoch [2/8], Step [581/750], Loss: 0.0280\n",
      "Epoch [2/8], Step [582/750], Loss: 0.0488\n",
      "Epoch [2/8], Step [583/750], Loss: 0.1614\n",
      "Epoch [2/8], Step [584/750], Loss: 0.1708\n",
      "Epoch [2/8], Step [585/750], Loss: 0.0205\n",
      "Epoch [2/8], Step [586/750], Loss: 0.0303\n",
      "Epoch [2/8], Step [587/750], Loss: 0.0188\n",
      "Epoch [2/8], Step [588/750], Loss: 0.0268\n",
      "Epoch [2/8], Step [589/750], Loss: 0.0249\n",
      "Epoch [2/8], Step [590/750], Loss: 0.0545\n",
      "Epoch [2/8], Step [591/750], Loss: 0.0417\n",
      "Epoch [2/8], Step [592/750], Loss: 0.0767\n",
      "Epoch [2/8], Step [593/750], Loss: 0.0270\n",
      "Epoch [2/8], Step [594/750], Loss: 0.0076\n",
      "Epoch [2/8], Step [595/750], Loss: 0.0125\n",
      "Epoch [2/8], Step [596/750], Loss: 0.0169\n",
      "Epoch [2/8], Step [597/750], Loss: 0.0172\n",
      "Epoch [2/8], Step [598/750], Loss: 0.0789\n",
      "Epoch [2/8], Step [599/750], Loss: 0.0466\n",
      "Epoch [2/8], Step [600/750], Loss: 0.0512\n",
      "Epoch [2/8], Step [601/750], Loss: 0.1156\n",
      "Epoch [2/8], Step [602/750], Loss: 0.1128\n",
      "Epoch [2/8], Step [603/750], Loss: 0.0364\n",
      "Epoch [2/8], Step [604/750], Loss: 0.0859\n",
      "Epoch [2/8], Step [605/750], Loss: 0.0019\n",
      "Epoch [2/8], Step [606/750], Loss: 0.0141\n",
      "Epoch [2/8], Step [607/750], Loss: 0.0406\n",
      "Epoch [2/8], Step [608/750], Loss: 0.0701\n",
      "Epoch [2/8], Step [609/750], Loss: 0.0206\n",
      "Epoch [2/8], Step [610/750], Loss: 0.0439\n",
      "Epoch [2/8], Step [611/750], Loss: 0.0140\n",
      "Epoch [2/8], Step [612/750], Loss: 0.1079\n",
      "Epoch [2/8], Step [613/750], Loss: 0.0713\n",
      "Epoch [2/8], Step [614/750], Loss: 0.0043\n",
      "Epoch [2/8], Step [615/750], Loss: 0.0042\n",
      "Epoch [2/8], Step [616/750], Loss: 0.0751\n",
      "Epoch [2/8], Step [617/750], Loss: 0.0872\n",
      "Epoch [2/8], Step [618/750], Loss: 0.0630\n",
      "Epoch [2/8], Step [619/750], Loss: 0.0781\n",
      "Epoch [2/8], Step [620/750], Loss: 0.0234\n",
      "Epoch [2/8], Step [621/750], Loss: 0.0032\n",
      "Epoch [2/8], Step [622/750], Loss: 0.0436\n",
      "Epoch [2/8], Step [623/750], Loss: 0.0964\n",
      "Epoch [2/8], Step [624/750], Loss: 0.1610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Step [625/750], Loss: 0.0117\n",
      "Epoch [2/8], Step [626/750], Loss: 0.0126\n",
      "Epoch [2/8], Step [627/750], Loss: 0.0520\n",
      "Epoch [2/8], Step [628/750], Loss: 0.0069\n",
      "Epoch [2/8], Step [629/750], Loss: 0.0995\n",
      "Epoch [2/8], Step [630/750], Loss: 0.1095\n",
      "Epoch [2/8], Step [631/750], Loss: 0.1646\n",
      "Epoch [2/8], Step [632/750], Loss: 0.0273\n",
      "Epoch [2/8], Step [633/750], Loss: 0.0114\n",
      "Epoch [2/8], Step [634/750], Loss: 0.0364\n",
      "Epoch [2/8], Step [635/750], Loss: 0.0305\n",
      "Epoch [2/8], Step [636/750], Loss: 0.1275\n",
      "Epoch [2/8], Step [637/750], Loss: 0.3158\n",
      "Epoch [2/8], Step [638/750], Loss: 0.0179\n",
      "Epoch [2/8], Step [639/750], Loss: 0.0468\n",
      "Epoch [2/8], Step [640/750], Loss: 0.0453\n",
      "Epoch [2/8], Step [641/750], Loss: 0.0739\n",
      "Epoch [2/8], Step [642/750], Loss: 0.0988\n",
      "Epoch [2/8], Step [643/750], Loss: 0.0221\n",
      "Epoch [2/8], Step [644/750], Loss: 0.0729\n",
      "Epoch [2/8], Step [645/750], Loss: 0.1425\n",
      "Epoch [2/8], Step [646/750], Loss: 0.0422\n",
      "Epoch [2/8], Step [647/750], Loss: 0.0594\n",
      "Epoch [2/8], Step [648/750], Loss: 0.0318\n",
      "Epoch [2/8], Step [649/750], Loss: 0.0257\n",
      "Epoch [2/8], Step [650/750], Loss: 0.0933\n",
      "Epoch [2/8], Step [651/750], Loss: 0.0777\n",
      "Epoch [2/8], Step [652/750], Loss: 0.0300\n",
      "Epoch [2/8], Step [653/750], Loss: 0.0326\n",
      "Epoch [2/8], Step [654/750], Loss: 0.0242\n",
      "Epoch [2/8], Step [655/750], Loss: 0.0158\n",
      "Epoch [2/8], Step [656/750], Loss: 0.0198\n",
      "Epoch [2/8], Step [657/750], Loss: 0.0152\n",
      "Epoch [2/8], Step [658/750], Loss: 0.1557\n",
      "Epoch [2/8], Step [659/750], Loss: 0.0167\n",
      "Epoch [2/8], Step [660/750], Loss: 0.0197\n",
      "Epoch [2/8], Step [661/750], Loss: 0.1031\n",
      "Epoch [2/8], Step [662/750], Loss: 0.0279\n",
      "Epoch [2/8], Step [663/750], Loss: 0.0115\n",
      "Epoch [2/8], Step [664/750], Loss: 0.0491\n",
      "Epoch [2/8], Step [665/750], Loss: 0.0386\n",
      "Epoch [2/8], Step [666/750], Loss: 0.0165\n",
      "Epoch [2/8], Step [667/750], Loss: 0.0069\n",
      "Epoch [2/8], Step [668/750], Loss: 0.0939\n",
      "Epoch [2/8], Step [669/750], Loss: 0.0031\n",
      "Epoch [2/8], Step [670/750], Loss: 0.0994\n",
      "Epoch [2/8], Step [671/750], Loss: 0.0713\n",
      "Epoch [2/8], Step [672/750], Loss: 0.0487\n",
      "Epoch [2/8], Step [673/750], Loss: 0.0909\n",
      "Epoch [2/8], Step [674/750], Loss: 0.0090\n",
      "Epoch [2/8], Step [675/750], Loss: 0.0024\n",
      "Epoch [2/8], Step [676/750], Loss: 0.0282\n",
      "Epoch [2/8], Step [677/750], Loss: 0.0486\n",
      "Epoch [2/8], Step [678/750], Loss: 0.0225\n",
      "Epoch [2/8], Step [679/750], Loss: 0.0618\n",
      "Epoch [2/8], Step [680/750], Loss: 0.0068\n",
      "Epoch [2/8], Step [681/750], Loss: 0.0719\n",
      "Epoch [2/8], Step [682/750], Loss: 0.0503\n",
      "Epoch [2/8], Step [683/750], Loss: 0.1145\n",
      "Epoch [2/8], Step [684/750], Loss: 0.0924\n",
      "Epoch [2/8], Step [685/750], Loss: 0.0310\n",
      "Epoch [2/8], Step [686/750], Loss: 0.0149\n",
      "Epoch [2/8], Step [687/750], Loss: 0.0043\n",
      "Epoch [2/8], Step [688/750], Loss: 0.0637\n",
      "Epoch [2/8], Step [689/750], Loss: 0.0938\n",
      "Epoch [2/8], Step [690/750], Loss: 0.0349\n",
      "Epoch [2/8], Step [691/750], Loss: 0.0077\n",
      "Epoch [2/8], Step [692/750], Loss: 0.0892\n",
      "Epoch [2/8], Step [693/750], Loss: 0.0960\n",
      "Epoch [2/8], Step [694/750], Loss: 0.0062\n",
      "Epoch [2/8], Step [695/750], Loss: 0.0580\n",
      "Epoch [2/8], Step [696/750], Loss: 0.0622\n",
      "Epoch [2/8], Step [697/750], Loss: 0.0068\n",
      "Epoch [2/8], Step [698/750], Loss: 0.0449\n",
      "Epoch [2/8], Step [699/750], Loss: 0.0270\n",
      "Epoch [2/8], Step [700/750], Loss: 0.0607\n",
      "Epoch [2/8], Step [701/750], Loss: 0.0386\n",
      "Epoch [2/8], Step [702/750], Loss: 0.1235\n",
      "Epoch [2/8], Step [703/750], Loss: 0.0140\n",
      "Epoch [2/8], Step [704/750], Loss: 0.1245\n",
      "Epoch [2/8], Step [705/750], Loss: 0.0179\n",
      "Epoch [2/8], Step [706/750], Loss: 0.2134\n",
      "Epoch [2/8], Step [707/750], Loss: 0.0127\n",
      "Epoch [2/8], Step [708/750], Loss: 0.2000\n",
      "Epoch [2/8], Step [709/750], Loss: 0.0331\n",
      "Epoch [2/8], Step [710/750], Loss: 0.0075\n",
      "Epoch [2/8], Step [711/750], Loss: 0.0317\n",
      "Epoch [2/8], Step [712/750], Loss: 0.0138\n",
      "Epoch [2/8], Step [713/750], Loss: 0.0087\n",
      "Epoch [2/8], Step [714/750], Loss: 0.0304\n",
      "Epoch [2/8], Step [715/750], Loss: 0.0554\n",
      "Epoch [2/8], Step [716/750], Loss: 0.0058\n",
      "Epoch [2/8], Step [717/750], Loss: 0.0139\n",
      "Epoch [2/8], Step [718/750], Loss: 0.0252\n",
      "Epoch [2/8], Step [719/750], Loss: 0.0288\n",
      "Epoch [2/8], Step [720/750], Loss: 0.0415\n",
      "Epoch [2/8], Step [721/750], Loss: 0.0240\n",
      "Epoch [2/8], Step [722/750], Loss: 0.0792\n",
      "Epoch [2/8], Step [723/750], Loss: 0.0017\n",
      "Epoch [2/8], Step [724/750], Loss: 0.0303\n",
      "Epoch [2/8], Step [725/750], Loss: 0.0829\n",
      "Epoch [2/8], Step [726/750], Loss: 0.1072\n",
      "Epoch [2/8], Step [727/750], Loss: 0.0041\n",
      "Epoch [2/8], Step [728/750], Loss: 0.0623\n",
      "Epoch [2/8], Step [729/750], Loss: 0.0238\n",
      "Epoch [2/8], Step [730/750], Loss: 0.0280\n",
      "Epoch [2/8], Step [731/750], Loss: 0.0664\n",
      "Epoch [2/8], Step [732/750], Loss: 0.0152\n",
      "Epoch [2/8], Step [733/750], Loss: 0.0059\n",
      "Epoch [2/8], Step [734/750], Loss: 0.0917\n",
      "Epoch [2/8], Step [735/750], Loss: 0.1602\n",
      "Epoch [2/8], Step [736/750], Loss: 0.0297\n",
      "Epoch [2/8], Step [737/750], Loss: 0.0562\n",
      "Epoch [2/8], Step [738/750], Loss: 0.0064\n",
      "Epoch [2/8], Step [739/750], Loss: 0.1115\n",
      "Epoch [2/8], Step [740/750], Loss: 0.0143\n",
      "Epoch [2/8], Step [741/750], Loss: 0.0644\n",
      "Epoch [2/8], Step [742/750], Loss: 0.0182\n",
      "Epoch [2/8], Step [743/750], Loss: 0.0902\n",
      "Epoch [2/8], Step [744/750], Loss: 0.0714\n",
      "Epoch [2/8], Step [745/750], Loss: 0.0280\n",
      "Epoch [2/8], Step [746/750], Loss: 0.2245\n",
      "Epoch [2/8], Step [747/750], Loss: 0.0678\n",
      "Epoch [2/8], Step [748/750], Loss: 0.0087\n",
      "Epoch [2/8], Step [749/750], Loss: 0.0445\n",
      "Epoch [2/8], Step [750/750], Loss: 0.1154\n",
      "Epoch [2/8], Tr. loss: 0.2222. Test loss: 0.1058\n",
      "\n",
      "\n",
      "Epoch [3/8], Step [1/750], Loss: 0.0076\n",
      "Epoch [3/8], Step [2/750], Loss: 0.0986\n",
      "Epoch [3/8], Step [3/750], Loss: 0.0577\n",
      "Epoch [3/8], Step [4/750], Loss: 0.0653\n",
      "Epoch [3/8], Step [5/750], Loss: 0.0125\n",
      "Epoch [3/8], Step [6/750], Loss: 0.0064\n",
      "Epoch [3/8], Step [7/750], Loss: 0.0032\n",
      "Epoch [3/8], Step [8/750], Loss: 0.0062\n",
      "Epoch [3/8], Step [9/750], Loss: 0.0932\n",
      "Epoch [3/8], Step [10/750], Loss: 0.0073\n",
      "Epoch [3/8], Step [11/750], Loss: 0.1100\n",
      "Epoch [3/8], Step [12/750], Loss: 0.0410\n",
      "Epoch [3/8], Step [13/750], Loss: 0.0100\n",
      "Epoch [3/8], Step [14/750], Loss: 0.0363\n",
      "Epoch [3/8], Step [15/750], Loss: 0.0277\n",
      "Epoch [3/8], Step [16/750], Loss: 0.0991\n",
      "Epoch [3/8], Step [17/750], Loss: 0.0503\n",
      "Epoch [3/8], Step [18/750], Loss: 0.0305\n",
      "Epoch [3/8], Step [19/750], Loss: 0.0137\n",
      "Epoch [3/8], Step [20/750], Loss: 0.0160\n",
      "Epoch [3/8], Step [21/750], Loss: 0.0565\n",
      "Epoch [3/8], Step [22/750], Loss: 0.0042\n",
      "Epoch [3/8], Step [23/750], Loss: 0.0173\n",
      "Epoch [3/8], Step [24/750], Loss: 0.0414\n",
      "Epoch [3/8], Step [25/750], Loss: 0.0079\n",
      "Epoch [3/8], Step [26/750], Loss: 0.0126\n",
      "Epoch [3/8], Step [27/750], Loss: 0.0448\n",
      "Epoch [3/8], Step [28/750], Loss: 0.0200\n",
      "Epoch [3/8], Step [29/750], Loss: 0.0343\n",
      "Epoch [3/8], Step [30/750], Loss: 0.0777\n",
      "Epoch [3/8], Step [31/750], Loss: 0.0151\n",
      "Epoch [3/8], Step [32/750], Loss: 0.0134\n",
      "Epoch [3/8], Step [33/750], Loss: 0.0584\n",
      "Epoch [3/8], Step [34/750], Loss: 0.0045\n",
      "Epoch [3/8], Step [35/750], Loss: 0.0451\n",
      "Epoch [3/8], Step [36/750], Loss: 0.0165\n",
      "Epoch [3/8], Step [37/750], Loss: 0.0074\n",
      "Epoch [3/8], Step [38/750], Loss: 0.0018\n",
      "Epoch [3/8], Step [39/750], Loss: 0.1314\n",
      "Epoch [3/8], Step [40/750], Loss: 0.0140\n",
      "Epoch [3/8], Step [41/750], Loss: 0.0075\n",
      "Epoch [3/8], Step [42/750], Loss: 0.0490\n",
      "Epoch [3/8], Step [43/750], Loss: 0.0076\n",
      "Epoch [3/8], Step [44/750], Loss: 0.0119\n",
      "Epoch [3/8], Step [45/750], Loss: 0.0258\n",
      "Epoch [3/8], Step [46/750], Loss: 0.0124\n",
      "Epoch [3/8], Step [47/750], Loss: 0.0108\n",
      "Epoch [3/8], Step [48/750], Loss: 0.0179\n",
      "Epoch [3/8], Step [49/750], Loss: 0.0409\n",
      "Epoch [3/8], Step [50/750], Loss: 0.0045\n",
      "Epoch [3/8], Step [51/750], Loss: 0.0648\n",
      "Epoch [3/8], Step [52/750], Loss: 0.0110\n",
      "Epoch [3/8], Step [53/750], Loss: 0.0806\n",
      "Epoch [3/8], Step [54/750], Loss: 0.1266\n",
      "Epoch [3/8], Step [55/750], Loss: 0.0032\n",
      "Epoch [3/8], Step [56/750], Loss: 0.0204\n",
      "Epoch [3/8], Step [57/750], Loss: 0.0098\n",
      "Epoch [3/8], Step [58/750], Loss: 0.0445\n",
      "Epoch [3/8], Step [59/750], Loss: 0.0244\n",
      "Epoch [3/8], Step [60/750], Loss: 0.0263\n",
      "Epoch [3/8], Step [61/750], Loss: 0.0204\n",
      "Epoch [3/8], Step [62/750], Loss: 0.0734\n",
      "Epoch [3/8], Step [63/750], Loss: 0.0213\n",
      "Epoch [3/8], Step [64/750], Loss: 0.0567\n",
      "Epoch [3/8], Step [65/750], Loss: 0.1202\n",
      "Epoch [3/8], Step [66/750], Loss: 0.0170\n",
      "Epoch [3/8], Step [67/750], Loss: 0.1234\n",
      "Epoch [3/8], Step [68/750], Loss: 0.1315\n",
      "Epoch [3/8], Step [69/750], Loss: 0.0188\n",
      "Epoch [3/8], Step [70/750], Loss: 0.0732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], Step [71/750], Loss: 0.0710\n",
      "Epoch [3/8], Step [72/750], Loss: 0.0196\n",
      "Epoch [3/8], Step [73/750], Loss: 0.0810\n",
      "Epoch [3/8], Step [74/750], Loss: 0.0908\n",
      "Epoch [3/8], Step [75/750], Loss: 0.0066\n",
      "Epoch [3/8], Step [76/750], Loss: 0.0126\n",
      "Epoch [3/8], Step [77/750], Loss: 0.0643\n",
      "Epoch [3/8], Step [78/750], Loss: 0.0358\n",
      "Epoch [3/8], Step [79/750], Loss: 0.0231\n",
      "Epoch [3/8], Step [80/750], Loss: 0.0182\n",
      "Epoch [3/8], Step [81/750], Loss: 0.0366\n",
      "Epoch [3/8], Step [82/750], Loss: 0.0481\n",
      "Epoch [3/8], Step [83/750], Loss: 0.0088\n",
      "Epoch [3/8], Step [84/750], Loss: 0.0133\n",
      "Epoch [3/8], Step [85/750], Loss: 0.0219\n",
      "Epoch [3/8], Step [86/750], Loss: 0.0627\n",
      "Epoch [3/8], Step [87/750], Loss: 0.0236\n",
      "Epoch [3/8], Step [88/750], Loss: 0.0501\n",
      "Epoch [3/8], Step [89/750], Loss: 0.0114\n",
      "Epoch [3/8], Step [90/750], Loss: 0.0122\n",
      "Epoch [3/8], Step [91/750], Loss: 0.0294\n",
      "Epoch [3/8], Step [92/750], Loss: 0.0194\n",
      "Epoch [3/8], Step [93/750], Loss: 0.1052\n",
      "Epoch [3/8], Step [94/750], Loss: 0.0089\n",
      "Epoch [3/8], Step [95/750], Loss: 0.0329\n",
      "Epoch [3/8], Step [96/750], Loss: 0.0106\n",
      "Epoch [3/8], Step [97/750], Loss: 0.0311\n",
      "Epoch [3/8], Step [98/750], Loss: 0.0339\n",
      "Epoch [3/8], Step [99/750], Loss: 0.0090\n",
      "Epoch [3/8], Step [100/750], Loss: 0.0466\n",
      "Epoch [3/8], Step [101/750], Loss: 0.0428\n",
      "Epoch [3/8], Step [102/750], Loss: 0.0793\n",
      "Epoch [3/8], Step [103/750], Loss: 0.0559\n",
      "Epoch [3/8], Step [104/750], Loss: 0.1034\n",
      "Epoch [3/8], Step [105/750], Loss: 0.0033\n",
      "Epoch [3/8], Step [106/750], Loss: 0.0202\n",
      "Epoch [3/8], Step [107/750], Loss: 0.0135\n",
      "Epoch [3/8], Step [108/750], Loss: 0.0491\n",
      "Epoch [3/8], Step [109/750], Loss: 0.0248\n",
      "Epoch [3/8], Step [110/750], Loss: 0.1196\n",
      "Epoch [3/8], Step [111/750], Loss: 0.0493\n",
      "Epoch [3/8], Step [112/750], Loss: 0.0139\n",
      "Epoch [3/8], Step [113/750], Loss: 0.0218\n",
      "Epoch [3/8], Step [114/750], Loss: 0.1208\n",
      "Epoch [3/8], Step [115/750], Loss: 0.0078\n",
      "Epoch [3/8], Step [116/750], Loss: 0.1032\n",
      "Epoch [3/8], Step [117/750], Loss: 0.0681\n",
      "Epoch [3/8], Step [118/750], Loss: 0.1673\n",
      "Epoch [3/8], Step [119/750], Loss: 0.0035\n",
      "Epoch [3/8], Step [120/750], Loss: 0.0432\n",
      "Epoch [3/8], Step [121/750], Loss: 0.0636\n",
      "Epoch [3/8], Step [122/750], Loss: 0.1426\n",
      "Epoch [3/8], Step [123/750], Loss: 0.0364\n",
      "Epoch [3/8], Step [124/750], Loss: 0.0305\n",
      "Epoch [3/8], Step [125/750], Loss: 0.0173\n",
      "Epoch [3/8], Step [126/750], Loss: 0.0357\n",
      "Epoch [3/8], Step [127/750], Loss: 0.0506\n",
      "Epoch [3/8], Step [128/750], Loss: 0.0561\n",
      "Epoch [3/8], Step [129/750], Loss: 0.1352\n",
      "Epoch [3/8], Step [130/750], Loss: 0.0198\n",
      "Epoch [3/8], Step [131/750], Loss: 0.0504\n",
      "Epoch [3/8], Step [132/750], Loss: 0.0830\n",
      "Epoch [3/8], Step [133/750], Loss: 0.0685\n",
      "Epoch [3/8], Step [134/750], Loss: 0.0139\n",
      "Epoch [3/8], Step [135/750], Loss: 0.0560\n",
      "Epoch [3/8], Step [136/750], Loss: 0.0688\n",
      "Epoch [3/8], Step [137/750], Loss: 0.0177\n",
      "Epoch [3/8], Step [138/750], Loss: 0.0287\n",
      "Epoch [3/8], Step [139/750], Loss: 0.0476\n",
      "Epoch [3/8], Step [140/750], Loss: 0.0348\n",
      "Epoch [3/8], Step [141/750], Loss: 0.0108\n",
      "Epoch [3/8], Step [142/750], Loss: 0.0310\n",
      "Epoch [3/8], Step [143/750], Loss: 0.0293\n",
      "Epoch [3/8], Step [144/750], Loss: 0.0719\n",
      "Epoch [3/8], Step [145/750], Loss: 0.0016\n",
      "Epoch [3/8], Step [146/750], Loss: 0.0283\n",
      "Epoch [3/8], Step [147/750], Loss: 0.0198\n",
      "Epoch [3/8], Step [148/750], Loss: 0.0578\n",
      "Epoch [3/8], Step [149/750], Loss: 0.0114\n",
      "Epoch [3/8], Step [150/750], Loss: 0.0027\n",
      "Epoch [3/8], Step [151/750], Loss: 0.0543\n",
      "Epoch [3/8], Step [152/750], Loss: 0.0050\n",
      "Epoch [3/8], Step [153/750], Loss: 0.0182\n",
      "Epoch [3/8], Step [154/750], Loss: 0.0353\n",
      "Epoch [3/8], Step [155/750], Loss: 0.0690\n",
      "Epoch [3/8], Step [156/750], Loss: 0.0449\n",
      "Epoch [3/8], Step [157/750], Loss: 0.0354\n",
      "Epoch [3/8], Step [158/750], Loss: 0.0292\n",
      "Epoch [3/8], Step [159/750], Loss: 0.0182\n",
      "Epoch [3/8], Step [160/750], Loss: 0.0545\n",
      "Epoch [3/8], Step [161/750], Loss: 0.0080\n",
      "Epoch [3/8], Step [162/750], Loss: 0.0108\n",
      "Epoch [3/8], Step [163/750], Loss: 0.0175\n",
      "Epoch [3/8], Step [164/750], Loss: 0.0581\n",
      "Epoch [3/8], Step [165/750], Loss: 0.0719\n",
      "Epoch [3/8], Step [166/750], Loss: 0.0836\n",
      "Epoch [3/8], Step [167/750], Loss: 0.0750\n",
      "Epoch [3/8], Step [168/750], Loss: 0.0444\n",
      "Epoch [3/8], Step [169/750], Loss: 0.0460\n",
      "Epoch [3/8], Step [170/750], Loss: 0.0540\n",
      "Epoch [3/8], Step [171/750], Loss: 0.0218\n",
      "Epoch [3/8], Step [172/750], Loss: 0.0169\n",
      "Epoch [3/8], Step [173/750], Loss: 0.0226\n",
      "Epoch [3/8], Step [174/750], Loss: 0.1621\n",
      "Epoch [3/8], Step [175/750], Loss: 0.0072\n",
      "Epoch [3/8], Step [176/750], Loss: 0.0180\n",
      "Epoch [3/8], Step [177/750], Loss: 0.0859\n",
      "Epoch [3/8], Step [178/750], Loss: 0.0083\n",
      "Epoch [3/8], Step [179/750], Loss: 0.0054\n",
      "Epoch [3/8], Step [180/750], Loss: 0.0054\n",
      "Epoch [3/8], Step [181/750], Loss: 0.0164\n",
      "Epoch [3/8], Step [182/750], Loss: 0.0395\n",
      "Epoch [3/8], Step [183/750], Loss: 0.0089\n",
      "Epoch [3/8], Step [184/750], Loss: 0.0503\n",
      "Epoch [3/8], Step [185/750], Loss: 0.0584\n",
      "Epoch [3/8], Step [186/750], Loss: 0.0436\n",
      "Epoch [3/8], Step [187/750], Loss: 0.0183\n",
      "Epoch [3/8], Step [188/750], Loss: 0.0204\n",
      "Epoch [3/8], Step [189/750], Loss: 0.0629\n",
      "Epoch [3/8], Step [190/750], Loss: 0.0203\n",
      "Epoch [3/8], Step [191/750], Loss: 0.0172\n",
      "Epoch [3/8], Step [192/750], Loss: 0.0570\n",
      "Epoch [3/8], Step [193/750], Loss: 0.0130\n",
      "Epoch [3/8], Step [194/750], Loss: 0.0073\n",
      "Epoch [3/8], Step [195/750], Loss: 0.0110\n",
      "Epoch [3/8], Step [196/750], Loss: 0.0186\n",
      "Epoch [3/8], Step [197/750], Loss: 0.0886\n",
      "Epoch [3/8], Step [198/750], Loss: 0.0480\n",
      "Epoch [3/8], Step [199/750], Loss: 0.0047\n",
      "Epoch [3/8], Step [200/750], Loss: 0.0217\n",
      "Epoch [3/8], Step [201/750], Loss: 0.0055\n",
      "Epoch [3/8], Step [202/750], Loss: 0.0263\n",
      "Epoch [3/8], Step [203/750], Loss: 0.0032\n",
      "Epoch [3/8], Step [204/750], Loss: 0.1486\n",
      "Epoch [3/8], Step [205/750], Loss: 0.0092\n",
      "Epoch [3/8], Step [206/750], Loss: 0.0909\n",
      "Epoch [3/8], Step [207/750], Loss: 0.1440\n",
      "Epoch [3/8], Step [208/750], Loss: 0.0041\n",
      "Epoch [3/8], Step [209/750], Loss: 0.0878\n",
      "Epoch [3/8], Step [210/750], Loss: 0.0429\n",
      "Epoch [3/8], Step [211/750], Loss: 0.0851\n",
      "Epoch [3/8], Step [212/750], Loss: 0.0463\n",
      "Epoch [3/8], Step [213/750], Loss: 0.0191\n",
      "Epoch [3/8], Step [214/750], Loss: 0.0531\n",
      "Epoch [3/8], Step [215/750], Loss: 0.0859\n",
      "Epoch [3/8], Step [216/750], Loss: 0.0102\n",
      "Epoch [3/8], Step [217/750], Loss: 0.0324\n",
      "Epoch [3/8], Step [218/750], Loss: 0.0547\n",
      "Epoch [3/8], Step [219/750], Loss: 0.0242\n",
      "Epoch [3/8], Step [220/750], Loss: 0.0110\n",
      "Epoch [3/8], Step [221/750], Loss: 0.0285\n",
      "Epoch [3/8], Step [222/750], Loss: 0.0652\n",
      "Epoch [3/8], Step [223/750], Loss: 0.0371\n",
      "Epoch [3/8], Step [224/750], Loss: 0.0535\n",
      "Epoch [3/8], Step [225/750], Loss: 0.0141\n",
      "Epoch [3/8], Step [226/750], Loss: 0.0360\n",
      "Epoch [3/8], Step [227/750], Loss: 0.0671\n",
      "Epoch [3/8], Step [228/750], Loss: 0.0292\n",
      "Epoch [3/8], Step [229/750], Loss: 0.0118\n",
      "Epoch [3/8], Step [230/750], Loss: 0.0595\n",
      "Epoch [3/8], Step [231/750], Loss: 0.0375\n",
      "Epoch [3/8], Step [232/750], Loss: 0.0288\n",
      "Epoch [3/8], Step [233/750], Loss: 0.0715\n",
      "Epoch [3/8], Step [234/750], Loss: 0.0166\n",
      "Epoch [3/8], Step [235/750], Loss: 0.0977\n",
      "Epoch [3/8], Step [236/750], Loss: 0.0299\n",
      "Epoch [3/8], Step [237/750], Loss: 0.0292\n",
      "Epoch [3/8], Step [238/750], Loss: 0.0247\n",
      "Epoch [3/8], Step [239/750], Loss: 0.0602\n",
      "Epoch [3/8], Step [240/750], Loss: 0.0652\n",
      "Epoch [3/8], Step [241/750], Loss: 0.0146\n",
      "Epoch [3/8], Step [242/750], Loss: 0.0725\n",
      "Epoch [3/8], Step [243/750], Loss: 0.0896\n",
      "Epoch [3/8], Step [244/750], Loss: 0.0548\n",
      "Epoch [3/8], Step [245/750], Loss: 0.0524\n",
      "Epoch [3/8], Step [246/750], Loss: 0.0205\n",
      "Epoch [3/8], Step [247/750], Loss: 0.0423\n",
      "Epoch [3/8], Step [248/750], Loss: 0.0915\n",
      "Epoch [3/8], Step [249/750], Loss: 0.0125\n",
      "Epoch [3/8], Step [250/750], Loss: 0.0392\n",
      "Epoch [3/8], Step [251/750], Loss: 0.1187\n",
      "Epoch [3/8], Step [252/750], Loss: 0.0696\n",
      "Epoch [3/8], Step [253/750], Loss: 0.0376\n",
      "Epoch [3/8], Step [254/750], Loss: 0.1240\n",
      "Epoch [3/8], Step [255/750], Loss: 0.0926\n",
      "Epoch [3/8], Step [256/750], Loss: 0.0361\n",
      "Epoch [3/8], Step [257/750], Loss: 0.0230\n",
      "Epoch [3/8], Step [258/750], Loss: 0.0049\n",
      "Epoch [3/8], Step [259/750], Loss: 0.1722\n",
      "Epoch [3/8], Step [260/750], Loss: 0.0697\n",
      "Epoch [3/8], Step [261/750], Loss: 0.0575\n",
      "Epoch [3/8], Step [262/750], Loss: 0.0402\n",
      "Epoch [3/8], Step [263/750], Loss: 0.1080\n",
      "Epoch [3/8], Step [264/750], Loss: 0.1000\n",
      "Epoch [3/8], Step [265/750], Loss: 0.0703\n",
      "Epoch [3/8], Step [266/750], Loss: 0.0882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], Step [267/750], Loss: 0.0279\n",
      "Epoch [3/8], Step [268/750], Loss: 0.0520\n",
      "Epoch [3/8], Step [269/750], Loss: 0.0342\n",
      "Epoch [3/8], Step [270/750], Loss: 0.0103\n",
      "Epoch [3/8], Step [271/750], Loss: 0.0033\n",
      "Epoch [3/8], Step [272/750], Loss: 0.0555\n",
      "Epoch [3/8], Step [273/750], Loss: 0.0326\n",
      "Epoch [3/8], Step [274/750], Loss: 0.2048\n",
      "Epoch [3/8], Step [275/750], Loss: 0.0206\n",
      "Epoch [3/8], Step [276/750], Loss: 0.1554\n",
      "Epoch [3/8], Step [277/750], Loss: 0.0984\n",
      "Epoch [3/8], Step [278/750], Loss: 0.0115\n",
      "Epoch [3/8], Step [279/750], Loss: 0.0375\n",
      "Epoch [3/8], Step [280/750], Loss: 0.0298\n",
      "Epoch [3/8], Step [281/750], Loss: 0.0474\n",
      "Epoch [3/8], Step [282/750], Loss: 0.0849\n",
      "Epoch [3/8], Step [283/750], Loss: 0.0143\n",
      "Epoch [3/8], Step [284/750], Loss: 0.0501\n",
      "Epoch [3/8], Step [285/750], Loss: 0.1040\n",
      "Epoch [3/8], Step [286/750], Loss: 0.0633\n",
      "Epoch [3/8], Step [287/750], Loss: 0.0176\n",
      "Epoch [3/8], Step [288/750], Loss: 0.0101\n",
      "Epoch [3/8], Step [289/750], Loss: 0.0247\n",
      "Epoch [3/8], Step [290/750], Loss: 0.0603\n",
      "Epoch [3/8], Step [291/750], Loss: 0.0391\n",
      "Epoch [3/8], Step [292/750], Loss: 0.0638\n",
      "Epoch [3/8], Step [293/750], Loss: 0.0201\n",
      "Epoch [3/8], Step [294/750], Loss: 0.0287\n",
      "Epoch [3/8], Step [295/750], Loss: 0.0197\n",
      "Epoch [3/8], Step [296/750], Loss: 0.0223\n",
      "Epoch [3/8], Step [297/750], Loss: 0.0656\n",
      "Epoch [3/8], Step [298/750], Loss: 0.0770\n",
      "Epoch [3/8], Step [299/750], Loss: 0.0062\n",
      "Epoch [3/8], Step [300/750], Loss: 0.0176\n",
      "Epoch [3/8], Step [301/750], Loss: 0.0230\n",
      "Epoch [3/8], Step [302/750], Loss: 0.0365\n",
      "Epoch [3/8], Step [303/750], Loss: 0.0930\n",
      "Epoch [3/8], Step [304/750], Loss: 0.1096\n",
      "Epoch [3/8], Step [305/750], Loss: 0.1340\n",
      "Epoch [3/8], Step [306/750], Loss: 0.0024\n",
      "Epoch [3/8], Step [307/750], Loss: 0.0135\n",
      "Epoch [3/8], Step [308/750], Loss: 0.1006\n",
      "Epoch [3/8], Step [309/750], Loss: 0.1111\n",
      "Epoch [3/8], Step [310/750], Loss: 0.0795\n",
      "Epoch [3/8], Step [311/750], Loss: 0.0520\n",
      "Epoch [3/8], Step [312/750], Loss: 0.0403\n",
      "Epoch [3/8], Step [313/750], Loss: 0.0047\n",
      "Epoch [3/8], Step [314/750], Loss: 0.0523\n",
      "Epoch [3/8], Step [315/750], Loss: 0.0320\n",
      "Epoch [3/8], Step [316/750], Loss: 0.0553\n",
      "Epoch [3/8], Step [317/750], Loss: 0.0210\n",
      "Epoch [3/8], Step [318/750], Loss: 0.0146\n",
      "Epoch [3/8], Step [319/750], Loss: 0.1029\n",
      "Epoch [3/8], Step [320/750], Loss: 0.0272\n",
      "Epoch [3/8], Step [321/750], Loss: 0.0556\n",
      "Epoch [3/8], Step [322/750], Loss: 0.0039\n",
      "Epoch [3/8], Step [323/750], Loss: 0.0141\n",
      "Epoch [3/8], Step [324/750], Loss: 0.0526\n",
      "Epoch [3/8], Step [325/750], Loss: 0.0523\n",
      "Epoch [3/8], Step [326/750], Loss: 0.0166\n",
      "Epoch [3/8], Step [327/750], Loss: 0.0041\n",
      "Epoch [3/8], Step [328/750], Loss: 0.0416\n",
      "Epoch [3/8], Step [329/750], Loss: 0.0231\n",
      "Epoch [3/8], Step [330/750], Loss: 0.0126\n",
      "Epoch [3/8], Step [331/750], Loss: 0.0337\n",
      "Epoch [3/8], Step [332/750], Loss: 0.0362\n",
      "Epoch [3/8], Step [333/750], Loss: 0.0126\n",
      "Epoch [3/8], Step [334/750], Loss: 0.0741\n",
      "Epoch [3/8], Step [335/750], Loss: 0.0394\n",
      "Epoch [3/8], Step [336/750], Loss: 0.0055\n",
      "Epoch [3/8], Step [337/750], Loss: 0.0170\n",
      "Epoch [3/8], Step [338/750], Loss: 0.0147\n",
      "Epoch [3/8], Step [339/750], Loss: 0.0202\n",
      "Epoch [3/8], Step [340/750], Loss: 0.0588\n",
      "Epoch [3/8], Step [341/750], Loss: 0.0399\n",
      "Epoch [3/8], Step [342/750], Loss: 0.1331\n",
      "Epoch [3/8], Step [343/750], Loss: 0.0268\n",
      "Epoch [3/8], Step [344/750], Loss: 0.0037\n",
      "Epoch [3/8], Step [345/750], Loss: 0.0281\n",
      "Epoch [3/8], Step [346/750], Loss: 0.0081\n",
      "Epoch [3/8], Step [347/750], Loss: 0.0289\n",
      "Epoch [3/8], Step [348/750], Loss: 0.0667\n",
      "Epoch [3/8], Step [349/750], Loss: 0.0365\n",
      "Epoch [3/8], Step [350/750], Loss: 0.1319\n",
      "Epoch [3/8], Step [351/750], Loss: 0.0187\n",
      "Epoch [3/8], Step [352/750], Loss: 0.0889\n",
      "Epoch [3/8], Step [353/750], Loss: 0.0078\n",
      "Epoch [3/8], Step [354/750], Loss: 0.0041\n",
      "Epoch [3/8], Step [355/750], Loss: 0.0928\n",
      "Epoch [3/8], Step [356/750], Loss: 0.0596\n",
      "Epoch [3/8], Step [357/750], Loss: 0.0514\n",
      "Epoch [3/8], Step [358/750], Loss: 0.0401\n",
      "Epoch [3/8], Step [359/750], Loss: 0.0509\n",
      "Epoch [3/8], Step [360/750], Loss: 0.0208\n",
      "Epoch [3/8], Step [361/750], Loss: 0.0462\n",
      "Epoch [3/8], Step [362/750], Loss: 0.0067\n",
      "Epoch [3/8], Step [363/750], Loss: 0.0311\n",
      "Epoch [3/8], Step [364/750], Loss: 0.0497\n",
      "Epoch [3/8], Step [365/750], Loss: 0.0054\n",
      "Epoch [3/8], Step [366/750], Loss: 0.0750\n",
      "Epoch [3/8], Step [367/750], Loss: 0.0104\n",
      "Epoch [3/8], Step [368/750], Loss: 0.0136\n",
      "Epoch [3/8], Step [369/750], Loss: 0.0155\n",
      "Epoch [3/8], Step [370/750], Loss: 0.0899\n",
      "Epoch [3/8], Step [371/750], Loss: 0.0185\n",
      "Epoch [3/8], Step [372/750], Loss: 0.0119\n",
      "Epoch [3/8], Step [373/750], Loss: 0.0062\n",
      "Epoch [3/8], Step [374/750], Loss: 0.0193\n",
      "Epoch [3/8], Step [375/750], Loss: 0.0068\n",
      "Epoch [3/8], Step [376/750], Loss: 0.0094\n",
      "Epoch [3/8], Step [377/750], Loss: 0.0218\n",
      "Epoch [3/8], Step [378/750], Loss: 0.0076\n",
      "Epoch [3/8], Step [379/750], Loss: 0.0693\n",
      "Epoch [3/8], Step [380/750], Loss: 0.0167\n",
      "Epoch [3/8], Step [381/750], Loss: 0.0441\n",
      "Epoch [3/8], Step [382/750], Loss: 0.0308\n",
      "Epoch [3/8], Step [383/750], Loss: 0.0482\n",
      "Epoch [3/8], Step [384/750], Loss: 0.0111\n",
      "Epoch [3/8], Step [385/750], Loss: 0.0299\n",
      "Epoch [3/8], Step [386/750], Loss: 0.0134\n",
      "Epoch [3/8], Step [387/750], Loss: 0.0234\n",
      "Epoch [3/8], Step [388/750], Loss: 0.0144\n",
      "Epoch [3/8], Step [389/750], Loss: 0.0772\n",
      "Epoch [3/8], Step [390/750], Loss: 0.0124\n",
      "Epoch [3/8], Step [391/750], Loss: 0.0159\n",
      "Epoch [3/8], Step [392/750], Loss: 0.0530\n",
      "Epoch [3/8], Step [393/750], Loss: 0.0040\n",
      "Epoch [3/8], Step [394/750], Loss: 0.0247\n",
      "Epoch [3/8], Step [395/750], Loss: 0.0024\n",
      "Epoch [3/8], Step [396/750], Loss: 0.0877\n",
      "Epoch [3/8], Step [397/750], Loss: 0.0111\n",
      "Epoch [3/8], Step [398/750], Loss: 0.0257\n",
      "Epoch [3/8], Step [399/750], Loss: 0.1068\n",
      "Epoch [3/8], Step [400/750], Loss: 0.0129\n",
      "Epoch [3/8], Step [401/750], Loss: 0.1059\n",
      "Epoch [3/8], Step [402/750], Loss: 0.0151\n",
      "Epoch [3/8], Step [403/750], Loss: 0.0041\n",
      "Epoch [3/8], Step [404/750], Loss: 0.0037\n",
      "Epoch [3/8], Step [405/750], Loss: 0.0648\n",
      "Epoch [3/8], Step [406/750], Loss: 0.1246\n",
      "Epoch [3/8], Step [407/750], Loss: 0.0365\n",
      "Epoch [3/8], Step [408/750], Loss: 0.0090\n",
      "Epoch [3/8], Step [409/750], Loss: 0.0745\n",
      "Epoch [3/8], Step [410/750], Loss: 0.0065\n",
      "Epoch [3/8], Step [411/750], Loss: 0.0705\n",
      "Epoch [3/8], Step [412/750], Loss: 0.0509\n",
      "Epoch [3/8], Step [413/750], Loss: 0.0597\n",
      "Epoch [3/8], Step [414/750], Loss: 0.0076\n",
      "Epoch [3/8], Step [415/750], Loss: 0.0298\n",
      "Epoch [3/8], Step [416/750], Loss: 0.0419\n",
      "Epoch [3/8], Step [417/750], Loss: 0.0885\n",
      "Epoch [3/8], Step [418/750], Loss: 0.0071\n",
      "Epoch [3/8], Step [419/750], Loss: 0.2130\n",
      "Epoch [3/8], Step [420/750], Loss: 0.1607\n",
      "Epoch [3/8], Step [421/750], Loss: 0.0020\n",
      "Epoch [3/8], Step [422/750], Loss: 0.0386\n",
      "Epoch [3/8], Step [423/750], Loss: 0.0084\n",
      "Epoch [3/8], Step [424/750], Loss: 0.0694\n",
      "Epoch [3/8], Step [425/750], Loss: 0.0267\n",
      "Epoch [3/8], Step [426/750], Loss: 0.1073\n",
      "Epoch [3/8], Step [427/750], Loss: 0.0338\n",
      "Epoch [3/8], Step [428/750], Loss: 0.1904\n",
      "Epoch [3/8], Step [429/750], Loss: 0.0087\n",
      "Epoch [3/8], Step [430/750], Loss: 0.0459\n",
      "Epoch [3/8], Step [431/750], Loss: 0.0377\n",
      "Epoch [3/8], Step [432/750], Loss: 0.0176\n",
      "Epoch [3/8], Step [433/750], Loss: 0.0209\n",
      "Epoch [3/8], Step [434/750], Loss: 0.0528\n",
      "Epoch [3/8], Step [435/750], Loss: 0.1000\n",
      "Epoch [3/8], Step [436/750], Loss: 0.0307\n",
      "Epoch [3/8], Step [437/750], Loss: 0.0052\n",
      "Epoch [3/8], Step [438/750], Loss: 0.0198\n",
      "Epoch [3/8], Step [439/750], Loss: 0.0736\n",
      "Epoch [3/8], Step [440/750], Loss: 0.0062\n",
      "Epoch [3/8], Step [441/750], Loss: 0.0578\n",
      "Epoch [3/8], Step [442/750], Loss: 0.0258\n",
      "Epoch [3/8], Step [443/750], Loss: 0.0594\n",
      "Epoch [3/8], Step [444/750], Loss: 0.0290\n",
      "Epoch [3/8], Step [445/750], Loss: 0.0145\n",
      "Epoch [3/8], Step [446/750], Loss: 0.0377\n",
      "Epoch [3/8], Step [447/750], Loss: 0.0307\n",
      "Epoch [3/8], Step [448/750], Loss: 0.0412\n",
      "Epoch [3/8], Step [449/750], Loss: 0.0077\n",
      "Epoch [3/8], Step [450/750], Loss: 0.0097\n",
      "Epoch [3/8], Step [451/750], Loss: 0.0070\n",
      "Epoch [3/8], Step [452/750], Loss: 0.0047\n",
      "Epoch [3/8], Step [453/750], Loss: 0.0490\n",
      "Epoch [3/8], Step [454/750], Loss: 0.0069\n",
      "Epoch [3/8], Step [455/750], Loss: 0.0062\n",
      "Epoch [3/8], Step [456/750], Loss: 0.0860\n",
      "Epoch [3/8], Step [457/750], Loss: 0.0360\n",
      "Epoch [3/8], Step [458/750], Loss: 0.0540\n",
      "Epoch [3/8], Step [459/750], Loss: 0.0498\n",
      "Epoch [3/8], Step [460/750], Loss: 0.0284\n",
      "Epoch [3/8], Step [461/750], Loss: 0.0217\n",
      "Epoch [3/8], Step [462/750], Loss: 0.0118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], Step [463/750], Loss: 0.0959\n",
      "Epoch [3/8], Step [464/750], Loss: 0.0125\n",
      "Epoch [3/8], Step [465/750], Loss: 0.0578\n",
      "Epoch [3/8], Step [466/750], Loss: 0.0111\n",
      "Epoch [3/8], Step [467/750], Loss: 0.0306\n",
      "Epoch [3/8], Step [468/750], Loss: 0.0033\n",
      "Epoch [3/8], Step [469/750], Loss: 0.0569\n",
      "Epoch [3/8], Step [470/750], Loss: 0.0194\n",
      "Epoch [3/8], Step [471/750], Loss: 0.0756\n",
      "Epoch [3/8], Step [472/750], Loss: 0.0038\n",
      "Epoch [3/8], Step [473/750], Loss: 0.0293\n",
      "Epoch [3/8], Step [474/750], Loss: 0.0100\n",
      "Epoch [3/8], Step [475/750], Loss: 0.0828\n",
      "Epoch [3/8], Step [476/750], Loss: 0.0048\n",
      "Epoch [3/8], Step [477/750], Loss: 0.0556\n",
      "Epoch [3/8], Step [478/750], Loss: 0.0900\n",
      "Epoch [3/8], Step [479/750], Loss: 0.0195\n",
      "Epoch [3/8], Step [480/750], Loss: 0.0972\n",
      "Epoch [3/8], Step [481/750], Loss: 0.0119\n",
      "Epoch [3/8], Step [482/750], Loss: 0.0131\n",
      "Epoch [3/8], Step [483/750], Loss: 0.0054\n",
      "Epoch [3/8], Step [484/750], Loss: 0.0081\n",
      "Epoch [3/8], Step [485/750], Loss: 0.1211\n",
      "Epoch [3/8], Step [486/750], Loss: 0.0018\n",
      "Epoch [3/8], Step [487/750], Loss: 0.0089\n",
      "Epoch [3/8], Step [488/750], Loss: 0.1600\n",
      "Epoch [3/8], Step [489/750], Loss: 0.0793\n",
      "Epoch [3/8], Step [490/750], Loss: 0.0166\n",
      "Epoch [3/8], Step [491/750], Loss: 0.0021\n",
      "Epoch [3/8], Step [492/750], Loss: 0.0843\n",
      "Epoch [3/8], Step [493/750], Loss: 0.0539\n",
      "Epoch [3/8], Step [494/750], Loss: 0.0897\n",
      "Epoch [3/8], Step [495/750], Loss: 0.0076\n",
      "Epoch [3/8], Step [496/750], Loss: 0.0733\n",
      "Epoch [3/8], Step [497/750], Loss: 0.0091\n",
      "Epoch [3/8], Step [498/750], Loss: 0.0098\n",
      "Epoch [3/8], Step [499/750], Loss: 0.0164\n",
      "Epoch [3/8], Step [500/750], Loss: 0.0073\n",
      "Epoch [3/8], Step [501/750], Loss: 0.0017\n",
      "Epoch [3/8], Step [502/750], Loss: 0.0308\n",
      "Epoch [3/8], Step [503/750], Loss: 0.0697\n",
      "Epoch [3/8], Step [504/750], Loss: 0.0958\n",
      "Epoch [3/8], Step [505/750], Loss: 0.0186\n",
      "Epoch [3/8], Step [506/750], Loss: 0.0236\n",
      "Epoch [3/8], Step [507/750], Loss: 0.0674\n",
      "Epoch [3/8], Step [508/750], Loss: 0.0175\n",
      "Epoch [3/8], Step [509/750], Loss: 0.2084\n",
      "Epoch [3/8], Step [510/750], Loss: 0.0086\n",
      "Epoch [3/8], Step [511/750], Loss: 0.0831\n",
      "Epoch [3/8], Step [512/750], Loss: 0.0490\n",
      "Epoch [3/8], Step [513/750], Loss: 0.0238\n",
      "Epoch [3/8], Step [514/750], Loss: 0.0052\n",
      "Epoch [3/8], Step [515/750], Loss: 0.0090\n",
      "Epoch [3/8], Step [516/750], Loss: 0.0106\n",
      "Epoch [3/8], Step [517/750], Loss: 0.0910\n",
      "Epoch [3/8], Step [518/750], Loss: 0.1620\n",
      "Epoch [3/8], Step [519/750], Loss: 0.0126\n",
      "Epoch [3/8], Step [520/750], Loss: 0.0612\n",
      "Epoch [3/8], Step [521/750], Loss: 0.1375\n",
      "Epoch [3/8], Step [522/750], Loss: 0.1843\n",
      "Epoch [3/8], Step [523/750], Loss: 0.0311\n",
      "Epoch [3/8], Step [524/750], Loss: 0.0088\n",
      "Epoch [3/8], Step [525/750], Loss: 0.0402\n",
      "Epoch [3/8], Step [526/750], Loss: 0.0030\n",
      "Epoch [3/8], Step [527/750], Loss: 0.0179\n",
      "Epoch [3/8], Step [528/750], Loss: 0.0445\n",
      "Epoch [3/8], Step [529/750], Loss: 0.1492\n",
      "Epoch [3/8], Step [530/750], Loss: 0.0130\n",
      "Epoch [3/8], Step [531/750], Loss: 0.1411\n",
      "Epoch [3/8], Step [532/750], Loss: 0.0371\n",
      "Epoch [3/8], Step [533/750], Loss: 0.0042\n",
      "Epoch [3/8], Step [534/750], Loss: 0.0510\n",
      "Epoch [3/8], Step [535/750], Loss: 0.0221\n",
      "Epoch [3/8], Step [536/750], Loss: 0.0244\n",
      "Epoch [3/8], Step [537/750], Loss: 0.0762\n",
      "Epoch [3/8], Step [538/750], Loss: 0.0287\n",
      "Epoch [3/8], Step [539/750], Loss: 0.0380\n",
      "Epoch [3/8], Step [540/750], Loss: 0.0571\n",
      "Epoch [3/8], Step [541/750], Loss: 0.0047\n",
      "Epoch [3/8], Step [542/750], Loss: 0.0659\n",
      "Epoch [3/8], Step [543/750], Loss: 0.0294\n",
      "Epoch [3/8], Step [544/750], Loss: 0.0401\n",
      "Epoch [3/8], Step [545/750], Loss: 0.0424\n",
      "Epoch [3/8], Step [546/750], Loss: 0.0092\n",
      "Epoch [3/8], Step [547/750], Loss: 0.0165\n",
      "Epoch [3/8], Step [548/750], Loss: 0.0227\n",
      "Epoch [3/8], Step [549/750], Loss: 0.1741\n",
      "Epoch [3/8], Step [550/750], Loss: 0.1046\n",
      "Epoch [3/8], Step [551/750], Loss: 0.0558\n",
      "Epoch [3/8], Step [552/750], Loss: 0.0047\n",
      "Epoch [3/8], Step [553/750], Loss: 0.0258\n",
      "Epoch [3/8], Step [554/750], Loss: 0.0680\n",
      "Epoch [3/8], Step [555/750], Loss: 0.0339\n",
      "Epoch [3/8], Step [556/750], Loss: 0.0044\n",
      "Epoch [3/8], Step [557/750], Loss: 0.0056\n",
      "Epoch [3/8], Step [558/750], Loss: 0.1764\n",
      "Epoch [3/8], Step [559/750], Loss: 0.0061\n",
      "Epoch [3/8], Step [560/750], Loss: 0.0432\n",
      "Epoch [3/8], Step [561/750], Loss: 0.0142\n",
      "Epoch [3/8], Step [562/750], Loss: 0.0430\n",
      "Epoch [3/8], Step [563/750], Loss: 0.0239\n",
      "Epoch [3/8], Step [564/750], Loss: 0.0125\n",
      "Epoch [3/8], Step [565/750], Loss: 0.0895\n",
      "Epoch [3/8], Step [566/750], Loss: 0.1250\n",
      "Epoch [3/8], Step [567/750], Loss: 0.1069\n",
      "Epoch [3/8], Step [568/750], Loss: 0.0036\n",
      "Epoch [3/8], Step [569/750], Loss: 0.0621\n",
      "Epoch [3/8], Step [570/750], Loss: 0.0033\n",
      "Epoch [3/8], Step [571/750], Loss: 0.0338\n",
      "Epoch [3/8], Step [572/750], Loss: 0.0259\n",
      "Epoch [3/8], Step [573/750], Loss: 0.0042\n",
      "Epoch [3/8], Step [574/750], Loss: 0.0195\n",
      "Epoch [3/8], Step [575/750], Loss: 0.0120\n",
      "Epoch [3/8], Step [576/750], Loss: 0.0227\n",
      "Epoch [3/8], Step [577/750], Loss: 0.0570\n",
      "Epoch [3/8], Step [578/750], Loss: 0.0574\n",
      "Epoch [3/8], Step [579/750], Loss: 0.0974\n",
      "Epoch [3/8], Step [580/750], Loss: 0.0841\n",
      "Epoch [3/8], Step [581/750], Loss: 0.0381\n",
      "Epoch [3/8], Step [582/750], Loss: 0.1023\n",
      "Epoch [3/8], Step [583/750], Loss: 0.0805\n",
      "Epoch [3/8], Step [584/750], Loss: 0.0774\n",
      "Epoch [3/8], Step [585/750], Loss: 0.0866\n",
      "Epoch [3/8], Step [586/750], Loss: 0.0070\n",
      "Epoch [3/8], Step [587/750], Loss: 0.0604\n",
      "Epoch [3/8], Step [588/750], Loss: 0.0451\n",
      "Epoch [3/8], Step [589/750], Loss: 0.0255\n",
      "Epoch [3/8], Step [590/750], Loss: 0.1090\n",
      "Epoch [3/8], Step [591/750], Loss: 0.0682\n",
      "Epoch [3/8], Step [592/750], Loss: 0.0110\n",
      "Epoch [3/8], Step [593/750], Loss: 0.0032\n",
      "Epoch [3/8], Step [594/750], Loss: 0.0779\n",
      "Epoch [3/8], Step [595/750], Loss: 0.1252\n",
      "Epoch [3/8], Step [596/750], Loss: 0.0118\n",
      "Epoch [3/8], Step [597/750], Loss: 0.0390\n",
      "Epoch [3/8], Step [598/750], Loss: 0.0360\n",
      "Epoch [3/8], Step [599/750], Loss: 0.0109\n",
      "Epoch [3/8], Step [600/750], Loss: 0.0979\n",
      "Epoch [3/8], Step [601/750], Loss: 0.0307\n",
      "Epoch [3/8], Step [602/750], Loss: 0.0930\n",
      "Epoch [3/8], Step [603/750], Loss: 0.0453\n",
      "Epoch [3/8], Step [604/750], Loss: 0.0297\n",
      "Epoch [3/8], Step [605/750], Loss: 0.0185\n",
      "Epoch [3/8], Step [606/750], Loss: 0.0485\n",
      "Epoch [3/8], Step [607/750], Loss: 0.0307\n",
      "Epoch [3/8], Step [608/750], Loss: 0.0554\n",
      "Epoch [3/8], Step [609/750], Loss: 0.0919\n",
      "Epoch [3/8], Step [610/750], Loss: 0.1760\n",
      "Epoch [3/8], Step [611/750], Loss: 0.0313\n",
      "Epoch [3/8], Step [612/750], Loss: 0.0337\n",
      "Epoch [3/8], Step [613/750], Loss: 0.0790\n",
      "Epoch [3/8], Step [614/750], Loss: 0.0779\n",
      "Epoch [3/8], Step [615/750], Loss: 0.1165\n",
      "Epoch [3/8], Step [616/750], Loss: 0.0827\n",
      "Epoch [3/8], Step [617/750], Loss: 0.0148\n",
      "Epoch [3/8], Step [618/750], Loss: 0.0183\n",
      "Epoch [3/8], Step [619/750], Loss: 0.1049\n",
      "Epoch [3/8], Step [620/750], Loss: 0.0420\n",
      "Epoch [3/8], Step [621/750], Loss: 0.0498\n",
      "Epoch [3/8], Step [622/750], Loss: 0.0928\n",
      "Epoch [3/8], Step [623/750], Loss: 0.0677\n",
      "Epoch [3/8], Step [624/750], Loss: 0.0131\n",
      "Epoch [3/8], Step [625/750], Loss: 0.0373\n",
      "Epoch [3/8], Step [626/750], Loss: 0.0890\n",
      "Epoch [3/8], Step [627/750], Loss: 0.0274\n",
      "Epoch [3/8], Step [628/750], Loss: 0.0246\n",
      "Epoch [3/8], Step [629/750], Loss: 0.0417\n",
      "Epoch [3/8], Step [630/750], Loss: 0.0483\n",
      "Epoch [3/8], Step [631/750], Loss: 0.0897\n",
      "Epoch [3/8], Step [632/750], Loss: 0.0236\n",
      "Epoch [3/8], Step [633/750], Loss: 0.0075\n",
      "Epoch [3/8], Step [634/750], Loss: 0.0686\n",
      "Epoch [3/8], Step [635/750], Loss: 0.1279\n",
      "Epoch [3/8], Step [636/750], Loss: 0.0184\n",
      "Epoch [3/8], Step [637/750], Loss: 0.0245\n",
      "Epoch [3/8], Step [638/750], Loss: 0.1509\n",
      "Epoch [3/8], Step [639/750], Loss: 0.0330\n",
      "Epoch [3/8], Step [640/750], Loss: 0.0267\n",
      "Epoch [3/8], Step [641/750], Loss: 0.0482\n",
      "Epoch [3/8], Step [642/750], Loss: 0.0044\n",
      "Epoch [3/8], Step [643/750], Loss: 0.0490\n",
      "Epoch [3/8], Step [644/750], Loss: 0.0142\n",
      "Epoch [3/8], Step [645/750], Loss: 0.0431\n",
      "Epoch [3/8], Step [646/750], Loss: 0.0088\n",
      "Epoch [3/8], Step [647/750], Loss: 0.0793\n",
      "Epoch [3/8], Step [648/750], Loss: 0.0082\n",
      "Epoch [3/8], Step [649/750], Loss: 0.0252\n",
      "Epoch [3/8], Step [650/750], Loss: 0.0827\n",
      "Epoch [3/8], Step [651/750], Loss: 0.0634\n",
      "Epoch [3/8], Step [652/750], Loss: 0.0171\n",
      "Epoch [3/8], Step [653/750], Loss: 0.2339\n",
      "Epoch [3/8], Step [654/750], Loss: 0.0180\n",
      "Epoch [3/8], Step [655/750], Loss: 0.0498\n",
      "Epoch [3/8], Step [656/750], Loss: 0.2061\n",
      "Epoch [3/8], Step [657/750], Loss: 0.0117\n",
      "Epoch [3/8], Step [658/750], Loss: 0.0484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], Step [659/750], Loss: 0.0153\n",
      "Epoch [3/8], Step [660/750], Loss: 0.0651\n",
      "Epoch [3/8], Step [661/750], Loss: 0.0065\n",
      "Epoch [3/8], Step [662/750], Loss: 0.1302\n",
      "Epoch [3/8], Step [663/750], Loss: 0.1240\n",
      "Epoch [3/8], Step [664/750], Loss: 0.0419\n",
      "Epoch [3/8], Step [665/750], Loss: 0.0116\n",
      "Epoch [3/8], Step [666/750], Loss: 0.0717\n",
      "Epoch [3/8], Step [667/750], Loss: 0.0042\n",
      "Epoch [3/8], Step [668/750], Loss: 0.0057\n",
      "Epoch [3/8], Step [669/750], Loss: 0.0150\n",
      "Epoch [3/8], Step [670/750], Loss: 0.0269\n",
      "Epoch [3/8], Step [671/750], Loss: 0.0489\n",
      "Epoch [3/8], Step [672/750], Loss: 0.0321\n",
      "Epoch [3/8], Step [673/750], Loss: 0.0081\n",
      "Epoch [3/8], Step [674/750], Loss: 0.0273\n",
      "Epoch [3/8], Step [675/750], Loss: 0.0521\n",
      "Epoch [3/8], Step [676/750], Loss: 0.0336\n",
      "Epoch [3/8], Step [677/750], Loss: 0.0458\n",
      "Epoch [3/8], Step [678/750], Loss: 0.0781\n",
      "Epoch [3/8], Step [679/750], Loss: 0.0598\n",
      "Epoch [3/8], Step [680/750], Loss: 0.0953\n",
      "Epoch [3/8], Step [681/750], Loss: 0.0930\n",
      "Epoch [3/8], Step [682/750], Loss: 0.0173\n",
      "Epoch [3/8], Step [683/750], Loss: 0.1392\n",
      "Epoch [3/8], Step [684/750], Loss: 0.0178\n",
      "Epoch [3/8], Step [685/750], Loss: 0.0616\n",
      "Epoch [3/8], Step [686/750], Loss: 0.0553\n",
      "Epoch [3/8], Step [687/750], Loss: 0.0640\n",
      "Epoch [3/8], Step [688/750], Loss: 0.0060\n",
      "Epoch [3/8], Step [689/750], Loss: 0.1017\n",
      "Epoch [3/8], Step [690/750], Loss: 0.0149\n",
      "Epoch [3/8], Step [691/750], Loss: 0.0815\n",
      "Epoch [3/8], Step [692/750], Loss: 0.0186\n",
      "Epoch [3/8], Step [693/750], Loss: 0.0070\n",
      "Epoch [3/8], Step [694/750], Loss: 0.1023\n",
      "Epoch [3/8], Step [695/750], Loss: 0.0157\n",
      "Epoch [3/8], Step [696/750], Loss: 0.1060\n",
      "Epoch [3/8], Step [697/750], Loss: 0.0031\n",
      "Epoch [3/8], Step [698/750], Loss: 0.0224\n",
      "Epoch [3/8], Step [699/750], Loss: 0.0134\n",
      "Epoch [3/8], Step [700/750], Loss: 0.0238\n",
      "Epoch [3/8], Step [701/750], Loss: 0.0106\n",
      "Epoch [3/8], Step [702/750], Loss: 0.0144\n",
      "Epoch [3/8], Step [703/750], Loss: 0.0755\n",
      "Epoch [3/8], Step [704/750], Loss: 0.0864\n",
      "Epoch [3/8], Step [705/750], Loss: 0.0227\n",
      "Epoch [3/8], Step [706/750], Loss: 0.0545\n",
      "Epoch [3/8], Step [707/750], Loss: 0.0880\n",
      "Epoch [3/8], Step [708/750], Loss: 0.0635\n",
      "Epoch [3/8], Step [709/750], Loss: 0.0152\n",
      "Epoch [3/8], Step [710/750], Loss: 0.0182\n",
      "Epoch [3/8], Step [711/750], Loss: 0.0710\n",
      "Epoch [3/8], Step [712/750], Loss: 0.0080\n",
      "Epoch [3/8], Step [713/750], Loss: 0.0685\n",
      "Epoch [3/8], Step [714/750], Loss: 0.0369\n",
      "Epoch [3/8], Step [715/750], Loss: 0.0095\n",
      "Epoch [3/8], Step [716/750], Loss: 0.0543\n",
      "Epoch [3/8], Step [717/750], Loss: 0.0844\n",
      "Epoch [3/8], Step [718/750], Loss: 0.0879\n",
      "Epoch [3/8], Step [719/750], Loss: 0.0066\n",
      "Epoch [3/8], Step [720/750], Loss: 0.0294\n",
      "Epoch [3/8], Step [721/750], Loss: 0.0929\n",
      "Epoch [3/8], Step [722/750], Loss: 0.0330\n",
      "Epoch [3/8], Step [723/750], Loss: 0.0694\n",
      "Epoch [3/8], Step [724/750], Loss: 0.0190\n",
      "Epoch [3/8], Step [725/750], Loss: 0.0019\n",
      "Epoch [3/8], Step [726/750], Loss: 0.0095\n",
      "Epoch [3/8], Step [727/750], Loss: 0.0097\n",
      "Epoch [3/8], Step [728/750], Loss: 0.0132\n",
      "Epoch [3/8], Step [729/750], Loss: 0.0387\n",
      "Epoch [3/8], Step [730/750], Loss: 0.0012\n",
      "Epoch [3/8], Step [731/750], Loss: 0.1041\n",
      "Epoch [3/8], Step [732/750], Loss: 0.0709\n",
      "Epoch [3/8], Step [733/750], Loss: 0.0023\n",
      "Epoch [3/8], Step [734/750], Loss: 0.0174\n",
      "Epoch [3/8], Step [735/750], Loss: 0.0151\n",
      "Epoch [3/8], Step [736/750], Loss: 0.0199\n",
      "Epoch [3/8], Step [737/750], Loss: 0.0276\n",
      "Epoch [3/8], Step [738/750], Loss: 0.0078\n",
      "Epoch [3/8], Step [739/750], Loss: 0.0056\n",
      "Epoch [3/8], Step [740/750], Loss: 0.0592\n",
      "Epoch [3/8], Step [741/750], Loss: 0.0045\n",
      "Epoch [3/8], Step [742/750], Loss: 0.1297\n",
      "Epoch [3/8], Step [743/750], Loss: 0.0898\n",
      "Epoch [3/8], Step [744/750], Loss: 0.1683\n",
      "Epoch [3/8], Step [745/750], Loss: 0.0431\n",
      "Epoch [3/8], Step [746/750], Loss: 0.0850\n",
      "Epoch [3/8], Step [747/750], Loss: 0.0465\n",
      "Epoch [3/8], Step [748/750], Loss: 0.0148\n",
      "Epoch [3/8], Step [749/750], Loss: 0.0487\n",
      "Epoch [3/8], Step [750/750], Loss: 0.0389\n",
      "Epoch [3/8], Tr. loss: 0.2672. Test loss: 0.1471\n",
      "\n",
      "\n",
      "Epoch [4/8], Step [1/750], Loss: 0.1054\n",
      "Epoch [4/8], Step [2/750], Loss: 0.0388\n",
      "Epoch [4/8], Step [3/750], Loss: 0.0293\n",
      "Epoch [4/8], Step [4/750], Loss: 0.0471\n",
      "Epoch [4/8], Step [5/750], Loss: 0.0304\n",
      "Epoch [4/8], Step [6/750], Loss: 0.0129\n",
      "Epoch [4/8], Step [7/750], Loss: 0.0072\n",
      "Epoch [4/8], Step [8/750], Loss: 0.0096\n",
      "Epoch [4/8], Step [9/750], Loss: 0.0036\n",
      "Epoch [4/8], Step [10/750], Loss: 0.0057\n",
      "Epoch [4/8], Step [11/750], Loss: 0.0109\n",
      "Epoch [4/8], Step [12/750], Loss: 0.0073\n",
      "Epoch [4/8], Step [13/750], Loss: 0.0039\n",
      "Epoch [4/8], Step [14/750], Loss: 0.0084\n",
      "Epoch [4/8], Step [15/750], Loss: 0.0036\n",
      "Epoch [4/8], Step [16/750], Loss: 0.0156\n",
      "Epoch [4/8], Step [17/750], Loss: 0.0257\n",
      "Epoch [4/8], Step [18/750], Loss: 0.0069\n",
      "Epoch [4/8], Step [19/750], Loss: 0.0060\n",
      "Epoch [4/8], Step [20/750], Loss: 0.0038\n",
      "Epoch [4/8], Step [21/750], Loss: 0.0112\n",
      "Epoch [4/8], Step [22/750], Loss: 0.0138\n",
      "Epoch [4/8], Step [23/750], Loss: 0.0103\n",
      "Epoch [4/8], Step [24/750], Loss: 0.0249\n",
      "Epoch [4/8], Step [25/750], Loss: 0.0120\n",
      "Epoch [4/8], Step [26/750], Loss: 0.0350\n",
      "Epoch [4/8], Step [27/750], Loss: 0.0016\n",
      "Epoch [4/8], Step [28/750], Loss: 0.0741\n",
      "Epoch [4/8], Step [29/750], Loss: 0.0698\n",
      "Epoch [4/8], Step [30/750], Loss: 0.0063\n",
      "Epoch [4/8], Step [31/750], Loss: 0.0018\n",
      "Epoch [4/8], Step [32/750], Loss: 0.0358\n",
      "Epoch [4/8], Step [33/750], Loss: 0.0521\n",
      "Epoch [4/8], Step [34/750], Loss: 0.0113\n",
      "Epoch [4/8], Step [35/750], Loss: 0.0635\n",
      "Epoch [4/8], Step [36/750], Loss: 0.0776\n",
      "Epoch [4/8], Step [37/750], Loss: 0.0156\n",
      "Epoch [4/8], Step [38/750], Loss: 0.0070\n",
      "Epoch [4/8], Step [39/750], Loss: 0.0553\n",
      "Epoch [4/8], Step [40/750], Loss: 0.0065\n",
      "Epoch [4/8], Step [41/750], Loss: 0.0240\n",
      "Epoch [4/8], Step [42/750], Loss: 0.0138\n",
      "Epoch [4/8], Step [43/750], Loss: 0.1835\n",
      "Epoch [4/8], Step [44/750], Loss: 0.0632\n",
      "Epoch [4/8], Step [45/750], Loss: 0.0857\n",
      "Epoch [4/8], Step [46/750], Loss: 0.0491\n",
      "Epoch [4/8], Step [47/750], Loss: 0.2397\n",
      "Epoch [4/8], Step [48/750], Loss: 0.0108\n",
      "Epoch [4/8], Step [49/750], Loss: 0.0384\n",
      "Epoch [4/8], Step [50/750], Loss: 0.0488\n",
      "Epoch [4/8], Step [51/750], Loss: 0.0176\n",
      "Epoch [4/8], Step [52/750], Loss: 0.0074\n",
      "Epoch [4/8], Step [53/750], Loss: 0.0472\n",
      "Epoch [4/8], Step [54/750], Loss: 0.0218\n",
      "Epoch [4/8], Step [55/750], Loss: 0.1979\n",
      "Epoch [4/8], Step [56/750], Loss: 0.0308\n",
      "Epoch [4/8], Step [57/750], Loss: 0.0229\n",
      "Epoch [4/8], Step [58/750], Loss: 0.1340\n",
      "Epoch [4/8], Step [59/750], Loss: 0.0155\n",
      "Epoch [4/8], Step [60/750], Loss: 0.0315\n",
      "Epoch [4/8], Step [61/750], Loss: 0.0221\n",
      "Epoch [4/8], Step [62/750], Loss: 0.0221\n",
      "Epoch [4/8], Step [63/750], Loss: 0.0079\n",
      "Epoch [4/8], Step [64/750], Loss: 0.0083\n",
      "Epoch [4/8], Step [65/750], Loss: 0.0072\n",
      "Epoch [4/8], Step [66/750], Loss: 0.0355\n",
      "Epoch [4/8], Step [67/750], Loss: 0.0096\n",
      "Epoch [4/8], Step [68/750], Loss: 0.0941\n",
      "Epoch [4/8], Step [69/750], Loss: 0.0281\n",
      "Epoch [4/8], Step [70/750], Loss: 0.0160\n",
      "Epoch [4/8], Step [71/750], Loss: 0.0227\n",
      "Epoch [4/8], Step [72/750], Loss: 0.0064\n",
      "Epoch [4/8], Step [73/750], Loss: 0.0351\n",
      "Epoch [4/8], Step [74/750], Loss: 0.0039\n",
      "Epoch [4/8], Step [75/750], Loss: 0.0351\n",
      "Epoch [4/8], Step [76/750], Loss: 0.0112\n",
      "Epoch [4/8], Step [77/750], Loss: 0.0203\n",
      "Epoch [4/8], Step [78/750], Loss: 0.0112\n",
      "Epoch [4/8], Step [79/750], Loss: 0.0018\n",
      "Epoch [4/8], Step [80/750], Loss: 0.0459\n",
      "Epoch [4/8], Step [81/750], Loss: 0.0425\n",
      "Epoch [4/8], Step [82/750], Loss: 0.0292\n",
      "Epoch [4/8], Step [83/750], Loss: 0.0304\n",
      "Epoch [4/8], Step [84/750], Loss: 0.0020\n",
      "Epoch [4/8], Step [85/750], Loss: 0.0465\n",
      "Epoch [4/8], Step [86/750], Loss: 0.0243\n",
      "Epoch [4/8], Step [87/750], Loss: 0.0121\n",
      "Epoch [4/8], Step [88/750], Loss: 0.0289\n",
      "Epoch [4/8], Step [89/750], Loss: 0.0050\n",
      "Epoch [4/8], Step [90/750], Loss: 0.0083\n",
      "Epoch [4/8], Step [91/750], Loss: 0.0212\n",
      "Epoch [4/8], Step [92/750], Loss: 0.0125\n",
      "Epoch [4/8], Step [93/750], Loss: 0.0654\n",
      "Epoch [4/8], Step [94/750], Loss: 0.0650\n",
      "Epoch [4/8], Step [95/750], Loss: 0.0418\n",
      "Epoch [4/8], Step [96/750], Loss: 0.0311\n",
      "Epoch [4/8], Step [97/750], Loss: 0.0078\n",
      "Epoch [4/8], Step [98/750], Loss: 0.0168\n",
      "Epoch [4/8], Step [99/750], Loss: 0.0392\n",
      "Epoch [4/8], Step [100/750], Loss: 0.0569\n",
      "Epoch [4/8], Step [101/750], Loss: 0.0613\n",
      "Epoch [4/8], Step [102/750], Loss: 0.0212\n",
      "Epoch [4/8], Step [103/750], Loss: 0.0146\n",
      "Epoch [4/8], Step [104/750], Loss: 0.0363\n",
      "Epoch [4/8], Step [105/750], Loss: 0.0160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Step [106/750], Loss: 0.0328\n",
      "Epoch [4/8], Step [107/750], Loss: 0.0200\n",
      "Epoch [4/8], Step [108/750], Loss: 0.0074\n",
      "Epoch [4/8], Step [109/750], Loss: 0.0974\n",
      "Epoch [4/8], Step [110/750], Loss: 0.0060\n",
      "Epoch [4/8], Step [111/750], Loss: 0.0139\n",
      "Epoch [4/8], Step [112/750], Loss: 0.0256\n",
      "Epoch [4/8], Step [113/750], Loss: 0.1329\n",
      "Epoch [4/8], Step [114/750], Loss: 0.0451\n",
      "Epoch [4/8], Step [115/750], Loss: 0.0188\n",
      "Epoch [4/8], Step [116/750], Loss: 0.0598\n",
      "Epoch [4/8], Step [117/750], Loss: 0.0035\n",
      "Epoch [4/8], Step [118/750], Loss: 0.0443\n",
      "Epoch [4/8], Step [119/750], Loss: 0.0104\n",
      "Epoch [4/8], Step [120/750], Loss: 0.0293\n",
      "Epoch [4/8], Step [121/750], Loss: 0.0496\n",
      "Epoch [4/8], Step [122/750], Loss: 0.0921\n",
      "Epoch [4/8], Step [123/750], Loss: 0.0141\n",
      "Epoch [4/8], Step [124/750], Loss: 0.0940\n",
      "Epoch [4/8], Step [125/750], Loss: 0.0154\n",
      "Epoch [4/8], Step [126/750], Loss: 0.0252\n",
      "Epoch [4/8], Step [127/750], Loss: 0.0156\n",
      "Epoch [4/8], Step [128/750], Loss: 0.0991\n",
      "Epoch [4/8], Step [129/750], Loss: 0.0459\n",
      "Epoch [4/8], Step [130/750], Loss: 0.0191\n",
      "Epoch [4/8], Step [131/750], Loss: 0.1179\n",
      "Epoch [4/8], Step [132/750], Loss: 0.0025\n",
      "Epoch [4/8], Step [133/750], Loss: 0.0023\n",
      "Epoch [4/8], Step [134/750], Loss: 0.0045\n",
      "Epoch [4/8], Step [135/750], Loss: 0.0316\n",
      "Epoch [4/8], Step [136/750], Loss: 0.0186\n",
      "Epoch [4/8], Step [137/750], Loss: 0.0117\n",
      "Epoch [4/8], Step [138/750], Loss: 0.0304\n",
      "Epoch [4/8], Step [139/750], Loss: 0.2350\n",
      "Epoch [4/8], Step [140/750], Loss: 0.0020\n",
      "Epoch [4/8], Step [141/750], Loss: 0.0149\n",
      "Epoch [4/8], Step [142/750], Loss: 0.0155\n",
      "Epoch [4/8], Step [143/750], Loss: 0.0018\n",
      "Epoch [4/8], Step [144/750], Loss: 0.0015\n",
      "Epoch [4/8], Step [145/750], Loss: 0.0168\n",
      "Epoch [4/8], Step [146/750], Loss: 0.0552\n",
      "Epoch [4/8], Step [147/750], Loss: 0.0480\n",
      "Epoch [4/8], Step [148/750], Loss: 0.0039\n",
      "Epoch [4/8], Step [149/750], Loss: 0.0153\n",
      "Epoch [4/8], Step [150/750], Loss: 0.0749\n",
      "Epoch [4/8], Step [151/750], Loss: 0.0053\n",
      "Epoch [4/8], Step [152/750], Loss: 0.0491\n",
      "Epoch [4/8], Step [153/750], Loss: 0.0296\n",
      "Epoch [4/8], Step [154/750], Loss: 0.0088\n",
      "Epoch [4/8], Step [155/750], Loss: 0.0050\n",
      "Epoch [4/8], Step [156/750], Loss: 0.0034\n",
      "Epoch [4/8], Step [157/750], Loss: 0.0227\n",
      "Epoch [4/8], Step [158/750], Loss: 0.0096\n",
      "Epoch [4/8], Step [159/750], Loss: 0.0293\n",
      "Epoch [4/8], Step [160/750], Loss: 0.0612\n",
      "Epoch [4/8], Step [161/750], Loss: 0.0103\n",
      "Epoch [4/8], Step [162/750], Loss: 0.1002\n",
      "Epoch [4/8], Step [163/750], Loss: 0.0032\n",
      "Epoch [4/8], Step [164/750], Loss: 0.0217\n",
      "Epoch [4/8], Step [165/750], Loss: 0.0229\n",
      "Epoch [4/8], Step [166/750], Loss: 0.0169\n",
      "Epoch [4/8], Step [167/750], Loss: 0.0192\n",
      "Epoch [4/8], Step [168/750], Loss: 0.0195\n",
      "Epoch [4/8], Step [169/750], Loss: 0.0318\n",
      "Epoch [4/8], Step [170/750], Loss: 0.0154\n",
      "Epoch [4/8], Step [171/750], Loss: 0.0201\n",
      "Epoch [4/8], Step [172/750], Loss: 0.0265\n",
      "Epoch [4/8], Step [173/750], Loss: 0.0195\n",
      "Epoch [4/8], Step [174/750], Loss: 0.0509\n",
      "Epoch [4/8], Step [175/750], Loss: 0.0893\n",
      "Epoch [4/8], Step [176/750], Loss: 0.0522\n",
      "Epoch [4/8], Step [177/750], Loss: 0.0175\n",
      "Epoch [4/8], Step [178/750], Loss: 0.0571\n",
      "Epoch [4/8], Step [179/750], Loss: 0.0125\n",
      "Epoch [4/8], Step [180/750], Loss: 0.0045\n",
      "Epoch [4/8], Step [181/750], Loss: 0.0295\n",
      "Epoch [4/8], Step [182/750], Loss: 0.1055\n",
      "Epoch [4/8], Step [183/750], Loss: 0.0395\n",
      "Epoch [4/8], Step [184/750], Loss: 0.1131\n",
      "Epoch [4/8], Step [185/750], Loss: 0.0334\n",
      "Epoch [4/8], Step [186/750], Loss: 0.0144\n",
      "Epoch [4/8], Step [187/750], Loss: 0.0082\n",
      "Epoch [4/8], Step [188/750], Loss: 0.0236\n",
      "Epoch [4/8], Step [189/750], Loss: 0.0076\n",
      "Epoch [4/8], Step [190/750], Loss: 0.0086\n",
      "Epoch [4/8], Step [191/750], Loss: 0.0291\n",
      "Epoch [4/8], Step [192/750], Loss: 0.0524\n",
      "Epoch [4/8], Step [193/750], Loss: 0.1146\n",
      "Epoch [4/8], Step [194/750], Loss: 0.1262\n",
      "Epoch [4/8], Step [195/750], Loss: 0.0133\n",
      "Epoch [4/8], Step [196/750], Loss: 0.0292\n",
      "Epoch [4/8], Step [197/750], Loss: 0.0489\n",
      "Epoch [4/8], Step [198/750], Loss: 0.0022\n",
      "Epoch [4/8], Step [199/750], Loss: 0.0818\n",
      "Epoch [4/8], Step [200/750], Loss: 0.0246\n",
      "Epoch [4/8], Step [201/750], Loss: 0.0796\n",
      "Epoch [4/8], Step [202/750], Loss: 0.0745\n",
      "Epoch [4/8], Step [203/750], Loss: 0.1129\n",
      "Epoch [4/8], Step [204/750], Loss: 0.0278\n",
      "Epoch [4/8], Step [205/750], Loss: 0.0132\n",
      "Epoch [4/8], Step [206/750], Loss: 0.1073\n",
      "Epoch [4/8], Step [207/750], Loss: 0.0179\n",
      "Epoch [4/8], Step [208/750], Loss: 0.0091\n",
      "Epoch [4/8], Step [209/750], Loss: 0.0385\n",
      "Epoch [4/8], Step [210/750], Loss: 0.0267\n",
      "Epoch [4/8], Step [211/750], Loss: 0.1104\n",
      "Epoch [4/8], Step [212/750], Loss: 0.0250\n",
      "Epoch [4/8], Step [213/750], Loss: 0.0264\n",
      "Epoch [4/8], Step [214/750], Loss: 0.0621\n",
      "Epoch [4/8], Step [215/750], Loss: 0.0485\n",
      "Epoch [4/8], Step [216/750], Loss: 0.0083\n",
      "Epoch [4/8], Step [217/750], Loss: 0.0076\n",
      "Epoch [4/8], Step [218/750], Loss: 0.0250\n",
      "Epoch [4/8], Step [219/750], Loss: 0.0211\n",
      "Epoch [4/8], Step [220/750], Loss: 0.1333\n",
      "Epoch [4/8], Step [221/750], Loss: 0.0128\n",
      "Epoch [4/8], Step [222/750], Loss: 0.0059\n",
      "Epoch [4/8], Step [223/750], Loss: 0.0152\n",
      "Epoch [4/8], Step [224/750], Loss: 0.0056\n",
      "Epoch [4/8], Step [225/750], Loss: 0.0050\n",
      "Epoch [4/8], Step [226/750], Loss: 0.0497\n",
      "Epoch [4/8], Step [227/750], Loss: 0.0069\n",
      "Epoch [4/8], Step [228/750], Loss: 0.0365\n",
      "Epoch [4/8], Step [229/750], Loss: 0.0027\n",
      "Epoch [4/8], Step [230/750], Loss: 0.0091\n",
      "Epoch [4/8], Step [231/750], Loss: 0.0788\n",
      "Epoch [4/8], Step [232/750], Loss: 0.0044\n",
      "Epoch [4/8], Step [233/750], Loss: 0.0307\n",
      "Epoch [4/8], Step [234/750], Loss: 0.0521\n",
      "Epoch [4/8], Step [235/750], Loss: 0.0550\n",
      "Epoch [4/8], Step [236/750], Loss: 0.0469\n",
      "Epoch [4/8], Step [237/750], Loss: 0.0269\n",
      "Epoch [4/8], Step [238/750], Loss: 0.0484\n",
      "Epoch [4/8], Step [239/750], Loss: 0.0114\n",
      "Epoch [4/8], Step [240/750], Loss: 0.0553\n",
      "Epoch [4/8], Step [241/750], Loss: 0.0328\n",
      "Epoch [4/8], Step [242/750], Loss: 0.0531\n",
      "Epoch [4/8], Step [243/750], Loss: 0.0260\n",
      "Epoch [4/8], Step [244/750], Loss: 0.0030\n",
      "Epoch [4/8], Step [245/750], Loss: 0.0359\n",
      "Epoch [4/8], Step [246/750], Loss: 0.0132\n",
      "Epoch [4/8], Step [247/750], Loss: 0.0670\n",
      "Epoch [4/8], Step [248/750], Loss: 0.0082\n",
      "Epoch [4/8], Step [249/750], Loss: 0.0103\n",
      "Epoch [4/8], Step [250/750], Loss: 0.0100\n",
      "Epoch [4/8], Step [251/750], Loss: 0.0120\n",
      "Epoch [4/8], Step [252/750], Loss: 0.0175\n",
      "Epoch [4/8], Step [253/750], Loss: 0.0573\n",
      "Epoch [4/8], Step [254/750], Loss: 0.0018\n",
      "Epoch [4/8], Step [255/750], Loss: 0.0254\n",
      "Epoch [4/8], Step [256/750], Loss: 0.0136\n",
      "Epoch [4/8], Step [257/750], Loss: 0.0153\n",
      "Epoch [4/8], Step [258/750], Loss: 0.0055\n",
      "Epoch [4/8], Step [259/750], Loss: 0.0339\n",
      "Epoch [4/8], Step [260/750], Loss: 0.0718\n",
      "Epoch [4/8], Step [261/750], Loss: 0.0051\n",
      "Epoch [4/8], Step [262/750], Loss: 0.0721\n",
      "Epoch [4/8], Step [263/750], Loss: 0.0516\n",
      "Epoch [4/8], Step [264/750], Loss: 0.0077\n",
      "Epoch [4/8], Step [265/750], Loss: 0.0133\n",
      "Epoch [4/8], Step [266/750], Loss: 0.0257\n",
      "Epoch [4/8], Step [267/750], Loss: 0.0082\n",
      "Epoch [4/8], Step [268/750], Loss: 0.0637\n",
      "Epoch [4/8], Step [269/750], Loss: 0.0177\n",
      "Epoch [4/8], Step [270/750], Loss: 0.0130\n",
      "Epoch [4/8], Step [271/750], Loss: 0.0793\n",
      "Epoch [4/8], Step [272/750], Loss: 0.0553\n",
      "Epoch [4/8], Step [273/750], Loss: 0.0580\n",
      "Epoch [4/8], Step [274/750], Loss: 0.0026\n",
      "Epoch [4/8], Step [275/750], Loss: 0.0183\n",
      "Epoch [4/8], Step [276/750], Loss: 0.0113\n",
      "Epoch [4/8], Step [277/750], Loss: 0.0724\n",
      "Epoch [4/8], Step [278/750], Loss: 0.0055\n",
      "Epoch [4/8], Step [279/750], Loss: 0.0272\n",
      "Epoch [4/8], Step [280/750], Loss: 0.0017\n",
      "Epoch [4/8], Step [281/750], Loss: 0.0712\n",
      "Epoch [4/8], Step [282/750], Loss: 0.0057\n",
      "Epoch [4/8], Step [283/750], Loss: 0.0136\n",
      "Epoch [4/8], Step [284/750], Loss: 0.1244\n",
      "Epoch [4/8], Step [285/750], Loss: 0.0271\n",
      "Epoch [4/8], Step [286/750], Loss: 0.0143\n",
      "Epoch [4/8], Step [287/750], Loss: 0.0079\n",
      "Epoch [4/8], Step [288/750], Loss: 0.0157\n",
      "Epoch [4/8], Step [289/750], Loss: 0.0412\n",
      "Epoch [4/8], Step [290/750], Loss: 0.0158\n",
      "Epoch [4/8], Step [291/750], Loss: 0.0197\n",
      "Epoch [4/8], Step [292/750], Loss: 0.0448\n",
      "Epoch [4/8], Step [293/750], Loss: 0.0219\n",
      "Epoch [4/8], Step [294/750], Loss: 0.0499\n",
      "Epoch [4/8], Step [295/750], Loss: 0.0222\n",
      "Epoch [4/8], Step [296/750], Loss: 0.0167\n",
      "Epoch [4/8], Step [297/750], Loss: 0.0065\n",
      "Epoch [4/8], Step [298/750], Loss: 0.0543\n",
      "Epoch [4/8], Step [299/750], Loss: 0.0346\n",
      "Epoch [4/8], Step [300/750], Loss: 0.1610\n",
      "Epoch [4/8], Step [301/750], Loss: 0.0412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Step [302/750], Loss: 0.0465\n",
      "Epoch [4/8], Step [303/750], Loss: 0.0450\n",
      "Epoch [4/8], Step [304/750], Loss: 0.0241\n",
      "Epoch [4/8], Step [305/750], Loss: 0.0162\n",
      "Epoch [4/8], Step [306/750], Loss: 0.0073\n",
      "Epoch [4/8], Step [307/750], Loss: 0.0340\n",
      "Epoch [4/8], Step [308/750], Loss: 0.0055\n",
      "Epoch [4/8], Step [309/750], Loss: 0.0744\n",
      "Epoch [4/8], Step [310/750], Loss: 0.0070\n",
      "Epoch [4/8], Step [311/750], Loss: 0.0166\n",
      "Epoch [4/8], Step [312/750], Loss: 0.0353\n",
      "Epoch [4/8], Step [313/750], Loss: 0.0109\n",
      "Epoch [4/8], Step [314/750], Loss: 0.0059\n",
      "Epoch [4/8], Step [315/750], Loss: 0.1054\n",
      "Epoch [4/8], Step [316/750], Loss: 0.0723\n",
      "Epoch [4/8], Step [317/750], Loss: 0.1668\n",
      "Epoch [4/8], Step [318/750], Loss: 0.0329\n",
      "Epoch [4/8], Step [319/750], Loss: 0.0195\n",
      "Epoch [4/8], Step [320/750], Loss: 0.0072\n",
      "Epoch [4/8], Step [321/750], Loss: 0.0220\n",
      "Epoch [4/8], Step [322/750], Loss: 0.0162\n",
      "Epoch [4/8], Step [323/750], Loss: 0.0539\n",
      "Epoch [4/8], Step [324/750], Loss: 0.1244\n",
      "Epoch [4/8], Step [325/750], Loss: 0.0538\n",
      "Epoch [4/8], Step [326/750], Loss: 0.0067\n",
      "Epoch [4/8], Step [327/750], Loss: 0.0076\n",
      "Epoch [4/8], Step [328/750], Loss: 0.1157\n",
      "Epoch [4/8], Step [329/750], Loss: 0.0269\n",
      "Epoch [4/8], Step [330/750], Loss: 0.0033\n",
      "Epoch [4/8], Step [331/750], Loss: 0.0040\n",
      "Epoch [4/8], Step [332/750], Loss: 0.0328\n",
      "Epoch [4/8], Step [333/750], Loss: 0.0065\n",
      "Epoch [4/8], Step [334/750], Loss: 0.1021\n",
      "Epoch [4/8], Step [335/750], Loss: 0.0034\n",
      "Epoch [4/8], Step [336/750], Loss: 0.0083\n",
      "Epoch [4/8], Step [337/750], Loss: 0.0294\n",
      "Epoch [4/8], Step [338/750], Loss: 0.0148\n",
      "Epoch [4/8], Step [339/750], Loss: 0.0370\n",
      "Epoch [4/8], Step [340/750], Loss: 0.0640\n",
      "Epoch [4/8], Step [341/750], Loss: 0.0581\n",
      "Epoch [4/8], Step [342/750], Loss: 0.0102\n",
      "Epoch [4/8], Step [343/750], Loss: 0.0724\n",
      "Epoch [4/8], Step [344/750], Loss: 0.0049\n",
      "Epoch [4/8], Step [345/750], Loss: 0.0123\n",
      "Epoch [4/8], Step [346/750], Loss: 0.0235\n",
      "Epoch [4/8], Step [347/750], Loss: 0.0058\n",
      "Epoch [4/8], Step [348/750], Loss: 0.0526\n",
      "Epoch [4/8], Step [349/750], Loss: 0.0321\n",
      "Epoch [4/8], Step [350/750], Loss: 0.0156\n",
      "Epoch [4/8], Step [351/750], Loss: 0.0081\n",
      "Epoch [4/8], Step [352/750], Loss: 0.0294\n",
      "Epoch [4/8], Step [353/750], Loss: 0.0111\n",
      "Epoch [4/8], Step [354/750], Loss: 0.0129\n",
      "Epoch [4/8], Step [355/750], Loss: 0.0491\n",
      "Epoch [4/8], Step [356/750], Loss: 0.0698\n",
      "Epoch [4/8], Step [357/750], Loss: 0.0250\n",
      "Epoch [4/8], Step [358/750], Loss: 0.0579\n",
      "Epoch [4/8], Step [359/750], Loss: 0.0485\n",
      "Epoch [4/8], Step [360/750], Loss: 0.0527\n",
      "Epoch [4/8], Step [361/750], Loss: 0.0025\n",
      "Epoch [4/8], Step [362/750], Loss: 0.0472\n",
      "Epoch [4/8], Step [363/750], Loss: 0.0520\n",
      "Epoch [4/8], Step [364/750], Loss: 0.0191\n",
      "Epoch [4/8], Step [365/750], Loss: 0.0419\n",
      "Epoch [4/8], Step [366/750], Loss: 0.0042\n",
      "Epoch [4/8], Step [367/750], Loss: 0.0346\n",
      "Epoch [4/8], Step [368/750], Loss: 0.0454\n",
      "Epoch [4/8], Step [369/750], Loss: 0.0337\n",
      "Epoch [4/8], Step [370/750], Loss: 0.0286\n",
      "Epoch [4/8], Step [371/750], Loss: 0.0059\n",
      "Epoch [4/8], Step [372/750], Loss: 0.0036\n",
      "Epoch [4/8], Step [373/750], Loss: 0.1399\n",
      "Epoch [4/8], Step [374/750], Loss: 0.0067\n",
      "Epoch [4/8], Step [375/750], Loss: 0.0061\n",
      "Epoch [4/8], Step [376/750], Loss: 0.1429\n",
      "Epoch [4/8], Step [377/750], Loss: 0.1285\n",
      "Epoch [4/8], Step [378/750], Loss: 0.0095\n",
      "Epoch [4/8], Step [379/750], Loss: 0.0054\n",
      "Epoch [4/8], Step [380/750], Loss: 0.0252\n",
      "Epoch [4/8], Step [381/750], Loss: 0.0432\n",
      "Epoch [4/8], Step [382/750], Loss: 0.0418\n",
      "Epoch [4/8], Step [383/750], Loss: 0.0326\n",
      "Epoch [4/8], Step [384/750], Loss: 0.0291\n",
      "Epoch [4/8], Step [385/750], Loss: 0.1036\n",
      "Epoch [4/8], Step [386/750], Loss: 0.0957\n",
      "Epoch [4/8], Step [387/750], Loss: 0.0421\n",
      "Epoch [4/8], Step [388/750], Loss: 0.0122\n",
      "Epoch [4/8], Step [389/750], Loss: 0.0411\n",
      "Epoch [4/8], Step [390/750], Loss: 0.1167\n",
      "Epoch [4/8], Step [391/750], Loss: 0.0269\n",
      "Epoch [4/8], Step [392/750], Loss: 0.0983\n",
      "Epoch [4/8], Step [393/750], Loss: 0.0083\n",
      "Epoch [4/8], Step [394/750], Loss: 0.0325\n",
      "Epoch [4/8], Step [395/750], Loss: 0.0182\n",
      "Epoch [4/8], Step [396/750], Loss: 0.0129\n",
      "Epoch [4/8], Step [397/750], Loss: 0.0198\n",
      "Epoch [4/8], Step [398/750], Loss: 0.0461\n",
      "Epoch [4/8], Step [399/750], Loss: 0.0182\n",
      "Epoch [4/8], Step [400/750], Loss: 0.1729\n",
      "Epoch [4/8], Step [401/750], Loss: 0.0080\n",
      "Epoch [4/8], Step [402/750], Loss: 0.0128\n",
      "Epoch [4/8], Step [403/750], Loss: 0.0077\n",
      "Epoch [4/8], Step [404/750], Loss: 0.0214\n",
      "Epoch [4/8], Step [405/750], Loss: 0.0519\n",
      "Epoch [4/8], Step [406/750], Loss: 0.0312\n",
      "Epoch [4/8], Step [407/750], Loss: 0.0185\n",
      "Epoch [4/8], Step [408/750], Loss: 0.0237\n",
      "Epoch [4/8], Step [409/750], Loss: 0.0404\n",
      "Epoch [4/8], Step [410/750], Loss: 0.0121\n",
      "Epoch [4/8], Step [411/750], Loss: 0.0091\n",
      "Epoch [4/8], Step [412/750], Loss: 0.0044\n",
      "Epoch [4/8], Step [413/750], Loss: 0.0183\n",
      "Epoch [4/8], Step [414/750], Loss: 0.0167\n",
      "Epoch [4/8], Step [415/750], Loss: 0.1434\n",
      "Epoch [4/8], Step [416/750], Loss: 0.0049\n",
      "Epoch [4/8], Step [417/750], Loss: 0.0283\n",
      "Epoch [4/8], Step [418/750], Loss: 0.0968\n",
      "Epoch [4/8], Step [419/750], Loss: 0.0186\n",
      "Epoch [4/8], Step [420/750], Loss: 0.0688\n",
      "Epoch [4/8], Step [421/750], Loss: 0.0452\n",
      "Epoch [4/8], Step [422/750], Loss: 0.0764\n",
      "Epoch [4/8], Step [423/750], Loss: 0.0103\n",
      "Epoch [4/8], Step [424/750], Loss: 0.0393\n",
      "Epoch [4/8], Step [425/750], Loss: 0.0064\n",
      "Epoch [4/8], Step [426/750], Loss: 0.0433\n",
      "Epoch [4/8], Step [427/750], Loss: 0.0614\n",
      "Epoch [4/8], Step [428/750], Loss: 0.0262\n",
      "Epoch [4/8], Step [429/750], Loss: 0.0098\n",
      "Epoch [4/8], Step [430/750], Loss: 0.0035\n",
      "Epoch [4/8], Step [431/750], Loss: 0.0009\n",
      "Epoch [4/8], Step [432/750], Loss: 0.0030\n",
      "Epoch [4/8], Step [433/750], Loss: 0.0517\n",
      "Epoch [4/8], Step [434/750], Loss: 0.0034\n",
      "Epoch [4/8], Step [435/750], Loss: 0.1262\n",
      "Epoch [4/8], Step [436/750], Loss: 0.0023\n",
      "Epoch [4/8], Step [437/750], Loss: 0.0153\n",
      "Epoch [4/8], Step [438/750], Loss: 0.0618\n",
      "Epoch [4/8], Step [439/750], Loss: 0.0107\n",
      "Epoch [4/8], Step [440/750], Loss: 0.0281\n",
      "Epoch [4/8], Step [441/750], Loss: 0.0011\n",
      "Epoch [4/8], Step [442/750], Loss: 0.0312\n",
      "Epoch [4/8], Step [443/750], Loss: 0.0016\n",
      "Epoch [4/8], Step [444/750], Loss: 0.0483\n",
      "Epoch [4/8], Step [445/750], Loss: 0.0030\n",
      "Epoch [4/8], Step [446/750], Loss: 0.0888\n",
      "Epoch [4/8], Step [447/750], Loss: 0.0219\n",
      "Epoch [4/8], Step [448/750], Loss: 0.0456\n",
      "Epoch [4/8], Step [449/750], Loss: 0.0239\n",
      "Epoch [4/8], Step [450/750], Loss: 0.0058\n",
      "Epoch [4/8], Step [451/750], Loss: 0.0177\n",
      "Epoch [4/8], Step [452/750], Loss: 0.0377\n",
      "Epoch [4/8], Step [453/750], Loss: 0.0226\n",
      "Epoch [4/8], Step [454/750], Loss: 0.0191\n",
      "Epoch [4/8], Step [455/750], Loss: 0.0272\n",
      "Epoch [4/8], Step [456/750], Loss: 0.0026\n",
      "Epoch [4/8], Step [457/750], Loss: 0.0246\n",
      "Epoch [4/8], Step [458/750], Loss: 0.0429\n",
      "Epoch [4/8], Step [459/750], Loss: 0.0045\n",
      "Epoch [4/8], Step [460/750], Loss: 0.0347\n",
      "Epoch [4/8], Step [461/750], Loss: 0.1186\n",
      "Epoch [4/8], Step [462/750], Loss: 0.0038\n",
      "Epoch [4/8], Step [463/750], Loss: 0.0871\n",
      "Epoch [4/8], Step [464/750], Loss: 0.0057\n",
      "Epoch [4/8], Step [465/750], Loss: 0.0346\n",
      "Epoch [4/8], Step [466/750], Loss: 0.0274\n",
      "Epoch [4/8], Step [467/750], Loss: 0.0768\n",
      "Epoch [4/8], Step [468/750], Loss: 0.0796\n",
      "Epoch [4/8], Step [469/750], Loss: 0.0076\n",
      "Epoch [4/8], Step [470/750], Loss: 0.0227\n",
      "Epoch [4/8], Step [471/750], Loss: 0.0162\n",
      "Epoch [4/8], Step [472/750], Loss: 0.0029\n",
      "Epoch [4/8], Step [473/750], Loss: 0.0114\n",
      "Epoch [4/8], Step [474/750], Loss: 0.0073\n",
      "Epoch [4/8], Step [475/750], Loss: 0.1668\n",
      "Epoch [4/8], Step [476/750], Loss: 0.0205\n",
      "Epoch [4/8], Step [477/750], Loss: 0.0083\n",
      "Epoch [4/8], Step [478/750], Loss: 0.0178\n",
      "Epoch [4/8], Step [479/750], Loss: 0.0489\n",
      "Epoch [4/8], Step [480/750], Loss: 0.0466\n",
      "Epoch [4/8], Step [481/750], Loss: 0.0060\n",
      "Epoch [4/8], Step [482/750], Loss: 0.0615\n",
      "Epoch [4/8], Step [483/750], Loss: 0.0705\n",
      "Epoch [4/8], Step [484/750], Loss: 0.0030\n",
      "Epoch [4/8], Step [485/750], Loss: 0.0554\n",
      "Epoch [4/8], Step [486/750], Loss: 0.0339\n",
      "Epoch [4/8], Step [487/750], Loss: 0.0544\n",
      "Epoch [4/8], Step [488/750], Loss: 0.0958\n",
      "Epoch [4/8], Step [489/750], Loss: 0.0050\n",
      "Epoch [4/8], Step [490/750], Loss: 0.0338\n",
      "Epoch [4/8], Step [491/750], Loss: 0.0620\n",
      "Epoch [4/8], Step [492/750], Loss: 0.0928\n",
      "Epoch [4/8], Step [493/750], Loss: 0.0703\n",
      "Epoch [4/8], Step [494/750], Loss: 0.0316\n",
      "Epoch [4/8], Step [495/750], Loss: 0.0835\n",
      "Epoch [4/8], Step [496/750], Loss: 0.0452\n",
      "Epoch [4/8], Step [497/750], Loss: 0.0291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Step [498/750], Loss: 0.0113\n",
      "Epoch [4/8], Step [499/750], Loss: 0.0719\n",
      "Epoch [4/8], Step [500/750], Loss: 0.1079\n",
      "Epoch [4/8], Step [501/750], Loss: 0.0196\n",
      "Epoch [4/8], Step [502/750], Loss: 0.0546\n",
      "Epoch [4/8], Step [503/750], Loss: 0.0107\n",
      "Epoch [4/8], Step [504/750], Loss: 0.0159\n",
      "Epoch [4/8], Step [505/750], Loss: 0.0251\n",
      "Epoch [4/8], Step [506/750], Loss: 0.0270\n",
      "Epoch [4/8], Step [507/750], Loss: 0.0180\n",
      "Epoch [4/8], Step [508/750], Loss: 0.0837\n",
      "Epoch [4/8], Step [509/750], Loss: 0.0048\n",
      "Epoch [4/8], Step [510/750], Loss: 0.0452\n",
      "Epoch [4/8], Step [511/750], Loss: 0.0019\n",
      "Epoch [4/8], Step [512/750], Loss: 0.0399\n",
      "Epoch [4/8], Step [513/750], Loss: 0.1000\n",
      "Epoch [4/8], Step [514/750], Loss: 0.0550\n",
      "Epoch [4/8], Step [515/750], Loss: 0.1142\n",
      "Epoch [4/8], Step [516/750], Loss: 0.0813\n",
      "Epoch [4/8], Step [517/750], Loss: 0.0644\n",
      "Epoch [4/8], Step [518/750], Loss: 0.0071\n",
      "Epoch [4/8], Step [519/750], Loss: 0.1023\n",
      "Epoch [4/8], Step [520/750], Loss: 0.0416\n",
      "Epoch [4/8], Step [521/750], Loss: 0.0683\n",
      "Epoch [4/8], Step [522/750], Loss: 0.0849\n",
      "Epoch [4/8], Step [523/750], Loss: 0.0608\n",
      "Epoch [4/8], Step [524/750], Loss: 0.0466\n",
      "Epoch [4/8], Step [525/750], Loss: 0.2668\n",
      "Epoch [4/8], Step [526/750], Loss: 0.2069\n",
      "Epoch [4/8], Step [527/750], Loss: 0.0437\n",
      "Epoch [4/8], Step [528/750], Loss: 0.0038\n",
      "Epoch [4/8], Step [529/750], Loss: 0.0553\n",
      "Epoch [4/8], Step [530/750], Loss: 0.0074\n",
      "Epoch [4/8], Step [531/750], Loss: 0.1314\n",
      "Epoch [4/8], Step [532/750], Loss: 0.1119\n",
      "Epoch [4/8], Step [533/750], Loss: 0.0317\n",
      "Epoch [4/8], Step [534/750], Loss: 0.0520\n",
      "Epoch [4/8], Step [535/750], Loss: 0.0325\n",
      "Epoch [4/8], Step [536/750], Loss: 0.0955\n",
      "Epoch [4/8], Step [537/750], Loss: 0.1146\n",
      "Epoch [4/8], Step [538/750], Loss: 0.0810\n",
      "Epoch [4/8], Step [539/750], Loss: 0.1974\n",
      "Epoch [4/8], Step [540/750], Loss: 0.0670\n",
      "Epoch [4/8], Step [541/750], Loss: 0.0204\n",
      "Epoch [4/8], Step [542/750], Loss: 0.0068\n",
      "Epoch [4/8], Step [543/750], Loss: 0.0853\n",
      "Epoch [4/8], Step [544/750], Loss: 0.1209\n",
      "Epoch [4/8], Step [545/750], Loss: 0.0738\n",
      "Epoch [4/8], Step [546/750], Loss: 0.0047\n",
      "Epoch [4/8], Step [547/750], Loss: 0.1075\n",
      "Epoch [4/8], Step [548/750], Loss: 0.1394\n",
      "Epoch [4/8], Step [549/750], Loss: 0.0308\n",
      "Epoch [4/8], Step [550/750], Loss: 0.0215\n",
      "Epoch [4/8], Step [551/750], Loss: 0.0041\n",
      "Epoch [4/8], Step [552/750], Loss: 0.0336\n",
      "Epoch [4/8], Step [553/750], Loss: 0.0220\n",
      "Epoch [4/8], Step [554/750], Loss: 0.0541\n",
      "Epoch [4/8], Step [555/750], Loss: 0.0223\n",
      "Epoch [4/8], Step [556/750], Loss: 0.1033\n",
      "Epoch [4/8], Step [557/750], Loss: 0.0528\n",
      "Epoch [4/8], Step [558/750], Loss: 0.0619\n",
      "Epoch [4/8], Step [559/750], Loss: 0.0317\n",
      "Epoch [4/8], Step [560/750], Loss: 0.0607\n",
      "Epoch [4/8], Step [561/750], Loss: 0.0468\n",
      "Epoch [4/8], Step [562/750], Loss: 0.0516\n",
      "Epoch [4/8], Step [563/750], Loss: 0.0129\n",
      "Epoch [4/8], Step [564/750], Loss: 0.0037\n",
      "Epoch [4/8], Step [565/750], Loss: 0.0521\n",
      "Epoch [4/8], Step [566/750], Loss: 0.0216\n",
      "Epoch [4/8], Step [567/750], Loss: 0.0648\n",
      "Epoch [4/8], Step [568/750], Loss: 0.0465\n",
      "Epoch [4/8], Step [569/750], Loss: 0.0164\n",
      "Epoch [4/8], Step [570/750], Loss: 0.0977\n",
      "Epoch [4/8], Step [571/750], Loss: 0.0035\n",
      "Epoch [4/8], Step [572/750], Loss: 0.0287\n",
      "Epoch [4/8], Step [573/750], Loss: 0.0043\n",
      "Epoch [4/8], Step [574/750], Loss: 0.0512\n",
      "Epoch [4/8], Step [575/750], Loss: 0.0211\n",
      "Epoch [4/8], Step [576/750], Loss: 0.0428\n",
      "Epoch [4/8], Step [577/750], Loss: 0.0904\n",
      "Epoch [4/8], Step [578/750], Loss: 0.0239\n",
      "Epoch [4/8], Step [579/750], Loss: 0.0324\n",
      "Epoch [4/8], Step [580/750], Loss: 0.0677\n",
      "Epoch [4/8], Step [581/750], Loss: 0.0053\n",
      "Epoch [4/8], Step [582/750], Loss: 0.0204\n",
      "Epoch [4/8], Step [583/750], Loss: 0.0505\n",
      "Epoch [4/8], Step [584/750], Loss: 0.0264\n",
      "Epoch [4/8], Step [585/750], Loss: 0.0190\n",
      "Epoch [4/8], Step [586/750], Loss: 0.0327\n",
      "Epoch [4/8], Step [587/750], Loss: 0.0013\n",
      "Epoch [4/8], Step [588/750], Loss: 0.0226\n",
      "Epoch [4/8], Step [589/750], Loss: 0.0029\n",
      "Epoch [4/8], Step [590/750], Loss: 0.0183\n",
      "Epoch [4/8], Step [591/750], Loss: 0.0930\n",
      "Epoch [4/8], Step [592/750], Loss: 0.0532\n",
      "Epoch [4/8], Step [593/750], Loss: 0.0136\n",
      "Epoch [4/8], Step [594/750], Loss: 0.0262\n",
      "Epoch [4/8], Step [595/750], Loss: 0.0240\n",
      "Epoch [4/8], Step [596/750], Loss: 0.0047\n",
      "Epoch [4/8], Step [597/750], Loss: 0.0253\n",
      "Epoch [4/8], Step [598/750], Loss: 0.0322\n",
      "Epoch [4/8], Step [599/750], Loss: 0.0135\n",
      "Epoch [4/8], Step [600/750], Loss: 0.0213\n",
      "Epoch [4/8], Step [601/750], Loss: 0.0256\n",
      "Epoch [4/8], Step [602/750], Loss: 0.0164\n",
      "Epoch [4/8], Step [603/750], Loss: 0.0140\n",
      "Epoch [4/8], Step [604/750], Loss: 0.0278\n",
      "Epoch [4/8], Step [605/750], Loss: 0.0396\n",
      "Epoch [4/8], Step [606/750], Loss: 0.0099\n",
      "Epoch [4/8], Step [607/750], Loss: 0.0670\n",
      "Epoch [4/8], Step [608/750], Loss: 0.0126\n",
      "Epoch [4/8], Step [609/750], Loss: 0.0374\n",
      "Epoch [4/8], Step [610/750], Loss: 0.0255\n",
      "Epoch [4/8], Step [611/750], Loss: 0.0118\n",
      "Epoch [4/8], Step [612/750], Loss: 0.0821\n",
      "Epoch [4/8], Step [613/750], Loss: 0.0267\n",
      "Epoch [4/8], Step [614/750], Loss: 0.0163\n",
      "Epoch [4/8], Step [615/750], Loss: 0.0931\n",
      "Epoch [4/8], Step [616/750], Loss: 0.1000\n",
      "Epoch [4/8], Step [617/750], Loss: 0.0129\n",
      "Epoch [4/8], Step [618/750], Loss: 0.0160\n",
      "Epoch [4/8], Step [619/750], Loss: 0.0230\n",
      "Epoch [4/8], Step [620/750], Loss: 0.0365\n",
      "Epoch [4/8], Step [621/750], Loss: 0.0206\n",
      "Epoch [4/8], Step [622/750], Loss: 0.0069\n",
      "Epoch [4/8], Step [623/750], Loss: 0.0388\n",
      "Epoch [4/8], Step [624/750], Loss: 0.0110\n",
      "Epoch [4/8], Step [625/750], Loss: 0.0197\n",
      "Epoch [4/8], Step [626/750], Loss: 0.1397\n",
      "Epoch [4/8], Step [627/750], Loss: 0.0213\n",
      "Epoch [4/8], Step [628/750], Loss: 0.0450\n",
      "Epoch [4/8], Step [629/750], Loss: 0.0046\n",
      "Epoch [4/8], Step [630/750], Loss: 0.0204\n",
      "Epoch [4/8], Step [631/750], Loss: 0.1611\n",
      "Epoch [4/8], Step [632/750], Loss: 0.0112\n",
      "Epoch [4/8], Step [633/750], Loss: 0.0037\n",
      "Epoch [4/8], Step [634/750], Loss: 0.3344\n",
      "Epoch [4/8], Step [635/750], Loss: 0.1580\n",
      "Epoch [4/8], Step [636/750], Loss: 0.0303\n",
      "Epoch [4/8], Step [637/750], Loss: 0.0365\n",
      "Epoch [4/8], Step [638/750], Loss: 0.0215\n",
      "Epoch [4/8], Step [639/750], Loss: 0.0094\n",
      "Epoch [4/8], Step [640/750], Loss: 0.0209\n",
      "Epoch [4/8], Step [641/750], Loss: 0.0478\n",
      "Epoch [4/8], Step [642/750], Loss: 0.0061\n",
      "Epoch [4/8], Step [643/750], Loss: 0.0320\n",
      "Epoch [4/8], Step [644/750], Loss: 0.0696\n",
      "Epoch [4/8], Step [645/750], Loss: 0.0443\n",
      "Epoch [4/8], Step [646/750], Loss: 0.0499\n",
      "Epoch [4/8], Step [647/750], Loss: 0.0921\n",
      "Epoch [4/8], Step [648/750], Loss: 0.0271\n",
      "Epoch [4/8], Step [649/750], Loss: 0.0067\n",
      "Epoch [4/8], Step [650/750], Loss: 0.0462\n",
      "Epoch [4/8], Step [651/750], Loss: 0.0086\n",
      "Epoch [4/8], Step [652/750], Loss: 0.0065\n",
      "Epoch [4/8], Step [653/750], Loss: 0.0063\n",
      "Epoch [4/8], Step [654/750], Loss: 0.0225\n",
      "Epoch [4/8], Step [655/750], Loss: 0.0131\n",
      "Epoch [4/8], Step [656/750], Loss: 0.1365\n",
      "Epoch [4/8], Step [657/750], Loss: 0.0280\n",
      "Epoch [4/8], Step [658/750], Loss: 0.0055\n",
      "Epoch [4/8], Step [659/750], Loss: 0.2134\n",
      "Epoch [4/8], Step [660/750], Loss: 0.0086\n",
      "Epoch [4/8], Step [661/750], Loss: 0.0568\n",
      "Epoch [4/8], Step [662/750], Loss: 0.0075\n",
      "Epoch [4/8], Step [663/750], Loss: 0.0138\n",
      "Epoch [4/8], Step [664/750], Loss: 0.0463\n",
      "Epoch [4/8], Step [665/750], Loss: 0.0225\n",
      "Epoch [4/8], Step [666/750], Loss: 0.0015\n",
      "Epoch [4/8], Step [667/750], Loss: 0.0010\n",
      "Epoch [4/8], Step [668/750], Loss: 0.0036\n",
      "Epoch [4/8], Step [669/750], Loss: 0.0409\n",
      "Epoch [4/8], Step [670/750], Loss: 0.0113\n",
      "Epoch [4/8], Step [671/750], Loss: 0.0090\n",
      "Epoch [4/8], Step [672/750], Loss: 0.0357\n",
      "Epoch [4/8], Step [673/750], Loss: 0.0506\n",
      "Epoch [4/8], Step [674/750], Loss: 0.0105\n",
      "Epoch [4/8], Step [675/750], Loss: 0.0757\n",
      "Epoch [4/8], Step [676/750], Loss: 0.0166\n",
      "Epoch [4/8], Step [677/750], Loss: 0.0311\n",
      "Epoch [4/8], Step [678/750], Loss: 0.0211\n",
      "Epoch [4/8], Step [679/750], Loss: 0.0256\n",
      "Epoch [4/8], Step [680/750], Loss: 0.0036\n",
      "Epoch [4/8], Step [681/750], Loss: 0.0540\n",
      "Epoch [4/8], Step [682/750], Loss: 0.0079\n",
      "Epoch [4/8], Step [683/750], Loss: 0.0499\n",
      "Epoch [4/8], Step [684/750], Loss: 0.0557\n",
      "Epoch [4/8], Step [685/750], Loss: 0.0311\n",
      "Epoch [4/8], Step [686/750], Loss: 0.0691\n",
      "Epoch [4/8], Step [687/750], Loss: 0.0026\n",
      "Epoch [4/8], Step [688/750], Loss: 0.0778\n",
      "Epoch [4/8], Step [689/750], Loss: 0.0010\n",
      "Epoch [4/8], Step [690/750], Loss: 0.1037\n",
      "Epoch [4/8], Step [691/750], Loss: 0.1635\n",
      "Epoch [4/8], Step [692/750], Loss: 0.0065\n",
      "Epoch [4/8], Step [693/750], Loss: 0.0418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Step [694/750], Loss: 0.0104\n",
      "Epoch [4/8], Step [695/750], Loss: 0.0438\n",
      "Epoch [4/8], Step [696/750], Loss: 0.0837\n",
      "Epoch [4/8], Step [697/750], Loss: 0.0577\n",
      "Epoch [4/8], Step [698/750], Loss: 0.0718\n",
      "Epoch [4/8], Step [699/750], Loss: 0.0288\n",
      "Epoch [4/8], Step [700/750], Loss: 0.0144\n",
      "Epoch [4/8], Step [701/750], Loss: 0.1033\n",
      "Epoch [4/8], Step [702/750], Loss: 0.0210\n",
      "Epoch [4/8], Step [703/750], Loss: 0.2065\n",
      "Epoch [4/8], Step [704/750], Loss: 0.0616\n",
      "Epoch [4/8], Step [705/750], Loss: 0.0403\n",
      "Epoch [4/8], Step [706/750], Loss: 0.0660\n",
      "Epoch [4/8], Step [707/750], Loss: 0.0576\n",
      "Epoch [4/8], Step [708/750], Loss: 0.0050\n",
      "Epoch [4/8], Step [709/750], Loss: 0.0722\n",
      "Epoch [4/8], Step [710/750], Loss: 0.0275\n",
      "Epoch [4/8], Step [711/750], Loss: 0.0246\n",
      "Epoch [4/8], Step [712/750], Loss: 0.0146\n",
      "Epoch [4/8], Step [713/750], Loss: 0.0138\n",
      "Epoch [4/8], Step [714/750], Loss: 0.0417\n",
      "Epoch [4/8], Step [715/750], Loss: 0.0295\n",
      "Epoch [4/8], Step [716/750], Loss: 0.0668\n",
      "Epoch [4/8], Step [717/750], Loss: 0.0223\n",
      "Epoch [4/8], Step [718/750], Loss: 0.0346\n",
      "Epoch [4/8], Step [719/750], Loss: 0.0123\n",
      "Epoch [4/8], Step [720/750], Loss: 0.0263\n",
      "Epoch [4/8], Step [721/750], Loss: 0.0164\n",
      "Epoch [4/8], Step [722/750], Loss: 0.0238\n",
      "Epoch [4/8], Step [723/750], Loss: 0.0025\n",
      "Epoch [4/8], Step [724/750], Loss: 0.0064\n",
      "Epoch [4/8], Step [725/750], Loss: 0.0197\n",
      "Epoch [4/8], Step [726/750], Loss: 0.0557\n",
      "Epoch [4/8], Step [727/750], Loss: 0.0240\n",
      "Epoch [4/8], Step [728/750], Loss: 0.0177\n",
      "Epoch [4/8], Step [729/750], Loss: 0.0038\n",
      "Epoch [4/8], Step [730/750], Loss: 0.0188\n",
      "Epoch [4/8], Step [731/750], Loss: 0.0026\n",
      "Epoch [4/8], Step [732/750], Loss: 0.0304\n",
      "Epoch [4/8], Step [733/750], Loss: 0.1420\n",
      "Epoch [4/8], Step [734/750], Loss: 0.1675\n",
      "Epoch [4/8], Step [735/750], Loss: 0.0106\n",
      "Epoch [4/8], Step [736/750], Loss: 0.0828\n",
      "Epoch [4/8], Step [737/750], Loss: 0.0062\n",
      "Epoch [4/8], Step [738/750], Loss: 0.0929\n",
      "Epoch [4/8], Step [739/750], Loss: 0.0463\n",
      "Epoch [4/8], Step [740/750], Loss: 0.0248\n",
      "Epoch [4/8], Step [741/750], Loss: 0.0686\n",
      "Epoch [4/8], Step [742/750], Loss: 0.0171\n",
      "Epoch [4/8], Step [743/750], Loss: 0.0112\n",
      "Epoch [4/8], Step [744/750], Loss: 0.0319\n",
      "Epoch [4/8], Step [745/750], Loss: 0.0317\n",
      "Epoch [4/8], Step [746/750], Loss: 0.0424\n",
      "Epoch [4/8], Step [747/750], Loss: 0.0306\n",
      "Epoch [4/8], Step [748/750], Loss: 0.0217\n",
      "Epoch [4/8], Step [749/750], Loss: 0.0062\n",
      "Epoch [4/8], Step [750/750], Loss: 0.0163\n",
      "Epoch [4/8], Tr. loss: 0.3060. Test loss: 0.1855\n",
      "\n",
      "\n",
      "Epoch [5/8], Step [1/750], Loss: 0.0092\n",
      "Epoch [5/8], Step [2/750], Loss: 0.0329\n",
      "Epoch [5/8], Step [3/750], Loss: 0.0368\n",
      "Epoch [5/8], Step [4/750], Loss: 0.2074\n",
      "Epoch [5/8], Step [5/750], Loss: 0.0125\n",
      "Epoch [5/8], Step [6/750], Loss: 0.0166\n",
      "Epoch [5/8], Step [7/750], Loss: 0.0215\n",
      "Epoch [5/8], Step [8/750], Loss: 0.0074\n",
      "Epoch [5/8], Step [9/750], Loss: 0.0196\n",
      "Epoch [5/8], Step [10/750], Loss: 0.0658\n",
      "Epoch [5/8], Step [11/750], Loss: 0.0356\n",
      "Epoch [5/8], Step [12/750], Loss: 0.0266\n",
      "Epoch [5/8], Step [13/750], Loss: 0.3159\n",
      "Epoch [5/8], Step [14/750], Loss: 0.0165\n",
      "Epoch [5/8], Step [15/750], Loss: 0.0040\n",
      "Epoch [5/8], Step [16/750], Loss: 0.0203\n",
      "Epoch [5/8], Step [17/750], Loss: 0.0393\n",
      "Epoch [5/8], Step [18/750], Loss: 0.0117\n",
      "Epoch [5/8], Step [19/750], Loss: 0.1330\n",
      "Epoch [5/8], Step [20/750], Loss: 0.0503\n",
      "Epoch [5/8], Step [21/750], Loss: 0.0271\n",
      "Epoch [5/8], Step [22/750], Loss: 0.0194\n",
      "Epoch [5/8], Step [23/750], Loss: 0.0039\n",
      "Epoch [5/8], Step [24/750], Loss: 0.0270\n",
      "Epoch [5/8], Step [25/750], Loss: 0.0038\n",
      "Epoch [5/8], Step [26/750], Loss: 0.0085\n",
      "Epoch [5/8], Step [27/750], Loss: 0.0223\n",
      "Epoch [5/8], Step [28/750], Loss: 0.0973\n",
      "Epoch [5/8], Step [29/750], Loss: 0.0054\n",
      "Epoch [5/8], Step [30/750], Loss: 0.0099\n",
      "Epoch [5/8], Step [31/750], Loss: 0.0882\n",
      "Epoch [5/8], Step [32/750], Loss: 0.0493\n",
      "Epoch [5/8], Step [33/750], Loss: 0.0204\n",
      "Epoch [5/8], Step [34/750], Loss: 0.0696\n",
      "Epoch [5/8], Step [35/750], Loss: 0.0158\n",
      "Epoch [5/8], Step [36/750], Loss: 0.0062\n",
      "Epoch [5/8], Step [37/750], Loss: 0.0135\n",
      "Epoch [5/8], Step [38/750], Loss: 0.0083\n",
      "Epoch [5/8], Step [39/750], Loss: 0.0024\n",
      "Epoch [5/8], Step [40/750], Loss: 0.0616\n",
      "Epoch [5/8], Step [41/750], Loss: 0.0438\n",
      "Epoch [5/8], Step [42/750], Loss: 0.0350\n",
      "Epoch [5/8], Step [43/750], Loss: 0.0431\n",
      "Epoch [5/8], Step [44/750], Loss: 0.0305\n",
      "Epoch [5/8], Step [45/750], Loss: 0.1019\n",
      "Epoch [5/8], Step [46/750], Loss: 0.0028\n",
      "Epoch [5/8], Step [47/750], Loss: 0.0056\n",
      "Epoch [5/8], Step [48/750], Loss: 0.0306\n",
      "Epoch [5/8], Step [49/750], Loss: 0.0700\n",
      "Epoch [5/8], Step [50/750], Loss: 0.0222\n",
      "Epoch [5/8], Step [51/750], Loss: 0.0085\n",
      "Epoch [5/8], Step [52/750], Loss: 0.0047\n",
      "Epoch [5/8], Step [53/750], Loss: 0.0143\n",
      "Epoch [5/8], Step [54/750], Loss: 0.0071\n",
      "Epoch [5/8], Step [55/750], Loss: 0.0391\n",
      "Epoch [5/8], Step [56/750], Loss: 0.0322\n",
      "Epoch [5/8], Step [57/750], Loss: 0.0953\n",
      "Epoch [5/8], Step [58/750], Loss: 0.0024\n",
      "Epoch [5/8], Step [59/750], Loss: 0.0162\n",
      "Epoch [5/8], Step [60/750], Loss: 0.0603\n",
      "Epoch [5/8], Step [61/750], Loss: 0.0178\n",
      "Epoch [5/8], Step [62/750], Loss: 0.0506\n",
      "Epoch [5/8], Step [63/750], Loss: 0.0022\n",
      "Epoch [5/8], Step [64/750], Loss: 0.0615\n",
      "Epoch [5/8], Step [65/750], Loss: 0.0135\n",
      "Epoch [5/8], Step [66/750], Loss: 0.0610\n",
      "Epoch [5/8], Step [67/750], Loss: 0.1296\n",
      "Epoch [5/8], Step [68/750], Loss: 0.0261\n",
      "Epoch [5/8], Step [69/750], Loss: 0.0212\n",
      "Epoch [5/8], Step [70/750], Loss: 0.0023\n",
      "Epoch [5/8], Step [71/750], Loss: 0.0023\n",
      "Epoch [5/8], Step [72/750], Loss: 0.0047\n",
      "Epoch [5/8], Step [73/750], Loss: 0.0765\n",
      "Epoch [5/8], Step [74/750], Loss: 0.1196\n",
      "Epoch [5/8], Step [75/750], Loss: 0.0296\n",
      "Epoch [5/8], Step [76/750], Loss: 0.0047\n",
      "Epoch [5/8], Step [77/750], Loss: 0.0067\n",
      "Epoch [5/8], Step [78/750], Loss: 0.1574\n",
      "Epoch [5/8], Step [79/750], Loss: 0.0211\n",
      "Epoch [5/8], Step [80/750], Loss: 0.0716\n",
      "Epoch [5/8], Step [81/750], Loss: 0.0203\n",
      "Epoch [5/8], Step [82/750], Loss: 0.0587\n",
      "Epoch [5/8], Step [83/750], Loss: 0.0194\n",
      "Epoch [5/8], Step [84/750], Loss: 0.1658\n",
      "Epoch [5/8], Step [85/750], Loss: 0.0205\n",
      "Epoch [5/8], Step [86/750], Loss: 0.0091\n",
      "Epoch [5/8], Step [87/750], Loss: 0.0461\n",
      "Epoch [5/8], Step [88/750], Loss: 0.0146\n",
      "Epoch [5/8], Step [89/750], Loss: 0.0024\n",
      "Epoch [5/8], Step [90/750], Loss: 0.1165\n",
      "Epoch [5/8], Step [91/750], Loss: 0.0101\n",
      "Epoch [5/8], Step [92/750], Loss: 0.0303\n",
      "Epoch [5/8], Step [93/750], Loss: 0.0286\n",
      "Epoch [5/8], Step [94/750], Loss: 0.0373\n",
      "Epoch [5/8], Step [95/750], Loss: 0.0116\n",
      "Epoch [5/8], Step [96/750], Loss: 0.0116\n",
      "Epoch [5/8], Step [97/750], Loss: 0.0569\n",
      "Epoch [5/8], Step [98/750], Loss: 0.1332\n",
      "Epoch [5/8], Step [99/750], Loss: 0.0178\n",
      "Epoch [5/8], Step [100/750], Loss: 0.0260\n",
      "Epoch [5/8], Step [101/750], Loss: 0.0507\n",
      "Epoch [5/8], Step [102/750], Loss: 0.0195\n",
      "Epoch [5/8], Step [103/750], Loss: 0.0024\n",
      "Epoch [5/8], Step [104/750], Loss: 0.0902\n",
      "Epoch [5/8], Step [105/750], Loss: 0.0031\n",
      "Epoch [5/8], Step [106/750], Loss: 0.0107\n",
      "Epoch [5/8], Step [107/750], Loss: 0.0428\n",
      "Epoch [5/8], Step [108/750], Loss: 0.0057\n",
      "Epoch [5/8], Step [109/750], Loss: 0.0214\n",
      "Epoch [5/8], Step [110/750], Loss: 0.0065\n",
      "Epoch [5/8], Step [111/750], Loss: 0.0107\n",
      "Epoch [5/8], Step [112/750], Loss: 0.0189\n",
      "Epoch [5/8], Step [113/750], Loss: 0.0470\n",
      "Epoch [5/8], Step [114/750], Loss: 0.0021\n",
      "Epoch [5/8], Step [115/750], Loss: 0.0031\n",
      "Epoch [5/8], Step [116/750], Loss: 0.0311\n",
      "Epoch [5/8], Step [117/750], Loss: 0.0065\n",
      "Epoch [5/8], Step [118/750], Loss: 0.0087\n",
      "Epoch [5/8], Step [119/750], Loss: 0.0352\n",
      "Epoch [5/8], Step [120/750], Loss: 0.0063\n",
      "Epoch [5/8], Step [121/750], Loss: 0.0153\n",
      "Epoch [5/8], Step [122/750], Loss: 0.0305\n",
      "Epoch [5/8], Step [123/750], Loss: 0.0366\n",
      "Epoch [5/8], Step [124/750], Loss: 0.0014\n",
      "Epoch [5/8], Step [125/750], Loss: 0.0391\n",
      "Epoch [5/8], Step [126/750], Loss: 0.0504\n",
      "Epoch [5/8], Step [127/750], Loss: 0.0395\n",
      "Epoch [5/8], Step [128/750], Loss: 0.0440\n",
      "Epoch [5/8], Step [129/750], Loss: 0.0048\n",
      "Epoch [5/8], Step [130/750], Loss: 0.0400\n",
      "Epoch [5/8], Step [131/750], Loss: 0.0103\n",
      "Epoch [5/8], Step [132/750], Loss: 0.0537\n",
      "Epoch [5/8], Step [133/750], Loss: 0.0101\n",
      "Epoch [5/8], Step [134/750], Loss: 0.0021\n",
      "Epoch [5/8], Step [135/750], Loss: 0.0133\n",
      "Epoch [5/8], Step [136/750], Loss: 0.0219\n",
      "Epoch [5/8], Step [137/750], Loss: 0.0035\n",
      "Epoch [5/8], Step [138/750], Loss: 0.0153\n",
      "Epoch [5/8], Step [139/750], Loss: 0.0102\n",
      "Epoch [5/8], Step [140/750], Loss: 0.0018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Step [141/750], Loss: 0.0283\n",
      "Epoch [5/8], Step [142/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [143/750], Loss: 0.0043\n",
      "Epoch [5/8], Step [144/750], Loss: 0.0038\n",
      "Epoch [5/8], Step [145/750], Loss: 0.1277\n",
      "Epoch [5/8], Step [146/750], Loss: 0.0125\n",
      "Epoch [5/8], Step [147/750], Loss: 0.0010\n",
      "Epoch [5/8], Step [148/750], Loss: 0.0219\n",
      "Epoch [5/8], Step [149/750], Loss: 0.0177\n",
      "Epoch [5/8], Step [150/750], Loss: 0.0350\n",
      "Epoch [5/8], Step [151/750], Loss: 0.0082\n",
      "Epoch [5/8], Step [152/750], Loss: 0.0201\n",
      "Epoch [5/8], Step [153/750], Loss: 0.0401\n",
      "Epoch [5/8], Step [154/750], Loss: 0.0050\n",
      "Epoch [5/8], Step [155/750], Loss: 0.0230\n",
      "Epoch [5/8], Step [156/750], Loss: 0.0092\n",
      "Epoch [5/8], Step [157/750], Loss: 0.0015\n",
      "Epoch [5/8], Step [158/750], Loss: 0.0041\n",
      "Epoch [5/8], Step [159/750], Loss: 0.0075\n",
      "Epoch [5/8], Step [160/750], Loss: 0.0250\n",
      "Epoch [5/8], Step [161/750], Loss: 0.0326\n",
      "Epoch [5/8], Step [162/750], Loss: 0.0597\n",
      "Epoch [5/8], Step [163/750], Loss: 0.0801\n",
      "Epoch [5/8], Step [164/750], Loss: 0.0041\n",
      "Epoch [5/8], Step [165/750], Loss: 0.0125\n",
      "Epoch [5/8], Step [166/750], Loss: 0.0070\n",
      "Epoch [5/8], Step [167/750], Loss: 0.0551\n",
      "Epoch [5/8], Step [168/750], Loss: 0.0382\n",
      "Epoch [5/8], Step [169/750], Loss: 0.1276\n",
      "Epoch [5/8], Step [170/750], Loss: 0.0094\n",
      "Epoch [5/8], Step [171/750], Loss: 0.0034\n",
      "Epoch [5/8], Step [172/750], Loss: 0.0327\n",
      "Epoch [5/8], Step [173/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [174/750], Loss: 0.0294\n",
      "Epoch [5/8], Step [175/750], Loss: 0.0216\n",
      "Epoch [5/8], Step [176/750], Loss: 0.0415\n",
      "Epoch [5/8], Step [177/750], Loss: 0.0083\n",
      "Epoch [5/8], Step [178/750], Loss: 0.0208\n",
      "Epoch [5/8], Step [179/750], Loss: 0.0457\n",
      "Epoch [5/8], Step [180/750], Loss: 0.0147\n",
      "Epoch [5/8], Step [181/750], Loss: 0.0636\n",
      "Epoch [5/8], Step [182/750], Loss: 0.0560\n",
      "Epoch [5/8], Step [183/750], Loss: 0.0411\n",
      "Epoch [5/8], Step [184/750], Loss: 0.0153\n",
      "Epoch [5/8], Step [185/750], Loss: 0.0658\n",
      "Epoch [5/8], Step [186/750], Loss: 0.1285\n",
      "Epoch [5/8], Step [187/750], Loss: 0.0229\n",
      "Epoch [5/8], Step [188/750], Loss: 0.0087\n",
      "Epoch [5/8], Step [189/750], Loss: 0.0381\n",
      "Epoch [5/8], Step [190/750], Loss: 0.0144\n",
      "Epoch [5/8], Step [191/750], Loss: 0.0147\n",
      "Epoch [5/8], Step [192/750], Loss: 0.1030\n",
      "Epoch [5/8], Step [193/750], Loss: 0.0224\n",
      "Epoch [5/8], Step [194/750], Loss: 0.0447\n",
      "Epoch [5/8], Step [195/750], Loss: 0.0089\n",
      "Epoch [5/8], Step [196/750], Loss: 0.0866\n",
      "Epoch [5/8], Step [197/750], Loss: 0.0094\n",
      "Epoch [5/8], Step [198/750], Loss: 0.0081\n",
      "Epoch [5/8], Step [199/750], Loss: 0.0196\n",
      "Epoch [5/8], Step [200/750], Loss: 0.0339\n",
      "Epoch [5/8], Step [201/750], Loss: 0.0275\n",
      "Epoch [5/8], Step [202/750], Loss: 0.0665\n",
      "Epoch [5/8], Step [203/750], Loss: 0.0131\n",
      "Epoch [5/8], Step [204/750], Loss: 0.0096\n",
      "Epoch [5/8], Step [205/750], Loss: 0.0630\n",
      "Epoch [5/8], Step [206/750], Loss: 0.0156\n",
      "Epoch [5/8], Step [207/750], Loss: 0.0353\n",
      "Epoch [5/8], Step [208/750], Loss: 0.1067\n",
      "Epoch [5/8], Step [209/750], Loss: 0.0302\n",
      "Epoch [5/8], Step [210/750], Loss: 0.0135\n",
      "Epoch [5/8], Step [211/750], Loss: 0.0331\n",
      "Epoch [5/8], Step [212/750], Loss: 0.0181\n",
      "Epoch [5/8], Step [213/750], Loss: 0.0295\n",
      "Epoch [5/8], Step [214/750], Loss: 0.0671\n",
      "Epoch [5/8], Step [215/750], Loss: 0.0151\n",
      "Epoch [5/8], Step [216/750], Loss: 0.0481\n",
      "Epoch [5/8], Step [217/750], Loss: 0.0160\n",
      "Epoch [5/8], Step [218/750], Loss: 0.0066\n",
      "Epoch [5/8], Step [219/750], Loss: 0.0075\n",
      "Epoch [5/8], Step [220/750], Loss: 0.0168\n",
      "Epoch [5/8], Step [221/750], Loss: 0.0103\n",
      "Epoch [5/8], Step [222/750], Loss: 0.0156\n",
      "Epoch [5/8], Step [223/750], Loss: 0.0265\n",
      "Epoch [5/8], Step [224/750], Loss: 0.0334\n",
      "Epoch [5/8], Step [225/750], Loss: 0.0173\n",
      "Epoch [5/8], Step [226/750], Loss: 0.0113\n",
      "Epoch [5/8], Step [227/750], Loss: 0.0091\n",
      "Epoch [5/8], Step [228/750], Loss: 0.0646\n",
      "Epoch [5/8], Step [229/750], Loss: 0.0290\n",
      "Epoch [5/8], Step [230/750], Loss: 0.0016\n",
      "Epoch [5/8], Step [231/750], Loss: 0.0071\n",
      "Epoch [5/8], Step [232/750], Loss: 0.0786\n",
      "Epoch [5/8], Step [233/750], Loss: 0.0052\n",
      "Epoch [5/8], Step [234/750], Loss: 0.0504\n",
      "Epoch [5/8], Step [235/750], Loss: 0.0080\n",
      "Epoch [5/8], Step [236/750], Loss: 0.0029\n",
      "Epoch [5/8], Step [237/750], Loss: 0.0672\n",
      "Epoch [5/8], Step [238/750], Loss: 0.0301\n",
      "Epoch [5/8], Step [239/750], Loss: 0.0155\n",
      "Epoch [5/8], Step [240/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [241/750], Loss: 0.0167\n",
      "Epoch [5/8], Step [242/750], Loss: 0.0100\n",
      "Epoch [5/8], Step [243/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [244/750], Loss: 0.0070\n",
      "Epoch [5/8], Step [245/750], Loss: 0.0371\n",
      "Epoch [5/8], Step [246/750], Loss: 0.0101\n",
      "Epoch [5/8], Step [247/750], Loss: 0.0078\n",
      "Epoch [5/8], Step [248/750], Loss: 0.0529\n",
      "Epoch [5/8], Step [249/750], Loss: 0.0454\n",
      "Epoch [5/8], Step [250/750], Loss: 0.1178\n",
      "Epoch [5/8], Step [251/750], Loss: 0.0081\n",
      "Epoch [5/8], Step [252/750], Loss: 0.0818\n",
      "Epoch [5/8], Step [253/750], Loss: 0.0048\n",
      "Epoch [5/8], Step [254/750], Loss: 0.0199\n",
      "Epoch [5/8], Step [255/750], Loss: 0.0353\n",
      "Epoch [5/8], Step [256/750], Loss: 0.0033\n",
      "Epoch [5/8], Step [257/750], Loss: 0.0112\n",
      "Epoch [5/8], Step [258/750], Loss: 0.0384\n",
      "Epoch [5/8], Step [259/750], Loss: 0.0707\n",
      "Epoch [5/8], Step [260/750], Loss: 0.0068\n",
      "Epoch [5/8], Step [261/750], Loss: 0.0015\n",
      "Epoch [5/8], Step [262/750], Loss: 0.0036\n",
      "Epoch [5/8], Step [263/750], Loss: 0.0072\n",
      "Epoch [5/8], Step [264/750], Loss: 0.0376\n",
      "Epoch [5/8], Step [265/750], Loss: 0.0038\n",
      "Epoch [5/8], Step [266/750], Loss: 0.1125\n",
      "Epoch [5/8], Step [267/750], Loss: 0.0091\n",
      "Epoch [5/8], Step [268/750], Loss: 0.0009\n",
      "Epoch [5/8], Step [269/750], Loss: 0.0600\n",
      "Epoch [5/8], Step [270/750], Loss: 0.0019\n",
      "Epoch [5/8], Step [271/750], Loss: 0.0048\n",
      "Epoch [5/8], Step [272/750], Loss: 0.0051\n",
      "Epoch [5/8], Step [273/750], Loss: 0.0177\n",
      "Epoch [5/8], Step [274/750], Loss: 0.0251\n",
      "Epoch [5/8], Step [275/750], Loss: 0.0105\n",
      "Epoch [5/8], Step [276/750], Loss: 0.0326\n",
      "Epoch [5/8], Step [277/750], Loss: 0.0103\n",
      "Epoch [5/8], Step [278/750], Loss: 0.0064\n",
      "Epoch [5/8], Step [279/750], Loss: 0.0028\n",
      "Epoch [5/8], Step [280/750], Loss: 0.0352\n",
      "Epoch [5/8], Step [281/750], Loss: 0.0688\n",
      "Epoch [5/8], Step [282/750], Loss: 0.0530\n",
      "Epoch [5/8], Step [283/750], Loss: 0.0055\n",
      "Epoch [5/8], Step [284/750], Loss: 0.0038\n",
      "Epoch [5/8], Step [285/750], Loss: 0.0081\n",
      "Epoch [5/8], Step [286/750], Loss: 0.0030\n",
      "Epoch [5/8], Step [287/750], Loss: 0.0242\n",
      "Epoch [5/8], Step [288/750], Loss: 0.0087\n",
      "Epoch [5/8], Step [289/750], Loss: 0.0053\n",
      "Epoch [5/8], Step [290/750], Loss: 0.0057\n",
      "Epoch [5/8], Step [291/750], Loss: 0.0685\n",
      "Epoch [5/8], Step [292/750], Loss: 0.0905\n",
      "Epoch [5/8], Step [293/750], Loss: 0.0268\n",
      "Epoch [5/8], Step [294/750], Loss: 0.0063\n",
      "Epoch [5/8], Step [295/750], Loss: 0.0794\n",
      "Epoch [5/8], Step [296/750], Loss: 0.0081\n",
      "Epoch [5/8], Step [297/750], Loss: 0.0355\n",
      "Epoch [5/8], Step [298/750], Loss: 0.0021\n",
      "Epoch [5/8], Step [299/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [300/750], Loss: 0.1072\n",
      "Epoch [5/8], Step [301/750], Loss: 0.1103\n",
      "Epoch [5/8], Step [302/750], Loss: 0.0853\n",
      "Epoch [5/8], Step [303/750], Loss: 0.1352\n",
      "Epoch [5/8], Step [304/750], Loss: 0.0111\n",
      "Epoch [5/8], Step [305/750], Loss: 0.0159\n",
      "Epoch [5/8], Step [306/750], Loss: 0.0136\n",
      "Epoch [5/8], Step [307/750], Loss: 0.0277\n",
      "Epoch [5/8], Step [308/750], Loss: 0.0104\n",
      "Epoch [5/8], Step [309/750], Loss: 0.0087\n",
      "Epoch [5/8], Step [310/750], Loss: 0.1095\n",
      "Epoch [5/8], Step [311/750], Loss: 0.0597\n",
      "Epoch [5/8], Step [312/750], Loss: 0.0145\n",
      "Epoch [5/8], Step [313/750], Loss: 0.0173\n",
      "Epoch [5/8], Step [314/750], Loss: 0.0563\n",
      "Epoch [5/8], Step [315/750], Loss: 0.0151\n",
      "Epoch [5/8], Step [316/750], Loss: 0.0085\n",
      "Epoch [5/8], Step [317/750], Loss: 0.0104\n",
      "Epoch [5/8], Step [318/750], Loss: 0.0114\n",
      "Epoch [5/8], Step [319/750], Loss: 0.0252\n",
      "Epoch [5/8], Step [320/750], Loss: 0.0303\n",
      "Epoch [5/8], Step [321/750], Loss: 0.0066\n",
      "Epoch [5/8], Step [322/750], Loss: 0.0294\n",
      "Epoch [5/8], Step [323/750], Loss: 0.0045\n",
      "Epoch [5/8], Step [324/750], Loss: 0.0017\n",
      "Epoch [5/8], Step [325/750], Loss: 0.0027\n",
      "Epoch [5/8], Step [326/750], Loss: 0.0160\n",
      "Epoch [5/8], Step [327/750], Loss: 0.0103\n",
      "Epoch [5/8], Step [328/750], Loss: 0.0968\n",
      "Epoch [5/8], Step [329/750], Loss: 0.0046\n",
      "Epoch [5/8], Step [330/750], Loss: 0.0069\n",
      "Epoch [5/8], Step [331/750], Loss: 0.0381\n",
      "Epoch [5/8], Step [332/750], Loss: 0.0831\n",
      "Epoch [5/8], Step [333/750], Loss: 0.0093\n",
      "Epoch [5/8], Step [334/750], Loss: 0.0183\n",
      "Epoch [5/8], Step [335/750], Loss: 0.0082\n",
      "Epoch [5/8], Step [336/750], Loss: 0.0015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Step [337/750], Loss: 0.0024\n",
      "Epoch [5/8], Step [338/750], Loss: 0.0402\n",
      "Epoch [5/8], Step [339/750], Loss: 0.0033\n",
      "Epoch [5/8], Step [340/750], Loss: 0.0655\n",
      "Epoch [5/8], Step [341/750], Loss: 0.0132\n",
      "Epoch [5/8], Step [342/750], Loss: 0.0854\n",
      "Epoch [5/8], Step [343/750], Loss: 0.0073\n",
      "Epoch [5/8], Step [344/750], Loss: 0.0079\n",
      "Epoch [5/8], Step [345/750], Loss: 0.0523\n",
      "Epoch [5/8], Step [346/750], Loss: 0.0022\n",
      "Epoch [5/8], Step [347/750], Loss: 0.0030\n",
      "Epoch [5/8], Step [348/750], Loss: 0.0602\n",
      "Epoch [5/8], Step [349/750], Loss: 0.0027\n",
      "Epoch [5/8], Step [350/750], Loss: 0.0101\n",
      "Epoch [5/8], Step [351/750], Loss: 0.0167\n",
      "Epoch [5/8], Step [352/750], Loss: 0.0282\n",
      "Epoch [5/8], Step [353/750], Loss: 0.0105\n",
      "Epoch [5/8], Step [354/750], Loss: 0.0312\n",
      "Epoch [5/8], Step [355/750], Loss: 0.0299\n",
      "Epoch [5/8], Step [356/750], Loss: 0.0124\n",
      "Epoch [5/8], Step [357/750], Loss: 0.2331\n",
      "Epoch [5/8], Step [358/750], Loss: 0.0146\n",
      "Epoch [5/8], Step [359/750], Loss: 0.0200\n",
      "Epoch [5/8], Step [360/750], Loss: 0.0112\n",
      "Epoch [5/8], Step [361/750], Loss: 0.0078\n",
      "Epoch [5/8], Step [362/750], Loss: 0.0053\n",
      "Epoch [5/8], Step [363/750], Loss: 0.0129\n",
      "Epoch [5/8], Step [364/750], Loss: 0.0203\n",
      "Epoch [5/8], Step [365/750], Loss: 0.0683\n",
      "Epoch [5/8], Step [366/750], Loss: 0.0177\n",
      "Epoch [5/8], Step [367/750], Loss: 0.0415\n",
      "Epoch [5/8], Step [368/750], Loss: 0.0082\n",
      "Epoch [5/8], Step [369/750], Loss: 0.0282\n",
      "Epoch [5/8], Step [370/750], Loss: 0.0091\n",
      "Epoch [5/8], Step [371/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [372/750], Loss: 0.0020\n",
      "Epoch [5/8], Step [373/750], Loss: 0.0115\n",
      "Epoch [5/8], Step [374/750], Loss: 0.0194\n",
      "Epoch [5/8], Step [375/750], Loss: 0.0454\n",
      "Epoch [5/8], Step [376/750], Loss: 0.0727\n",
      "Epoch [5/8], Step [377/750], Loss: 0.0025\n",
      "Epoch [5/8], Step [378/750], Loss: 0.0072\n",
      "Epoch [5/8], Step [379/750], Loss: 0.0395\n",
      "Epoch [5/8], Step [380/750], Loss: 0.0379\n",
      "Epoch [5/8], Step [381/750], Loss: 0.0351\n",
      "Epoch [5/8], Step [382/750], Loss: 0.0656\n",
      "Epoch [5/8], Step [383/750], Loss: 0.1018\n",
      "Epoch [5/8], Step [384/750], Loss: 0.0093\n",
      "Epoch [5/8], Step [385/750], Loss: 0.0290\n",
      "Epoch [5/8], Step [386/750], Loss: 0.0298\n",
      "Epoch [5/8], Step [387/750], Loss: 0.1374\n",
      "Epoch [5/8], Step [388/750], Loss: 0.0126\n",
      "Epoch [5/8], Step [389/750], Loss: 0.0176\n",
      "Epoch [5/8], Step [390/750], Loss: 0.0505\n",
      "Epoch [5/8], Step [391/750], Loss: 0.0388\n",
      "Epoch [5/8], Step [392/750], Loss: 0.0199\n",
      "Epoch [5/8], Step [393/750], Loss: 0.0134\n",
      "Epoch [5/8], Step [394/750], Loss: 0.0122\n",
      "Epoch [5/8], Step [395/750], Loss: 0.0094\n",
      "Epoch [5/8], Step [396/750], Loss: 0.0355\n",
      "Epoch [5/8], Step [397/750], Loss: 0.0026\n",
      "Epoch [5/8], Step [398/750], Loss: 0.0094\n",
      "Epoch [5/8], Step [399/750], Loss: 0.0528\n",
      "Epoch [5/8], Step [400/750], Loss: 0.0468\n",
      "Epoch [5/8], Step [401/750], Loss: 0.0046\n",
      "Epoch [5/8], Step [402/750], Loss: 0.0008\n",
      "Epoch [5/8], Step [403/750], Loss: 0.0040\n",
      "Epoch [5/8], Step [404/750], Loss: 0.0356\n",
      "Epoch [5/8], Step [405/750], Loss: 0.0023\n",
      "Epoch [5/8], Step [406/750], Loss: 0.0040\n",
      "Epoch [5/8], Step [407/750], Loss: 0.0176\n",
      "Epoch [5/8], Step [408/750], Loss: 0.0437\n",
      "Epoch [5/8], Step [409/750], Loss: 0.0180\n",
      "Epoch [5/8], Step [410/750], Loss: 0.0094\n",
      "Epoch [5/8], Step [411/750], Loss: 0.0073\n",
      "Epoch [5/8], Step [412/750], Loss: 0.0099\n",
      "Epoch [5/8], Step [413/750], Loss: 0.0026\n",
      "Epoch [5/8], Step [414/750], Loss: 0.0073\n",
      "Epoch [5/8], Step [415/750], Loss: 0.0181\n",
      "Epoch [5/8], Step [416/750], Loss: 0.0179\n",
      "Epoch [5/8], Step [417/750], Loss: 0.0344\n",
      "Epoch [5/8], Step [418/750], Loss: 0.0804\n",
      "Epoch [5/8], Step [419/750], Loss: 0.0346\n",
      "Epoch [5/8], Step [420/750], Loss: 0.0482\n",
      "Epoch [5/8], Step [421/750], Loss: 0.0617\n",
      "Epoch [5/8], Step [422/750], Loss: 0.0372\n",
      "Epoch [5/8], Step [423/750], Loss: 0.0092\n",
      "Epoch [5/8], Step [424/750], Loss: 0.0263\n",
      "Epoch [5/8], Step [425/750], Loss: 0.0834\n",
      "Epoch [5/8], Step [426/750], Loss: 0.0735\n",
      "Epoch [5/8], Step [427/750], Loss: 0.0366\n",
      "Epoch [5/8], Step [428/750], Loss: 0.0039\n",
      "Epoch [5/8], Step [429/750], Loss: 0.0376\n",
      "Epoch [5/8], Step [430/750], Loss: 0.0144\n",
      "Epoch [5/8], Step [431/750], Loss: 0.0104\n",
      "Epoch [5/8], Step [432/750], Loss: 0.0168\n",
      "Epoch [5/8], Step [433/750], Loss: 0.0282\n",
      "Epoch [5/8], Step [434/750], Loss: 0.0928\n",
      "Epoch [5/8], Step [435/750], Loss: 0.0963\n",
      "Epoch [5/8], Step [436/750], Loss: 0.0551\n",
      "Epoch [5/8], Step [437/750], Loss: 0.1510\n",
      "Epoch [5/8], Step [438/750], Loss: 0.0019\n",
      "Epoch [5/8], Step [439/750], Loss: 0.0992\n",
      "Epoch [5/8], Step [440/750], Loss: 0.0158\n",
      "Epoch [5/8], Step [441/750], Loss: 0.0261\n",
      "Epoch [5/8], Step [442/750], Loss: 0.0863\n",
      "Epoch [5/8], Step [443/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [444/750], Loss: 0.0738\n",
      "Epoch [5/8], Step [445/750], Loss: 0.0052\n",
      "Epoch [5/8], Step [446/750], Loss: 0.1574\n",
      "Epoch [5/8], Step [447/750], Loss: 0.0031\n",
      "Epoch [5/8], Step [448/750], Loss: 0.0036\n",
      "Epoch [5/8], Step [449/750], Loss: 0.0061\n",
      "Epoch [5/8], Step [450/750], Loss: 0.0257\n",
      "Epoch [5/8], Step [451/750], Loss: 0.0243\n",
      "Epoch [5/8], Step [452/750], Loss: 0.0473\n",
      "Epoch [5/8], Step [453/750], Loss: 0.0266\n",
      "Epoch [5/8], Step [454/750], Loss: 0.0054\n",
      "Epoch [5/8], Step [455/750], Loss: 0.0033\n",
      "Epoch [5/8], Step [456/750], Loss: 0.0051\n",
      "Epoch [5/8], Step [457/750], Loss: 0.0398\n",
      "Epoch [5/8], Step [458/750], Loss: 0.0041\n",
      "Epoch [5/8], Step [459/750], Loss: 0.0605\n",
      "Epoch [5/8], Step [460/750], Loss: 0.0094\n",
      "Epoch [5/8], Step [461/750], Loss: 0.0550\n",
      "Epoch [5/8], Step [462/750], Loss: 0.0467\n",
      "Epoch [5/8], Step [463/750], Loss: 0.0087\n",
      "Epoch [5/8], Step [464/750], Loss: 0.1076\n",
      "Epoch [5/8], Step [465/750], Loss: 0.0015\n",
      "Epoch [5/8], Step [466/750], Loss: 0.0216\n",
      "Epoch [5/8], Step [467/750], Loss: 0.0598\n",
      "Epoch [5/8], Step [468/750], Loss: 0.1391\n",
      "Epoch [5/8], Step [469/750], Loss: 0.1365\n",
      "Epoch [5/8], Step [470/750], Loss: 0.0338\n",
      "Epoch [5/8], Step [471/750], Loss: 0.0436\n",
      "Epoch [5/8], Step [472/750], Loss: 0.0136\n",
      "Epoch [5/8], Step [473/750], Loss: 0.0048\n",
      "Epoch [5/8], Step [474/750], Loss: 0.0381\n",
      "Epoch [5/8], Step [475/750], Loss: 0.0039\n",
      "Epoch [5/8], Step [476/750], Loss: 0.0441\n",
      "Epoch [5/8], Step [477/750], Loss: 0.0516\n",
      "Epoch [5/8], Step [478/750], Loss: 0.1145\n",
      "Epoch [5/8], Step [479/750], Loss: 0.0086\n",
      "Epoch [5/8], Step [480/750], Loss: 0.0581\n",
      "Epoch [5/8], Step [481/750], Loss: 0.0758\n",
      "Epoch [5/8], Step [482/750], Loss: 0.0240\n",
      "Epoch [5/8], Step [483/750], Loss: 0.0212\n",
      "Epoch [5/8], Step [484/750], Loss: 0.0327\n",
      "Epoch [5/8], Step [485/750], Loss: 0.0430\n",
      "Epoch [5/8], Step [486/750], Loss: 0.0994\n",
      "Epoch [5/8], Step [487/750], Loss: 0.0715\n",
      "Epoch [5/8], Step [488/750], Loss: 0.0045\n",
      "Epoch [5/8], Step [489/750], Loss: 0.0049\n",
      "Epoch [5/8], Step [490/750], Loss: 0.0153\n",
      "Epoch [5/8], Step [491/750], Loss: 0.0239\n",
      "Epoch [5/8], Step [492/750], Loss: 0.0346\n",
      "Epoch [5/8], Step [493/750], Loss: 0.0027\n",
      "Epoch [5/8], Step [494/750], Loss: 0.1033\n",
      "Epoch [5/8], Step [495/750], Loss: 0.0041\n",
      "Epoch [5/8], Step [496/750], Loss: 0.0021\n",
      "Epoch [5/8], Step [497/750], Loss: 0.0141\n",
      "Epoch [5/8], Step [498/750], Loss: 0.0263\n",
      "Epoch [5/8], Step [499/750], Loss: 0.0272\n",
      "Epoch [5/8], Step [500/750], Loss: 0.0121\n",
      "Epoch [5/8], Step [501/750], Loss: 0.1274\n",
      "Epoch [5/8], Step [502/750], Loss: 0.0062\n",
      "Epoch [5/8], Step [503/750], Loss: 0.0201\n",
      "Epoch [5/8], Step [504/750], Loss: 0.0381\n",
      "Epoch [5/8], Step [505/750], Loss: 0.0410\n",
      "Epoch [5/8], Step [506/750], Loss: 0.0206\n",
      "Epoch [5/8], Step [507/750], Loss: 0.0133\n",
      "Epoch [5/8], Step [508/750], Loss: 0.0508\n",
      "Epoch [5/8], Step [509/750], Loss: 0.0017\n",
      "Epoch [5/8], Step [510/750], Loss: 0.1052\n",
      "Epoch [5/8], Step [511/750], Loss: 0.0603\n",
      "Epoch [5/8], Step [512/750], Loss: 0.0014\n",
      "Epoch [5/8], Step [513/750], Loss: 0.0131\n",
      "Epoch [5/8], Step [514/750], Loss: 0.0082\n",
      "Epoch [5/8], Step [515/750], Loss: 0.0494\n",
      "Epoch [5/8], Step [516/750], Loss: 0.0033\n",
      "Epoch [5/8], Step [517/750], Loss: 0.0115\n",
      "Epoch [5/8], Step [518/750], Loss: 0.0063\n",
      "Epoch [5/8], Step [519/750], Loss: 0.0625\n",
      "Epoch [5/8], Step [520/750], Loss: 0.0427\n",
      "Epoch [5/8], Step [521/750], Loss: 0.1098\n",
      "Epoch [5/8], Step [522/750], Loss: 0.0057\n",
      "Epoch [5/8], Step [523/750], Loss: 0.0062\n",
      "Epoch [5/8], Step [524/750], Loss: 0.0056\n",
      "Epoch [5/8], Step [525/750], Loss: 0.0485\n",
      "Epoch [5/8], Step [526/750], Loss: 0.0447\n",
      "Epoch [5/8], Step [527/750], Loss: 0.0189\n",
      "Epoch [5/8], Step [528/750], Loss: 0.0333\n",
      "Epoch [5/8], Step [529/750], Loss: 0.0112\n",
      "Epoch [5/8], Step [530/750], Loss: 0.0047\n",
      "Epoch [5/8], Step [531/750], Loss: 0.0043\n",
      "Epoch [5/8], Step [532/750], Loss: 0.0226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Step [533/750], Loss: 0.0453\n",
      "Epoch [5/8], Step [534/750], Loss: 0.0113\n",
      "Epoch [5/8], Step [535/750], Loss: 0.0343\n",
      "Epoch [5/8], Step [536/750], Loss: 0.0346\n",
      "Epoch [5/8], Step [537/750], Loss: 0.0093\n",
      "Epoch [5/8], Step [538/750], Loss: 0.0082\n",
      "Epoch [5/8], Step [539/750], Loss: 0.0326\n",
      "Epoch [5/8], Step [540/750], Loss: 0.0114\n",
      "Epoch [5/8], Step [541/750], Loss: 0.0348\n",
      "Epoch [5/8], Step [542/750], Loss: 0.0952\n",
      "Epoch [5/8], Step [543/750], Loss: 0.0030\n",
      "Epoch [5/8], Step [544/750], Loss: 0.0276\n",
      "Epoch [5/8], Step [545/750], Loss: 0.0284\n",
      "Epoch [5/8], Step [546/750], Loss: 0.0824\n",
      "Epoch [5/8], Step [547/750], Loss: 0.0211\n",
      "Epoch [5/8], Step [548/750], Loss: 0.1553\n",
      "Epoch [5/8], Step [549/750], Loss: 0.0051\n",
      "Epoch [5/8], Step [550/750], Loss: 0.0034\n",
      "Epoch [5/8], Step [551/750], Loss: 0.0045\n",
      "Epoch [5/8], Step [552/750], Loss: 0.0047\n",
      "Epoch [5/8], Step [553/750], Loss: 0.0098\n",
      "Epoch [5/8], Step [554/750], Loss: 0.0126\n",
      "Epoch [5/8], Step [555/750], Loss: 0.0767\n",
      "Epoch [5/8], Step [556/750], Loss: 0.0657\n",
      "Epoch [5/8], Step [557/750], Loss: 0.0967\n",
      "Epoch [5/8], Step [558/750], Loss: 0.0182\n",
      "Epoch [5/8], Step [559/750], Loss: 0.0807\n",
      "Epoch [5/8], Step [560/750], Loss: 0.0041\n",
      "Epoch [5/8], Step [561/750], Loss: 0.0486\n",
      "Epoch [5/8], Step [562/750], Loss: 0.0164\n",
      "Epoch [5/8], Step [563/750], Loss: 0.0105\n",
      "Epoch [5/8], Step [564/750], Loss: 0.0875\n",
      "Epoch [5/8], Step [565/750], Loss: 0.0924\n",
      "Epoch [5/8], Step [566/750], Loss: 0.0043\n",
      "Epoch [5/8], Step [567/750], Loss: 0.0537\n",
      "Epoch [5/8], Step [568/750], Loss: 0.0024\n",
      "Epoch [5/8], Step [569/750], Loss: 0.0069\n",
      "Epoch [5/8], Step [570/750], Loss: 0.0096\n",
      "Epoch [5/8], Step [571/750], Loss: 0.0219\n",
      "Epoch [5/8], Step [572/750], Loss: 0.0036\n",
      "Epoch [5/8], Step [573/750], Loss: 0.0126\n",
      "Epoch [5/8], Step [574/750], Loss: 0.0209\n",
      "Epoch [5/8], Step [575/750], Loss: 0.0253\n",
      "Epoch [5/8], Step [576/750], Loss: 0.0573\n",
      "Epoch [5/8], Step [577/750], Loss: 0.0244\n",
      "Epoch [5/8], Step [578/750], Loss: 0.0382\n",
      "Epoch [5/8], Step [579/750], Loss: 0.0749\n",
      "Epoch [5/8], Step [580/750], Loss: 0.0098\n",
      "Epoch [5/8], Step [581/750], Loss: 0.0037\n",
      "Epoch [5/8], Step [582/750], Loss: 0.0519\n",
      "Epoch [5/8], Step [583/750], Loss: 0.0254\n",
      "Epoch [5/8], Step [584/750], Loss: 0.0078\n",
      "Epoch [5/8], Step [585/750], Loss: 0.0457\n",
      "Epoch [5/8], Step [586/750], Loss: 0.0512\n",
      "Epoch [5/8], Step [587/750], Loss: 0.0179\n",
      "Epoch [5/8], Step [588/750], Loss: 0.0859\n",
      "Epoch [5/8], Step [589/750], Loss: 0.1285\n",
      "Epoch [5/8], Step [590/750], Loss: 0.0721\n",
      "Epoch [5/8], Step [591/750], Loss: 0.0725\n",
      "Epoch [5/8], Step [592/750], Loss: 0.0044\n",
      "Epoch [5/8], Step [593/750], Loss: 0.0065\n",
      "Epoch [5/8], Step [594/750], Loss: 0.0550\n",
      "Epoch [5/8], Step [595/750], Loss: 0.0130\n",
      "Epoch [5/8], Step [596/750], Loss: 0.1118\n",
      "Epoch [5/8], Step [597/750], Loss: 0.0310\n",
      "Epoch [5/8], Step [598/750], Loss: 0.0060\n",
      "Epoch [5/8], Step [599/750], Loss: 0.0503\n",
      "Epoch [5/8], Step [600/750], Loss: 0.0857\n",
      "Epoch [5/8], Step [601/750], Loss: 0.0104\n",
      "Epoch [5/8], Step [602/750], Loss: 0.0101\n",
      "Epoch [5/8], Step [603/750], Loss: 0.0221\n",
      "Epoch [5/8], Step [604/750], Loss: 0.0424\n",
      "Epoch [5/8], Step [605/750], Loss: 0.0121\n",
      "Epoch [5/8], Step [606/750], Loss: 0.1077\n",
      "Epoch [5/8], Step [607/750], Loss: 0.0394\n",
      "Epoch [5/8], Step [608/750], Loss: 0.0035\n",
      "Epoch [5/8], Step [609/750], Loss: 0.0086\n",
      "Epoch [5/8], Step [610/750], Loss: 0.0186\n",
      "Epoch [5/8], Step [611/750], Loss: 0.0525\n",
      "Epoch [5/8], Step [612/750], Loss: 0.0142\n",
      "Epoch [5/8], Step [613/750], Loss: 0.0101\n",
      "Epoch [5/8], Step [614/750], Loss: 0.0154\n",
      "Epoch [5/8], Step [615/750], Loss: 0.1086\n",
      "Epoch [5/8], Step [616/750], Loss: 0.0110\n",
      "Epoch [5/8], Step [617/750], Loss: 0.0321\n",
      "Epoch [5/8], Step [618/750], Loss: 0.0046\n",
      "Epoch [5/8], Step [619/750], Loss: 0.0296\n",
      "Epoch [5/8], Step [620/750], Loss: 0.0017\n",
      "Epoch [5/8], Step [621/750], Loss: 0.0012\n",
      "Epoch [5/8], Step [622/750], Loss: 0.0068\n",
      "Epoch [5/8], Step [623/750], Loss: 0.0200\n",
      "Epoch [5/8], Step [624/750], Loss: 0.0628\n",
      "Epoch [5/8], Step [625/750], Loss: 0.0011\n",
      "Epoch [5/8], Step [626/750], Loss: 0.0398\n",
      "Epoch [5/8], Step [627/750], Loss: 0.0111\n",
      "Epoch [5/8], Step [628/750], Loss: 0.0101\n",
      "Epoch [5/8], Step [629/750], Loss: 0.0221\n",
      "Epoch [5/8], Step [630/750], Loss: 0.0094\n",
      "Epoch [5/8], Step [631/750], Loss: 0.0567\n",
      "Epoch [5/8], Step [632/750], Loss: 0.0355\n",
      "Epoch [5/8], Step [633/750], Loss: 0.0423\n",
      "Epoch [5/8], Step [634/750], Loss: 0.0060\n",
      "Epoch [5/8], Step [635/750], Loss: 0.0108\n",
      "Epoch [5/8], Step [636/750], Loss: 0.0459\n",
      "Epoch [5/8], Step [637/750], Loss: 0.0083\n",
      "Epoch [5/8], Step [638/750], Loss: 0.0086\n",
      "Epoch [5/8], Step [639/750], Loss: 0.0063\n",
      "Epoch [5/8], Step [640/750], Loss: 0.0051\n",
      "Epoch [5/8], Step [641/750], Loss: 0.0329\n",
      "Epoch [5/8], Step [642/750], Loss: 0.0136\n",
      "Epoch [5/8], Step [643/750], Loss: 0.0430\n",
      "Epoch [5/8], Step [644/750], Loss: 0.0519\n",
      "Epoch [5/8], Step [645/750], Loss: 0.0072\n",
      "Epoch [5/8], Step [646/750], Loss: 0.0314\n",
      "Epoch [5/8], Step [647/750], Loss: 0.0269\n",
      "Epoch [5/8], Step [648/750], Loss: 0.0848\n",
      "Epoch [5/8], Step [649/750], Loss: 0.0057\n",
      "Epoch [5/8], Step [650/750], Loss: 0.0670\n",
      "Epoch [5/8], Step [651/750], Loss: 0.1028\n",
      "Epoch [5/8], Step [652/750], Loss: 0.1439\n",
      "Epoch [5/8], Step [653/750], Loss: 0.0092\n",
      "Epoch [5/8], Step [654/750], Loss: 0.0231\n",
      "Epoch [5/8], Step [655/750], Loss: 0.0088\n",
      "Epoch [5/8], Step [656/750], Loss: 0.0309\n",
      "Epoch [5/8], Step [657/750], Loss: 0.0029\n",
      "Epoch [5/8], Step [658/750], Loss: 0.0404\n",
      "Epoch [5/8], Step [659/750], Loss: 0.0769\n",
      "Epoch [5/8], Step [660/750], Loss: 0.0149\n",
      "Epoch [5/8], Step [661/750], Loss: 0.0292\n",
      "Epoch [5/8], Step [662/750], Loss: 0.0139\n",
      "Epoch [5/8], Step [663/750], Loss: 0.0093\n",
      "Epoch [5/8], Step [664/750], Loss: 0.0691\n",
      "Epoch [5/8], Step [665/750], Loss: 0.0017\n",
      "Epoch [5/8], Step [666/750], Loss: 0.0109\n",
      "Epoch [5/8], Step [667/750], Loss: 0.0103\n",
      "Epoch [5/8], Step [668/750], Loss: 0.0100\n",
      "Epoch [5/8], Step [669/750], Loss: 0.0747\n",
      "Epoch [5/8], Step [670/750], Loss: 0.0119\n",
      "Epoch [5/8], Step [671/750], Loss: 0.0813\n",
      "Epoch [5/8], Step [672/750], Loss: 0.0254\n",
      "Epoch [5/8], Step [673/750], Loss: 0.0660\n",
      "Epoch [5/8], Step [674/750], Loss: 0.0208\n",
      "Epoch [5/8], Step [675/750], Loss: 0.0151\n",
      "Epoch [5/8], Step [676/750], Loss: 0.0351\n",
      "Epoch [5/8], Step [677/750], Loss: 0.0053\n",
      "Epoch [5/8], Step [678/750], Loss: 0.0944\n",
      "Epoch [5/8], Step [679/750], Loss: 0.0309\n",
      "Epoch [5/8], Step [680/750], Loss: 0.0287\n",
      "Epoch [5/8], Step [681/750], Loss: 0.0689\n",
      "Epoch [5/8], Step [682/750], Loss: 0.0116\n",
      "Epoch [5/8], Step [683/750], Loss: 0.0090\n",
      "Epoch [5/8], Step [684/750], Loss: 0.0409\n",
      "Epoch [5/8], Step [685/750], Loss: 0.0186\n",
      "Epoch [5/8], Step [686/750], Loss: 0.0254\n",
      "Epoch [5/8], Step [687/750], Loss: 0.0056\n",
      "Epoch [5/8], Step [688/750], Loss: 0.0022\n",
      "Epoch [5/8], Step [689/750], Loss: 0.0486\n",
      "Epoch [5/8], Step [690/750], Loss: 0.0026\n",
      "Epoch [5/8], Step [691/750], Loss: 0.2747\n",
      "Epoch [5/8], Step [692/750], Loss: 0.0274\n",
      "Epoch [5/8], Step [693/750], Loss: 0.0571\n",
      "Epoch [5/8], Step [694/750], Loss: 0.0321\n",
      "Epoch [5/8], Step [695/750], Loss: 0.0086\n",
      "Epoch [5/8], Step [696/750], Loss: 0.0096\n",
      "Epoch [5/8], Step [697/750], Loss: 0.0570\n",
      "Epoch [5/8], Step [698/750], Loss: 0.0773\n",
      "Epoch [5/8], Step [699/750], Loss: 0.0376\n",
      "Epoch [5/8], Step [700/750], Loss: 0.0248\n",
      "Epoch [5/8], Step [701/750], Loss: 0.0189\n",
      "Epoch [5/8], Step [702/750], Loss: 0.0210\n",
      "Epoch [5/8], Step [703/750], Loss: 0.0063\n",
      "Epoch [5/8], Step [704/750], Loss: 0.0776\n",
      "Epoch [5/8], Step [705/750], Loss: 0.0446\n",
      "Epoch [5/8], Step [706/750], Loss: 0.0336\n",
      "Epoch [5/8], Step [707/750], Loss: 0.0169\n",
      "Epoch [5/8], Step [708/750], Loss: 0.0277\n",
      "Epoch [5/8], Step [709/750], Loss: 0.0267\n",
      "Epoch [5/8], Step [710/750], Loss: 0.0027\n",
      "Epoch [5/8], Step [711/750], Loss: 0.0104\n",
      "Epoch [5/8], Step [712/750], Loss: 0.0174\n",
      "Epoch [5/8], Step [713/750], Loss: 0.0279\n",
      "Epoch [5/8], Step [714/750], Loss: 0.1229\n",
      "Epoch [5/8], Step [715/750], Loss: 0.0140\n",
      "Epoch [5/8], Step [716/750], Loss: 0.0324\n",
      "Epoch [5/8], Step [717/750], Loss: 0.0574\n",
      "Epoch [5/8], Step [718/750], Loss: 0.0428\n",
      "Epoch [5/8], Step [719/750], Loss: 0.0414\n",
      "Epoch [5/8], Step [720/750], Loss: 0.0423\n",
      "Epoch [5/8], Step [721/750], Loss: 0.0475\n",
      "Epoch [5/8], Step [722/750], Loss: 0.2238\n",
      "Epoch [5/8], Step [723/750], Loss: 0.0250\n",
      "Epoch [5/8], Step [724/750], Loss: 0.0108\n",
      "Epoch [5/8], Step [725/750], Loss: 0.1247\n",
      "Epoch [5/8], Step [726/750], Loss: 0.0492\n",
      "Epoch [5/8], Step [727/750], Loss: 0.0454\n",
      "Epoch [5/8], Step [728/750], Loss: 0.0084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Step [729/750], Loss: 0.0136\n",
      "Epoch [5/8], Step [730/750], Loss: 0.0110\n",
      "Epoch [5/8], Step [731/750], Loss: 0.0766\n",
      "Epoch [5/8], Step [732/750], Loss: 0.0725\n",
      "Epoch [5/8], Step [733/750], Loss: 0.0124\n",
      "Epoch [5/8], Step [734/750], Loss: 0.0175\n",
      "Epoch [5/8], Step [735/750], Loss: 0.0883\n",
      "Epoch [5/8], Step [736/750], Loss: 0.0337\n",
      "Epoch [5/8], Step [737/750], Loss: 0.0382\n",
      "Epoch [5/8], Step [738/750], Loss: 0.0108\n",
      "Epoch [5/8], Step [739/750], Loss: 0.0128\n",
      "Epoch [5/8], Step [740/750], Loss: 0.0115\n",
      "Epoch [5/8], Step [741/750], Loss: 0.0037\n",
      "Epoch [5/8], Step [742/750], Loss: 0.0252\n",
      "Epoch [5/8], Step [743/750], Loss: 0.0251\n",
      "Epoch [5/8], Step [744/750], Loss: 0.0271\n",
      "Epoch [5/8], Step [745/750], Loss: 0.0046\n",
      "Epoch [5/8], Step [746/750], Loss: 0.0096\n",
      "Epoch [5/8], Step [747/750], Loss: 0.0725\n",
      "Epoch [5/8], Step [748/750], Loss: 0.0829\n",
      "Epoch [5/8], Step [749/750], Loss: 0.0734\n",
      "Epoch [5/8], Step [750/750], Loss: 0.0252\n",
      "Epoch [5/8], Tr. loss: 0.3392. Test loss: 0.2205\n",
      "\n",
      "\n",
      "Epoch [6/8], Step [1/750], Loss: 0.0028\n",
      "Epoch [6/8], Step [2/750], Loss: 0.0417\n",
      "Epoch [6/8], Step [3/750], Loss: 0.0049\n",
      "Epoch [6/8], Step [4/750], Loss: 0.0095\n",
      "Epoch [6/8], Step [5/750], Loss: 0.0337\n",
      "Epoch [6/8], Step [6/750], Loss: 0.0935\n",
      "Epoch [6/8], Step [7/750], Loss: 0.0861\n",
      "Epoch [6/8], Step [8/750], Loss: 0.0593\n",
      "Epoch [6/8], Step [9/750], Loss: 0.0081\n",
      "Epoch [6/8], Step [10/750], Loss: 0.0045\n",
      "Epoch [6/8], Step [11/750], Loss: 0.0502\n",
      "Epoch [6/8], Step [12/750], Loss: 0.0178\n",
      "Epoch [6/8], Step [13/750], Loss: 0.0309\n",
      "Epoch [6/8], Step [14/750], Loss: 0.0482\n",
      "Epoch [6/8], Step [15/750], Loss: 0.0042\n",
      "Epoch [6/8], Step [16/750], Loss: 0.0661\n",
      "Epoch [6/8], Step [17/750], Loss: 0.0035\n",
      "Epoch [6/8], Step [18/750], Loss: 0.0016\n",
      "Epoch [6/8], Step [19/750], Loss: 0.0053\n",
      "Epoch [6/8], Step [20/750], Loss: 0.0050\n",
      "Epoch [6/8], Step [21/750], Loss: 0.0863\n",
      "Epoch [6/8], Step [22/750], Loss: 0.0290\n",
      "Epoch [6/8], Step [23/750], Loss: 0.0245\n",
      "Epoch [6/8], Step [24/750], Loss: 0.0019\n",
      "Epoch [6/8], Step [25/750], Loss: 0.0036\n",
      "Epoch [6/8], Step [26/750], Loss: 0.0019\n",
      "Epoch [6/8], Step [27/750], Loss: 0.0169\n",
      "Epoch [6/8], Step [28/750], Loss: 0.0111\n",
      "Epoch [6/8], Step [29/750], Loss: 0.0024\n",
      "Epoch [6/8], Step [30/750], Loss: 0.0122\n",
      "Epoch [6/8], Step [31/750], Loss: 0.0084\n",
      "Epoch [6/8], Step [32/750], Loss: 0.0097\n",
      "Epoch [6/8], Step [33/750], Loss: 0.0167\n",
      "Epoch [6/8], Step [34/750], Loss: 0.0015\n",
      "Epoch [6/8], Step [35/750], Loss: 0.0098\n",
      "Epoch [6/8], Step [36/750], Loss: 0.0238\n",
      "Epoch [6/8], Step [37/750], Loss: 0.0064\n",
      "Epoch [6/8], Step [38/750], Loss: 0.0044\n",
      "Epoch [6/8], Step [39/750], Loss: 0.0097\n",
      "Epoch [6/8], Step [40/750], Loss: 0.0027\n",
      "Epoch [6/8], Step [41/750], Loss: 0.0016\n",
      "Epoch [6/8], Step [42/750], Loss: 0.0456\n",
      "Epoch [6/8], Step [43/750], Loss: 0.0053\n",
      "Epoch [6/8], Step [44/750], Loss: 0.0322\n",
      "Epoch [6/8], Step [45/750], Loss: 0.0060\n",
      "Epoch [6/8], Step [46/750], Loss: 0.0012\n",
      "Epoch [6/8], Step [47/750], Loss: 0.0861\n",
      "Epoch [6/8], Step [48/750], Loss: 0.0411\n",
      "Epoch [6/8], Step [49/750], Loss: 0.0498\n",
      "Epoch [6/8], Step [50/750], Loss: 0.0646\n",
      "Epoch [6/8], Step [51/750], Loss: 0.0308\n",
      "Epoch [6/8], Step [52/750], Loss: 0.0311\n",
      "Epoch [6/8], Step [53/750], Loss: 0.0156\n",
      "Epoch [6/8], Step [54/750], Loss: 0.0751\n",
      "Epoch [6/8], Step [55/750], Loss: 0.0025\n",
      "Epoch [6/8], Step [56/750], Loss: 0.0017\n",
      "Epoch [6/8], Step [57/750], Loss: 0.0018\n",
      "Epoch [6/8], Step [58/750], Loss: 0.0266\n",
      "Epoch [6/8], Step [59/750], Loss: 0.0093\n",
      "Epoch [6/8], Step [60/750], Loss: 0.0669\n",
      "Epoch [6/8], Step [61/750], Loss: 0.0817\n",
      "Epoch [6/8], Step [62/750], Loss: 0.0248\n",
      "Epoch [6/8], Step [63/750], Loss: 0.0339\n",
      "Epoch [6/8], Step [64/750], Loss: 0.0069\n",
      "Epoch [6/8], Step [65/750], Loss: 0.0146\n",
      "Epoch [6/8], Step [66/750], Loss: 0.1017\n",
      "Epoch [6/8], Step [67/750], Loss: 0.0040\n",
      "Epoch [6/8], Step [68/750], Loss: 0.0628\n",
      "Epoch [6/8], Step [69/750], Loss: 0.0362\n",
      "Epoch [6/8], Step [70/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [71/750], Loss: 0.0054\n",
      "Epoch [6/8], Step [72/750], Loss: 0.0117\n",
      "Epoch [6/8], Step [73/750], Loss: 0.0040\n",
      "Epoch [6/8], Step [74/750], Loss: 0.0105\n",
      "Epoch [6/8], Step [75/750], Loss: 0.0179\n",
      "Epoch [6/8], Step [76/750], Loss: 0.0039\n",
      "Epoch [6/8], Step [77/750], Loss: 0.0256\n",
      "Epoch [6/8], Step [78/750], Loss: 0.0157\n",
      "Epoch [6/8], Step [79/750], Loss: 0.0242\n",
      "Epoch [6/8], Step [80/750], Loss: 0.0841\n",
      "Epoch [6/8], Step [81/750], Loss: 0.1266\n",
      "Epoch [6/8], Step [82/750], Loss: 0.0193\n",
      "Epoch [6/8], Step [83/750], Loss: 0.0134\n",
      "Epoch [6/8], Step [84/750], Loss: 0.0182\n",
      "Epoch [6/8], Step [85/750], Loss: 0.1054\n",
      "Epoch [6/8], Step [86/750], Loss: 0.0574\n",
      "Epoch [6/8], Step [87/750], Loss: 0.0449\n",
      "Epoch [6/8], Step [88/750], Loss: 0.0216\n",
      "Epoch [6/8], Step [89/750], Loss: 0.0025\n",
      "Epoch [6/8], Step [90/750], Loss: 0.0206\n",
      "Epoch [6/8], Step [91/750], Loss: 0.0010\n",
      "Epoch [6/8], Step [92/750], Loss: 0.0163\n",
      "Epoch [6/8], Step [93/750], Loss: 0.0066\n",
      "Epoch [6/8], Step [94/750], Loss: 0.0197\n",
      "Epoch [6/8], Step [95/750], Loss: 0.0900\n",
      "Epoch [6/8], Step [96/750], Loss: 0.0638\n",
      "Epoch [6/8], Step [97/750], Loss: 0.0546\n",
      "Epoch [6/8], Step [98/750], Loss: 0.1074\n",
      "Epoch [6/8], Step [99/750], Loss: 0.0044\n",
      "Epoch [6/8], Step [100/750], Loss: 0.0115\n",
      "Epoch [6/8], Step [101/750], Loss: 0.0914\n",
      "Epoch [6/8], Step [102/750], Loss: 0.0956\n",
      "Epoch [6/8], Step [103/750], Loss: 0.0991\n",
      "Epoch [6/8], Step [104/750], Loss: 0.0541\n",
      "Epoch [6/8], Step [105/750], Loss: 0.0118\n",
      "Epoch [6/8], Step [106/750], Loss: 0.1028\n",
      "Epoch [6/8], Step [107/750], Loss: 0.0522\n",
      "Epoch [6/8], Step [108/750], Loss: 0.0660\n",
      "Epoch [6/8], Step [109/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [110/750], Loss: 0.0088\n",
      "Epoch [6/8], Step [111/750], Loss: 0.0115\n",
      "Epoch [6/8], Step [112/750], Loss: 0.0189\n",
      "Epoch [6/8], Step [113/750], Loss: 0.0612\n",
      "Epoch [6/8], Step [114/750], Loss: 0.0209\n",
      "Epoch [6/8], Step [115/750], Loss: 0.0453\n",
      "Epoch [6/8], Step [116/750], Loss: 0.0022\n",
      "Epoch [6/8], Step [117/750], Loss: 0.0149\n",
      "Epoch [6/8], Step [118/750], Loss: 0.1130\n",
      "Epoch [6/8], Step [119/750], Loss: 0.0062\n",
      "Epoch [6/8], Step [120/750], Loss: 0.0138\n",
      "Epoch [6/8], Step [121/750], Loss: 0.0248\n",
      "Epoch [6/8], Step [122/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [123/750], Loss: 0.0126\n",
      "Epoch [6/8], Step [124/750], Loss: 0.0567\n",
      "Epoch [6/8], Step [125/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [126/750], Loss: 0.0537\n",
      "Epoch [6/8], Step [127/750], Loss: 0.0060\n",
      "Epoch [6/8], Step [128/750], Loss: 0.0603\n",
      "Epoch [6/8], Step [129/750], Loss: 0.0105\n",
      "Epoch [6/8], Step [130/750], Loss: 0.0673\n",
      "Epoch [6/8], Step [131/750], Loss: 0.0077\n",
      "Epoch [6/8], Step [132/750], Loss: 0.0136\n",
      "Epoch [6/8], Step [133/750], Loss: 0.0065\n",
      "Epoch [6/8], Step [134/750], Loss: 0.0018\n",
      "Epoch [6/8], Step [135/750], Loss: 0.0048\n",
      "Epoch [6/8], Step [136/750], Loss: 0.0019\n",
      "Epoch [6/8], Step [137/750], Loss: 0.0026\n",
      "Epoch [6/8], Step [138/750], Loss: 0.0861\n",
      "Epoch [6/8], Step [139/750], Loss: 0.0040\n",
      "Epoch [6/8], Step [140/750], Loss: 0.0189\n",
      "Epoch [6/8], Step [141/750], Loss: 0.0214\n",
      "Epoch [6/8], Step [142/750], Loss: 0.0337\n",
      "Epoch [6/8], Step [143/750], Loss: 0.0627\n",
      "Epoch [6/8], Step [144/750], Loss: 0.0036\n",
      "Epoch [6/8], Step [145/750], Loss: 0.0950\n",
      "Epoch [6/8], Step [146/750], Loss: 0.0168\n",
      "Epoch [6/8], Step [147/750], Loss: 0.0193\n",
      "Epoch [6/8], Step [148/750], Loss: 0.0631\n",
      "Epoch [6/8], Step [149/750], Loss: 0.0020\n",
      "Epoch [6/8], Step [150/750], Loss: 0.0073\n",
      "Epoch [6/8], Step [151/750], Loss: 0.0143\n",
      "Epoch [6/8], Step [152/750], Loss: 0.1627\n",
      "Epoch [6/8], Step [153/750], Loss: 0.0325\n",
      "Epoch [6/8], Step [154/750], Loss: 0.0066\n",
      "Epoch [6/8], Step [155/750], Loss: 0.0045\n",
      "Epoch [6/8], Step [156/750], Loss: 0.0838\n",
      "Epoch [6/8], Step [157/750], Loss: 0.0067\n",
      "Epoch [6/8], Step [158/750], Loss: 0.0043\n",
      "Epoch [6/8], Step [159/750], Loss: 0.0161\n",
      "Epoch [6/8], Step [160/750], Loss: 0.0228\n",
      "Epoch [6/8], Step [161/750], Loss: 0.0023\n",
      "Epoch [6/8], Step [162/750], Loss: 0.0111\n",
      "Epoch [6/8], Step [163/750], Loss: 0.0243\n",
      "Epoch [6/8], Step [164/750], Loss: 0.1529\n",
      "Epoch [6/8], Step [165/750], Loss: 0.0040\n",
      "Epoch [6/8], Step [166/750], Loss: 0.0274\n",
      "Epoch [6/8], Step [167/750], Loss: 0.0013\n",
      "Epoch [6/8], Step [168/750], Loss: 0.0324\n",
      "Epoch [6/8], Step [169/750], Loss: 0.0033\n",
      "Epoch [6/8], Step [170/750], Loss: 0.0095\n",
      "Epoch [6/8], Step [171/750], Loss: 0.0079\n",
      "Epoch [6/8], Step [172/750], Loss: 0.0451\n",
      "Epoch [6/8], Step [173/750], Loss: 0.0259\n",
      "Epoch [6/8], Step [174/750], Loss: 0.1715\n",
      "Epoch [6/8], Step [175/750], Loss: 0.0112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8], Step [176/750], Loss: 0.1190\n",
      "Epoch [6/8], Step [177/750], Loss: 0.0291\n",
      "Epoch [6/8], Step [178/750], Loss: 0.0299\n",
      "Epoch [6/8], Step [179/750], Loss: 0.0023\n",
      "Epoch [6/8], Step [180/750], Loss: 0.0044\n",
      "Epoch [6/8], Step [181/750], Loss: 0.0218\n",
      "Epoch [6/8], Step [182/750], Loss: 0.0261\n",
      "Epoch [6/8], Step [183/750], Loss: 0.0164\n",
      "Epoch [6/8], Step [184/750], Loss: 0.0228\n",
      "Epoch [6/8], Step [185/750], Loss: 0.0926\n",
      "Epoch [6/8], Step [186/750], Loss: 0.0624\n",
      "Epoch [6/8], Step [187/750], Loss: 0.0152\n",
      "Epoch [6/8], Step [188/750], Loss: 0.0187\n",
      "Epoch [6/8], Step [189/750], Loss: 0.0041\n",
      "Epoch [6/8], Step [190/750], Loss: 0.0057\n",
      "Epoch [6/8], Step [191/750], Loss: 0.0117\n",
      "Epoch [6/8], Step [192/750], Loss: 0.0175\n",
      "Epoch [6/8], Step [193/750], Loss: 0.0203\n",
      "Epoch [6/8], Step [194/750], Loss: 0.0040\n",
      "Epoch [6/8], Step [195/750], Loss: 0.0089\n",
      "Epoch [6/8], Step [196/750], Loss: 0.0025\n",
      "Epoch [6/8], Step [197/750], Loss: 0.0351\n",
      "Epoch [6/8], Step [198/750], Loss: 0.0045\n",
      "Epoch [6/8], Step [199/750], Loss: 0.0220\n",
      "Epoch [6/8], Step [200/750], Loss: 0.0089\n",
      "Epoch [6/8], Step [201/750], Loss: 0.0329\n",
      "Epoch [6/8], Step [202/750], Loss: 0.0131\n",
      "Epoch [6/8], Step [203/750], Loss: 0.0048\n",
      "Epoch [6/8], Step [204/750], Loss: 0.0043\n",
      "Epoch [6/8], Step [205/750], Loss: 0.0062\n",
      "Epoch [6/8], Step [206/750], Loss: 0.1002\n",
      "Epoch [6/8], Step [207/750], Loss: 0.0041\n",
      "Epoch [6/8], Step [208/750], Loss: 0.0277\n",
      "Epoch [6/8], Step [209/750], Loss: 0.0027\n",
      "Epoch [6/8], Step [210/750], Loss: 0.0169\n",
      "Epoch [6/8], Step [211/750], Loss: 0.0028\n",
      "Epoch [6/8], Step [212/750], Loss: 0.0063\n",
      "Epoch [6/8], Step [213/750], Loss: 0.0023\n",
      "Epoch [6/8], Step [214/750], Loss: 0.0068\n",
      "Epoch [6/8], Step [215/750], Loss: 0.0040\n",
      "Epoch [6/8], Step [216/750], Loss: 0.1020\n",
      "Epoch [6/8], Step [217/750], Loss: 0.1138\n",
      "Epoch [6/8], Step [218/750], Loss: 0.0226\n",
      "Epoch [6/8], Step [219/750], Loss: 0.0096\n",
      "Epoch [6/8], Step [220/750], Loss: 0.0258\n",
      "Epoch [6/8], Step [221/750], Loss: 0.0023\n",
      "Epoch [6/8], Step [222/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [223/750], Loss: 0.0876\n",
      "Epoch [6/8], Step [224/750], Loss: 0.0301\n",
      "Epoch [6/8], Step [225/750], Loss: 0.0076\n",
      "Epoch [6/8], Step [226/750], Loss: 0.0610\n",
      "Epoch [6/8], Step [227/750], Loss: 0.0247\n",
      "Epoch [6/8], Step [228/750], Loss: 0.0066\n",
      "Epoch [6/8], Step [229/750], Loss: 0.0151\n",
      "Epoch [6/8], Step [230/750], Loss: 0.1236\n",
      "Epoch [6/8], Step [231/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [232/750], Loss: 0.0049\n",
      "Epoch [6/8], Step [233/750], Loss: 0.0387\n",
      "Epoch [6/8], Step [234/750], Loss: 0.0199\n",
      "Epoch [6/8], Step [235/750], Loss: 0.0025\n",
      "Epoch [6/8], Step [236/750], Loss: 0.0019\n",
      "Epoch [6/8], Step [237/750], Loss: 0.0086\n",
      "Epoch [6/8], Step [238/750], Loss: 0.0024\n",
      "Epoch [6/8], Step [239/750], Loss: 0.0412\n",
      "Epoch [6/8], Step [240/750], Loss: 0.0292\n",
      "Epoch [6/8], Step [241/750], Loss: 0.0278\n",
      "Epoch [6/8], Step [242/750], Loss: 0.0127\n",
      "Epoch [6/8], Step [243/750], Loss: 0.0020\n",
      "Epoch [6/8], Step [244/750], Loss: 0.0019\n",
      "Epoch [6/8], Step [245/750], Loss: 0.0284\n",
      "Epoch [6/8], Step [246/750], Loss: 0.0117\n",
      "Epoch [6/8], Step [247/750], Loss: 0.0278\n",
      "Epoch [6/8], Step [248/750], Loss: 0.0021\n",
      "Epoch [6/8], Step [249/750], Loss: 0.0704\n",
      "Epoch [6/8], Step [250/750], Loss: 0.0006\n",
      "Epoch [6/8], Step [251/750], Loss: 0.0010\n",
      "Epoch [6/8], Step [252/750], Loss: 0.0267\n",
      "Epoch [6/8], Step [253/750], Loss: 0.0007\n",
      "Epoch [6/8], Step [254/750], Loss: 0.0045\n",
      "Epoch [6/8], Step [255/750], Loss: 0.0208\n",
      "Epoch [6/8], Step [256/750], Loss: 0.0281\n",
      "Epoch [6/8], Step [257/750], Loss: 0.0010\n",
      "Epoch [6/8], Step [258/750], Loss: 0.0036\n",
      "Epoch [6/8], Step [259/750], Loss: 0.0376\n",
      "Epoch [6/8], Step [260/750], Loss: 0.0268\n",
      "Epoch [6/8], Step [261/750], Loss: 0.0980\n",
      "Epoch [6/8], Step [262/750], Loss: 0.0092\n",
      "Epoch [6/8], Step [263/750], Loss: 0.0055\n",
      "Epoch [6/8], Step [264/750], Loss: 0.0076\n",
      "Epoch [6/8], Step [265/750], Loss: 0.0064\n",
      "Epoch [6/8], Step [266/750], Loss: 0.0419\n",
      "Epoch [6/8], Step [267/750], Loss: 0.0138\n",
      "Epoch [6/8], Step [268/750], Loss: 0.0124\n",
      "Epoch [6/8], Step [269/750], Loss: 0.0040\n",
      "Epoch [6/8], Step [270/750], Loss: 0.0471\n",
      "Epoch [6/8], Step [271/750], Loss: 0.0283\n",
      "Epoch [6/8], Step [272/750], Loss: 0.0163\n",
      "Epoch [6/8], Step [273/750], Loss: 0.0099\n",
      "Epoch [6/8], Step [274/750], Loss: 0.1133\n",
      "Epoch [6/8], Step [275/750], Loss: 0.0021\n",
      "Epoch [6/8], Step [276/750], Loss: 0.0043\n",
      "Epoch [6/8], Step [277/750], Loss: 0.0424\n",
      "Epoch [6/8], Step [278/750], Loss: 0.0017\n",
      "Epoch [6/8], Step [279/750], Loss: 0.0206\n",
      "Epoch [6/8], Step [280/750], Loss: 0.0141\n",
      "Epoch [6/8], Step [281/750], Loss: 0.0972\n",
      "Epoch [6/8], Step [282/750], Loss: 0.0172\n",
      "Epoch [6/8], Step [283/750], Loss: 0.0050\n",
      "Epoch [6/8], Step [284/750], Loss: 0.0326\n",
      "Epoch [6/8], Step [285/750], Loss: 0.0358\n",
      "Epoch [6/8], Step [286/750], Loss: 0.0119\n",
      "Epoch [6/8], Step [287/750], Loss: 0.0059\n",
      "Epoch [6/8], Step [288/750], Loss: 0.0159\n",
      "Epoch [6/8], Step [289/750], Loss: 0.0386\n",
      "Epoch [6/8], Step [290/750], Loss: 0.1185\n",
      "Epoch [6/8], Step [291/750], Loss: 0.0067\n",
      "Epoch [6/8], Step [292/750], Loss: 0.0337\n",
      "Epoch [6/8], Step [293/750], Loss: 0.0067\n",
      "Epoch [6/8], Step [294/750], Loss: 0.0261\n",
      "Epoch [6/8], Step [295/750], Loss: 0.0594\n",
      "Epoch [6/8], Step [296/750], Loss: 0.0315\n",
      "Epoch [6/8], Step [297/750], Loss: 0.0042\n",
      "Epoch [6/8], Step [298/750], Loss: 0.0061\n",
      "Epoch [6/8], Step [299/750], Loss: 0.0317\n",
      "Epoch [6/8], Step [300/750], Loss: 0.0299\n",
      "Epoch [6/8], Step [301/750], Loss: 0.0049\n",
      "Epoch [6/8], Step [302/750], Loss: 0.0456\n",
      "Epoch [6/8], Step [303/750], Loss: 0.0063\n",
      "Epoch [6/8], Step [304/750], Loss: 0.0948\n",
      "Epoch [6/8], Step [305/750], Loss: 0.0012\n",
      "Epoch [6/8], Step [306/750], Loss: 0.0058\n",
      "Epoch [6/8], Step [307/750], Loss: 0.1314\n",
      "Epoch [6/8], Step [308/750], Loss: 0.0477\n",
      "Epoch [6/8], Step [309/750], Loss: 0.0039\n",
      "Epoch [6/8], Step [310/750], Loss: 0.0370\n",
      "Epoch [6/8], Step [311/750], Loss: 0.0924\n",
      "Epoch [6/8], Step [312/750], Loss: 0.0150\n",
      "Epoch [6/8], Step [313/750], Loss: 0.0434\n",
      "Epoch [6/8], Step [314/750], Loss: 0.0068\n",
      "Epoch [6/8], Step [315/750], Loss: 0.0020\n",
      "Epoch [6/8], Step [316/750], Loss: 0.0369\n",
      "Epoch [6/8], Step [317/750], Loss: 0.0038\n",
      "Epoch [6/8], Step [318/750], Loss: 0.1141\n",
      "Epoch [6/8], Step [319/750], Loss: 0.0959\n",
      "Epoch [6/8], Step [320/750], Loss: 0.0774\n",
      "Epoch [6/8], Step [321/750], Loss: 0.0837\n",
      "Epoch [6/8], Step [322/750], Loss: 0.0592\n",
      "Epoch [6/8], Step [323/750], Loss: 0.0063\n",
      "Epoch [6/8], Step [324/750], Loss: 0.0077\n",
      "Epoch [6/8], Step [325/750], Loss: 0.0163\n",
      "Epoch [6/8], Step [326/750], Loss: 0.0339\n",
      "Epoch [6/8], Step [327/750], Loss: 0.0510\n",
      "Epoch [6/8], Step [328/750], Loss: 0.0036\n",
      "Epoch [6/8], Step [329/750], Loss: 0.0048\n",
      "Epoch [6/8], Step [330/750], Loss: 0.0690\n",
      "Epoch [6/8], Step [331/750], Loss: 0.0072\n",
      "Epoch [6/8], Step [332/750], Loss: 0.0035\n",
      "Epoch [6/8], Step [333/750], Loss: 0.0081\n",
      "Epoch [6/8], Step [334/750], Loss: 0.0059\n",
      "Epoch [6/8], Step [335/750], Loss: 0.0959\n",
      "Epoch [6/8], Step [336/750], Loss: 0.0024\n",
      "Epoch [6/8], Step [337/750], Loss: 0.0146\n",
      "Epoch [6/8], Step [338/750], Loss: 0.0650\n",
      "Epoch [6/8], Step [339/750], Loss: 0.0092\n",
      "Epoch [6/8], Step [340/750], Loss: 0.0031\n",
      "Epoch [6/8], Step [341/750], Loss: 0.0472\n",
      "Epoch [6/8], Step [342/750], Loss: 0.0048\n",
      "Epoch [6/8], Step [343/750], Loss: 0.0079\n",
      "Epoch [6/8], Step [344/750], Loss: 0.0193\n",
      "Epoch [6/8], Step [345/750], Loss: 0.0184\n",
      "Epoch [6/8], Step [346/750], Loss: 0.0279\n",
      "Epoch [6/8], Step [347/750], Loss: 0.0072\n",
      "Epoch [6/8], Step [348/750], Loss: 0.0876\n",
      "Epoch [6/8], Step [349/750], Loss: 0.0230\n",
      "Epoch [6/8], Step [350/750], Loss: 0.0024\n",
      "Epoch [6/8], Step [351/750], Loss: 0.0222\n",
      "Epoch [6/8], Step [352/750], Loss: 0.0318\n",
      "Epoch [6/8], Step [353/750], Loss: 0.0204\n",
      "Epoch [6/8], Step [354/750], Loss: 0.0078\n",
      "Epoch [6/8], Step [355/750], Loss: 0.0758\n",
      "Epoch [6/8], Step [356/750], Loss: 0.0495\n",
      "Epoch [6/8], Step [357/750], Loss: 0.0024\n",
      "Epoch [6/8], Step [358/750], Loss: 0.0045\n",
      "Epoch [6/8], Step [359/750], Loss: 0.0337\n",
      "Epoch [6/8], Step [360/750], Loss: 0.0023\n",
      "Epoch [6/8], Step [361/750], Loss: 0.0476\n",
      "Epoch [6/8], Step [362/750], Loss: 0.0460\n",
      "Epoch [6/8], Step [363/750], Loss: 0.0239\n",
      "Epoch [6/8], Step [364/750], Loss: 0.0090\n",
      "Epoch [6/8], Step [365/750], Loss: 0.0196\n",
      "Epoch [6/8], Step [366/750], Loss: 0.0105\n",
      "Epoch [6/8], Step [367/750], Loss: 0.0033\n",
      "Epoch [6/8], Step [368/750], Loss: 0.0760\n",
      "Epoch [6/8], Step [369/750], Loss: 0.0025\n",
      "Epoch [6/8], Step [370/750], Loss: 0.0155\n",
      "Epoch [6/8], Step [371/750], Loss: 0.0485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8], Step [372/750], Loss: 0.0069\n",
      "Epoch [6/8], Step [373/750], Loss: 0.0055\n",
      "Epoch [6/8], Step [374/750], Loss: 0.0061\n",
      "Epoch [6/8], Step [375/750], Loss: 0.0292\n",
      "Epoch [6/8], Step [376/750], Loss: 0.0218\n",
      "Epoch [6/8], Step [377/750], Loss: 0.0437\n",
      "Epoch [6/8], Step [378/750], Loss: 0.0207\n",
      "Epoch [6/8], Step [379/750], Loss: 0.0098\n",
      "Epoch [6/8], Step [380/750], Loss: 0.0726\n",
      "Epoch [6/8], Step [381/750], Loss: 0.0170\n",
      "Epoch [6/8], Step [382/750], Loss: 0.0022\n",
      "Epoch [6/8], Step [383/750], Loss: 0.0311\n",
      "Epoch [6/8], Step [384/750], Loss: 0.0217\n",
      "Epoch [6/8], Step [385/750], Loss: 0.0410\n",
      "Epoch [6/8], Step [386/750], Loss: 0.0078\n",
      "Epoch [6/8], Step [387/750], Loss: 0.0865\n",
      "Epoch [6/8], Step [388/750], Loss: 0.0564\n",
      "Epoch [6/8], Step [389/750], Loss: 0.0516\n",
      "Epoch [6/8], Step [390/750], Loss: 0.0120\n",
      "Epoch [6/8], Step [391/750], Loss: 0.0241\n",
      "Epoch [6/8], Step [392/750], Loss: 0.0116\n",
      "Epoch [6/8], Step [393/750], Loss: 0.0060\n",
      "Epoch [6/8], Step [394/750], Loss: 0.0061\n",
      "Epoch [6/8], Step [395/750], Loss: 0.0306\n",
      "Epoch [6/8], Step [396/750], Loss: 0.0427\n",
      "Epoch [6/8], Step [397/750], Loss: 0.0071\n",
      "Epoch [6/8], Step [398/750], Loss: 0.0014\n",
      "Epoch [6/8], Step [399/750], Loss: 0.0011\n",
      "Epoch [6/8], Step [400/750], Loss: 0.0213\n",
      "Epoch [6/8], Step [401/750], Loss: 0.0191\n",
      "Epoch [6/8], Step [402/750], Loss: 0.0467\n",
      "Epoch [6/8], Step [403/750], Loss: 0.0013\n",
      "Epoch [6/8], Step [404/750], Loss: 0.0033\n",
      "Epoch [6/8], Step [405/750], Loss: 0.0079\n",
      "Epoch [6/8], Step [406/750], Loss: 0.0288\n",
      "Epoch [6/8], Step [407/750], Loss: 0.0165\n",
      "Epoch [6/8], Step [408/750], Loss: 0.0114\n",
      "Epoch [6/8], Step [409/750], Loss: 0.0150\n",
      "Epoch [6/8], Step [410/750], Loss: 0.0538\n",
      "Epoch [6/8], Step [411/750], Loss: 0.0576\n",
      "Epoch [6/8], Step [412/750], Loss: 0.0043\n",
      "Epoch [6/8], Step [413/750], Loss: 0.0023\n",
      "Epoch [6/8], Step [414/750], Loss: 0.0143\n",
      "Epoch [6/8], Step [415/750], Loss: 0.0249\n",
      "Epoch [6/8], Step [416/750], Loss: 0.0020\n",
      "Epoch [6/8], Step [417/750], Loss: 0.0105\n",
      "Epoch [6/8], Step [418/750], Loss: 0.0108\n",
      "Epoch [6/8], Step [419/750], Loss: 0.0007\n",
      "Epoch [6/8], Step [420/750], Loss: 0.0773\n",
      "Epoch [6/8], Step [421/750], Loss: 0.0173\n",
      "Epoch [6/8], Step [422/750], Loss: 0.0146\n",
      "Epoch [6/8], Step [423/750], Loss: 0.0728\n",
      "Epoch [6/8], Step [424/750], Loss: 0.0803\n",
      "Epoch [6/8], Step [425/750], Loss: 0.0065\n",
      "Epoch [6/8], Step [426/750], Loss: 0.0095\n",
      "Epoch [6/8], Step [427/750], Loss: 0.0497\n",
      "Epoch [6/8], Step [428/750], Loss: 0.0199\n",
      "Epoch [6/8], Step [429/750], Loss: 0.0351\n",
      "Epoch [6/8], Step [430/750], Loss: 0.0339\n",
      "Epoch [6/8], Step [431/750], Loss: 0.0263\n",
      "Epoch [6/8], Step [432/750], Loss: 0.0311\n",
      "Epoch [6/8], Step [433/750], Loss: 0.0197\n",
      "Epoch [6/8], Step [434/750], Loss: 0.0412\n",
      "Epoch [6/8], Step [435/750], Loss: 0.0059\n",
      "Epoch [6/8], Step [436/750], Loss: 0.0026\n",
      "Epoch [6/8], Step [437/750], Loss: 0.0287\n",
      "Epoch [6/8], Step [438/750], Loss: 0.0443\n",
      "Epoch [6/8], Step [439/750], Loss: 0.0381\n",
      "Epoch [6/8], Step [440/750], Loss: 0.0074\n",
      "Epoch [6/8], Step [441/750], Loss: 0.0255\n",
      "Epoch [6/8], Step [442/750], Loss: 0.0527\n",
      "Epoch [6/8], Step [443/750], Loss: 0.0068\n",
      "Epoch [6/8], Step [444/750], Loss: 0.0151\n",
      "Epoch [6/8], Step [445/750], Loss: 0.0045\n",
      "Epoch [6/8], Step [446/750], Loss: 0.0463\n",
      "Epoch [6/8], Step [447/750], Loss: 0.0220\n",
      "Epoch [6/8], Step [448/750], Loss: 0.0734\n",
      "Epoch [6/8], Step [449/750], Loss: 0.0625\n",
      "Epoch [6/8], Step [450/750], Loss: 0.1237\n",
      "Epoch [6/8], Step [451/750], Loss: 0.0134\n",
      "Epoch [6/8], Step [452/750], Loss: 0.0048\n",
      "Epoch [6/8], Step [453/750], Loss: 0.0510\n",
      "Epoch [6/8], Step [454/750], Loss: 0.0076\n",
      "Epoch [6/8], Step [455/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [456/750], Loss: 0.0403\n",
      "Epoch [6/8], Step [457/750], Loss: 0.0220\n",
      "Epoch [6/8], Step [458/750], Loss: 0.0165\n",
      "Epoch [6/8], Step [459/750], Loss: 0.0052\n",
      "Epoch [6/8], Step [460/750], Loss: 0.0049\n",
      "Epoch [6/8], Step [461/750], Loss: 0.0104\n",
      "Epoch [6/8], Step [462/750], Loss: 0.0402\n",
      "Epoch [6/8], Step [463/750], Loss: 0.0365\n",
      "Epoch [6/8], Step [464/750], Loss: 0.0244\n",
      "Epoch [6/8], Step [465/750], Loss: 0.0130\n",
      "Epoch [6/8], Step [466/750], Loss: 0.0445\n",
      "Epoch [6/8], Step [467/750], Loss: 0.0138\n",
      "Epoch [6/8], Step [468/750], Loss: 0.0033\n",
      "Epoch [6/8], Step [469/750], Loss: 0.0309\n",
      "Epoch [6/8], Step [470/750], Loss: 0.0697\n",
      "Epoch [6/8], Step [471/750], Loss: 0.1528\n",
      "Epoch [6/8], Step [472/750], Loss: 0.0811\n",
      "Epoch [6/8], Step [473/750], Loss: 0.0425\n",
      "Epoch [6/8], Step [474/750], Loss: 0.0189\n",
      "Epoch [6/8], Step [475/750], Loss: 0.0062\n",
      "Epoch [6/8], Step [476/750], Loss: 0.0135\n",
      "Epoch [6/8], Step [477/750], Loss: 0.0132\n",
      "Epoch [6/8], Step [478/750], Loss: 0.0090\n",
      "Epoch [6/8], Step [479/750], Loss: 0.0417\n",
      "Epoch [6/8], Step [480/750], Loss: 0.0154\n",
      "Epoch [6/8], Step [481/750], Loss: 0.0629\n",
      "Epoch [6/8], Step [482/750], Loss: 0.0335\n",
      "Epoch [6/8], Step [483/750], Loss: 0.0877\n",
      "Epoch [6/8], Step [484/750], Loss: 0.0717\n",
      "Epoch [6/8], Step [485/750], Loss: 0.0124\n",
      "Epoch [6/8], Step [486/750], Loss: 0.0124\n",
      "Epoch [6/8], Step [487/750], Loss: 0.0098\n",
      "Epoch [6/8], Step [488/750], Loss: 0.0437\n",
      "Epoch [6/8], Step [489/750], Loss: 0.0156\n",
      "Epoch [6/8], Step [490/750], Loss: 0.0641\n",
      "Epoch [6/8], Step [491/750], Loss: 0.0319\n",
      "Epoch [6/8], Step [492/750], Loss: 0.2259\n",
      "Epoch [6/8], Step [493/750], Loss: 0.0508\n",
      "Epoch [6/8], Step [494/750], Loss: 0.0154\n",
      "Epoch [6/8], Step [495/750], Loss: 0.0021\n",
      "Epoch [6/8], Step [496/750], Loss: 0.0272\n",
      "Epoch [6/8], Step [497/750], Loss: 0.0950\n",
      "Epoch [6/8], Step [498/750], Loss: 0.0350\n",
      "Epoch [6/8], Step [499/750], Loss: 0.0893\n",
      "Epoch [6/8], Step [500/750], Loss: 0.0182\n",
      "Epoch [6/8], Step [501/750], Loss: 0.0252\n",
      "Epoch [6/8], Step [502/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [503/750], Loss: 0.0143\n",
      "Epoch [6/8], Step [504/750], Loss: 0.0124\n",
      "Epoch [6/8], Step [505/750], Loss: 0.0086\n",
      "Epoch [6/8], Step [506/750], Loss: 0.0170\n",
      "Epoch [6/8], Step [507/750], Loss: 0.0035\n",
      "Epoch [6/8], Step [508/750], Loss: 0.0327\n",
      "Epoch [6/8], Step [509/750], Loss: 0.0197\n",
      "Epoch [6/8], Step [510/750], Loss: 0.0326\n",
      "Epoch [6/8], Step [511/750], Loss: 0.0028\n",
      "Epoch [6/8], Step [512/750], Loss: 0.0185\n",
      "Epoch [6/8], Step [513/750], Loss: 0.0092\n",
      "Epoch [6/8], Step [514/750], Loss: 0.0329\n",
      "Epoch [6/8], Step [515/750], Loss: 0.0070\n",
      "Epoch [6/8], Step [516/750], Loss: 0.0095\n",
      "Epoch [6/8], Step [517/750], Loss: 0.0574\n",
      "Epoch [6/8], Step [518/750], Loss: 0.0102\n",
      "Epoch [6/8], Step [519/750], Loss: 0.0065\n",
      "Epoch [6/8], Step [520/750], Loss: 0.0431\n",
      "Epoch [6/8], Step [521/750], Loss: 0.0970\n",
      "Epoch [6/8], Step [522/750], Loss: 0.0014\n",
      "Epoch [6/8], Step [523/750], Loss: 0.0584\n",
      "Epoch [6/8], Step [524/750], Loss: 0.0209\n",
      "Epoch [6/8], Step [525/750], Loss: 0.0515\n",
      "Epoch [6/8], Step [526/750], Loss: 0.0128\n",
      "Epoch [6/8], Step [527/750], Loss: 0.0179\n",
      "Epoch [6/8], Step [528/750], Loss: 0.0232\n",
      "Epoch [6/8], Step [529/750], Loss: 0.0127\n",
      "Epoch [6/8], Step [530/750], Loss: 0.0060\n",
      "Epoch [6/8], Step [531/750], Loss: 0.0330\n",
      "Epoch [6/8], Step [532/750], Loss: 0.0754\n",
      "Epoch [6/8], Step [533/750], Loss: 0.0292\n",
      "Epoch [6/8], Step [534/750], Loss: 0.0069\n",
      "Epoch [6/8], Step [535/750], Loss: 0.0289\n",
      "Epoch [6/8], Step [536/750], Loss: 0.0052\n",
      "Epoch [6/8], Step [537/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [538/750], Loss: 0.0081\n",
      "Epoch [6/8], Step [539/750], Loss: 0.0403\n",
      "Epoch [6/8], Step [540/750], Loss: 0.0014\n",
      "Epoch [6/8], Step [541/750], Loss: 0.0172\n",
      "Epoch [6/8], Step [542/750], Loss: 0.2855\n",
      "Epoch [6/8], Step [543/750], Loss: 0.0083\n",
      "Epoch [6/8], Step [544/750], Loss: 0.0575\n",
      "Epoch [6/8], Step [545/750], Loss: 0.0831\n",
      "Epoch [6/8], Step [546/750], Loss: 0.0027\n",
      "Epoch [6/8], Step [547/750], Loss: 0.0638\n",
      "Epoch [6/8], Step [548/750], Loss: 0.0183\n",
      "Epoch [6/8], Step [549/750], Loss: 0.0168\n",
      "Epoch [6/8], Step [550/750], Loss: 0.0068\n",
      "Epoch [6/8], Step [551/750], Loss: 0.2067\n",
      "Epoch [6/8], Step [552/750], Loss: 0.0343\n",
      "Epoch [6/8], Step [553/750], Loss: 0.0277\n",
      "Epoch [6/8], Step [554/750], Loss: 0.0287\n",
      "Epoch [6/8], Step [555/750], Loss: 0.0172\n",
      "Epoch [6/8], Step [556/750], Loss: 0.0041\n",
      "Epoch [6/8], Step [557/750], Loss: 0.0620\n",
      "Epoch [6/8], Step [558/750], Loss: 0.0041\n",
      "Epoch [6/8], Step [559/750], Loss: 0.1420\n",
      "Epoch [6/8], Step [560/750], Loss: 0.0090\n",
      "Epoch [6/8], Step [561/750], Loss: 0.0052\n",
      "Epoch [6/8], Step [562/750], Loss: 0.0625\n",
      "Epoch [6/8], Step [563/750], Loss: 0.0120\n",
      "Epoch [6/8], Step [564/750], Loss: 0.0322\n",
      "Epoch [6/8], Step [565/750], Loss: 0.0178\n",
      "Epoch [6/8], Step [566/750], Loss: 0.0066\n",
      "Epoch [6/8], Step [567/750], Loss: 0.0063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8], Step [568/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [569/750], Loss: 0.0580\n",
      "Epoch [6/8], Step [570/750], Loss: 0.0081\n",
      "Epoch [6/8], Step [571/750], Loss: 0.0410\n",
      "Epoch [6/8], Step [572/750], Loss: 0.0204\n",
      "Epoch [6/8], Step [573/750], Loss: 0.0017\n",
      "Epoch [6/8], Step [574/750], Loss: 0.0025\n",
      "Epoch [6/8], Step [575/750], Loss: 0.0124\n",
      "Epoch [6/8], Step [576/750], Loss: 0.0105\n",
      "Epoch [6/8], Step [577/750], Loss: 0.0094\n",
      "Epoch [6/8], Step [578/750], Loss: 0.0898\n",
      "Epoch [6/8], Step [579/750], Loss: 0.0814\n",
      "Epoch [6/8], Step [580/750], Loss: 0.0527\n",
      "Epoch [6/8], Step [581/750], Loss: 0.0056\n",
      "Epoch [6/8], Step [582/750], Loss: 0.0139\n",
      "Epoch [6/8], Step [583/750], Loss: 0.0047\n",
      "Epoch [6/8], Step [584/750], Loss: 0.0061\n",
      "Epoch [6/8], Step [585/750], Loss: 0.0770\n",
      "Epoch [6/8], Step [586/750], Loss: 0.0806\n",
      "Epoch [6/8], Step [587/750], Loss: 0.0982\n",
      "Epoch [6/8], Step [588/750], Loss: 0.0046\n",
      "Epoch [6/8], Step [589/750], Loss: 0.0058\n",
      "Epoch [6/8], Step [590/750], Loss: 0.0101\n",
      "Epoch [6/8], Step [591/750], Loss: 0.0158\n",
      "Epoch [6/8], Step [592/750], Loss: 0.0148\n",
      "Epoch [6/8], Step [593/750], Loss: 0.1602\n",
      "Epoch [6/8], Step [594/750], Loss: 0.0351\n",
      "Epoch [6/8], Step [595/750], Loss: 0.0060\n",
      "Epoch [6/8], Step [596/750], Loss: 0.0897\n",
      "Epoch [6/8], Step [597/750], Loss: 0.0140\n",
      "Epoch [6/8], Step [598/750], Loss: 0.0129\n",
      "Epoch [6/8], Step [599/750], Loss: 0.0815\n",
      "Epoch [6/8], Step [600/750], Loss: 0.0054\n",
      "Epoch [6/8], Step [601/750], Loss: 0.1347\n",
      "Epoch [6/8], Step [602/750], Loss: 0.0191\n",
      "Epoch [6/8], Step [603/750], Loss: 0.0307\n",
      "Epoch [6/8], Step [604/750], Loss: 0.0704\n",
      "Epoch [6/8], Step [605/750], Loss: 0.0270\n",
      "Epoch [6/8], Step [606/750], Loss: 0.0685\n",
      "Epoch [6/8], Step [607/750], Loss: 0.0381\n",
      "Epoch [6/8], Step [608/750], Loss: 0.0151\n",
      "Epoch [6/8], Step [609/750], Loss: 0.0047\n",
      "Epoch [6/8], Step [610/750], Loss: 0.0092\n",
      "Epoch [6/8], Step [611/750], Loss: 0.0081\n",
      "Epoch [6/8], Step [612/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [613/750], Loss: 0.0633\n",
      "Epoch [6/8], Step [614/750], Loss: 0.0181\n",
      "Epoch [6/8], Step [615/750], Loss: 0.0398\n",
      "Epoch [6/8], Step [616/750], Loss: 0.0134\n",
      "Epoch [6/8], Step [617/750], Loss: 0.0224\n",
      "Epoch [6/8], Step [618/750], Loss: 0.0083\n",
      "Epoch [6/8], Step [619/750], Loss: 0.0037\n",
      "Epoch [6/8], Step [620/750], Loss: 0.0114\n",
      "Epoch [6/8], Step [621/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [622/750], Loss: 0.0495\n",
      "Epoch [6/8], Step [623/750], Loss: 0.0086\n",
      "Epoch [6/8], Step [624/750], Loss: 0.0150\n",
      "Epoch [6/8], Step [625/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [626/750], Loss: 0.0040\n",
      "Epoch [6/8], Step [627/750], Loss: 0.0009\n",
      "Epoch [6/8], Step [628/750], Loss: 0.1115\n",
      "Epoch [6/8], Step [629/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [630/750], Loss: 0.0053\n",
      "Epoch [6/8], Step [631/750], Loss: 0.0088\n",
      "Epoch [6/8], Step [632/750], Loss: 0.0747\n",
      "Epoch [6/8], Step [633/750], Loss: 0.0293\n",
      "Epoch [6/8], Step [634/750], Loss: 0.0048\n",
      "Epoch [6/8], Step [635/750], Loss: 0.0076\n",
      "Epoch [6/8], Step [636/750], Loss: 0.0165\n",
      "Epoch [6/8], Step [637/750], Loss: 0.0085\n",
      "Epoch [6/8], Step [638/750], Loss: 0.0414\n",
      "Epoch [6/8], Step [639/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [640/750], Loss: 0.0866\n",
      "Epoch [6/8], Step [641/750], Loss: 0.0269\n",
      "Epoch [6/8], Step [642/750], Loss: 0.0410\n",
      "Epoch [6/8], Step [643/750], Loss: 0.0121\n",
      "Epoch [6/8], Step [644/750], Loss: 0.0534\n",
      "Epoch [6/8], Step [645/750], Loss: 0.0322\n",
      "Epoch [6/8], Step [646/750], Loss: 0.0051\n",
      "Epoch [6/8], Step [647/750], Loss: 0.0361\n",
      "Epoch [6/8], Step [648/750], Loss: 0.0222\n",
      "Epoch [6/8], Step [649/750], Loss: 0.0043\n",
      "Epoch [6/8], Step [650/750], Loss: 0.0215\n",
      "Epoch [6/8], Step [651/750], Loss: 0.0649\n",
      "Epoch [6/8], Step [652/750], Loss: 0.1336\n",
      "Epoch [6/8], Step [653/750], Loss: 0.0655\n",
      "Epoch [6/8], Step [654/750], Loss: 0.0107\n",
      "Epoch [6/8], Step [655/750], Loss: 0.0820\n",
      "Epoch [6/8], Step [656/750], Loss: 0.0317\n",
      "Epoch [6/8], Step [657/750], Loss: 0.0320\n",
      "Epoch [6/8], Step [658/750], Loss: 0.0455\n",
      "Epoch [6/8], Step [659/750], Loss: 0.0302\n",
      "Epoch [6/8], Step [660/750], Loss: 0.0254\n",
      "Epoch [6/8], Step [661/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [662/750], Loss: 0.0050\n",
      "Epoch [6/8], Step [663/750], Loss: 0.0170\n",
      "Epoch [6/8], Step [664/750], Loss: 0.0385\n",
      "Epoch [6/8], Step [665/750], Loss: 0.0118\n",
      "Epoch [6/8], Step [666/750], Loss: 0.0366\n",
      "Epoch [6/8], Step [667/750], Loss: 0.0256\n",
      "Epoch [6/8], Step [668/750], Loss: 0.0027\n",
      "Epoch [6/8], Step [669/750], Loss: 0.0770\n",
      "Epoch [6/8], Step [670/750], Loss: 0.0481\n",
      "Epoch [6/8], Step [671/750], Loss: 0.0065\n",
      "Epoch [6/8], Step [672/750], Loss: 0.0190\n",
      "Epoch [6/8], Step [673/750], Loss: 0.0431\n",
      "Epoch [6/8], Step [674/750], Loss: 0.0437\n",
      "Epoch [6/8], Step [675/750], Loss: 0.0234\n",
      "Epoch [6/8], Step [676/750], Loss: 0.0116\n",
      "Epoch [6/8], Step [677/750], Loss: 0.0034\n",
      "Epoch [6/8], Step [678/750], Loss: 0.0329\n",
      "Epoch [6/8], Step [679/750], Loss: 0.0355\n",
      "Epoch [6/8], Step [680/750], Loss: 0.0422\n",
      "Epoch [6/8], Step [681/750], Loss: 0.1052\n",
      "Epoch [6/8], Step [682/750], Loss: 0.0020\n",
      "Epoch [6/8], Step [683/750], Loss: 0.0072\n",
      "Epoch [6/8], Step [684/750], Loss: 0.0020\n",
      "Epoch [6/8], Step [685/750], Loss: 0.0013\n",
      "Epoch [6/8], Step [686/750], Loss: 0.0222\n",
      "Epoch [6/8], Step [687/750], Loss: 0.0040\n",
      "Epoch [6/8], Step [688/750], Loss: 0.0139\n",
      "Epoch [6/8], Step [689/750], Loss: 0.0756\n",
      "Epoch [6/8], Step [690/750], Loss: 0.0800\n",
      "Epoch [6/8], Step [691/750], Loss: 0.0124\n",
      "Epoch [6/8], Step [692/750], Loss: 0.1506\n",
      "Epoch [6/8], Step [693/750], Loss: 0.0127\n",
      "Epoch [6/8], Step [694/750], Loss: 0.0237\n",
      "Epoch [6/8], Step [695/750], Loss: 0.0034\n",
      "Epoch [6/8], Step [696/750], Loss: 0.0056\n",
      "Epoch [6/8], Step [697/750], Loss: 0.0105\n",
      "Epoch [6/8], Step [698/750], Loss: 0.0570\n",
      "Epoch [6/8], Step [699/750], Loss: 0.0305\n",
      "Epoch [6/8], Step [700/750], Loss: 0.1535\n",
      "Epoch [6/8], Step [701/750], Loss: 0.0086\n",
      "Epoch [6/8], Step [702/750], Loss: 0.0352\n",
      "Epoch [6/8], Step [703/750], Loss: 0.0343\n",
      "Epoch [6/8], Step [704/750], Loss: 0.0009\n",
      "Epoch [6/8], Step [705/750], Loss: 0.0154\n",
      "Epoch [6/8], Step [706/750], Loss: 0.1016\n",
      "Epoch [6/8], Step [707/750], Loss: 0.0089\n",
      "Epoch [6/8], Step [708/750], Loss: 0.0028\n",
      "Epoch [6/8], Step [709/750], Loss: 0.0230\n",
      "Epoch [6/8], Step [710/750], Loss: 0.0092\n",
      "Epoch [6/8], Step [711/750], Loss: 0.0048\n",
      "Epoch [6/8], Step [712/750], Loss: 0.0327\n",
      "Epoch [6/8], Step [713/750], Loss: 0.0430\n",
      "Epoch [6/8], Step [714/750], Loss: 0.0168\n",
      "Epoch [6/8], Step [715/750], Loss: 0.0372\n",
      "Epoch [6/8], Step [716/750], Loss: 0.0524\n",
      "Epoch [6/8], Step [717/750], Loss: 0.0266\n",
      "Epoch [6/8], Step [718/750], Loss: 0.0028\n",
      "Epoch [6/8], Step [719/750], Loss: 0.0394\n",
      "Epoch [6/8], Step [720/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [721/750], Loss: 0.0422\n",
      "Epoch [6/8], Step [722/750], Loss: 0.0804\n",
      "Epoch [6/8], Step [723/750], Loss: 0.0279\n",
      "Epoch [6/8], Step [724/750], Loss: 0.2311\n",
      "Epoch [6/8], Step [725/750], Loss: 0.0096\n",
      "Epoch [6/8], Step [726/750], Loss: 0.0267\n",
      "Epoch [6/8], Step [727/750], Loss: 0.0258\n",
      "Epoch [6/8], Step [728/750], Loss: 0.0095\n",
      "Epoch [6/8], Step [729/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [730/750], Loss: 0.0201\n",
      "Epoch [6/8], Step [731/750], Loss: 0.0250\n",
      "Epoch [6/8], Step [732/750], Loss: 0.0464\n",
      "Epoch [6/8], Step [733/750], Loss: 0.0270\n",
      "Epoch [6/8], Step [734/750], Loss: 0.0756\n",
      "Epoch [6/8], Step [735/750], Loss: 0.0241\n",
      "Epoch [6/8], Step [736/750], Loss: 0.0161\n",
      "Epoch [6/8], Step [737/750], Loss: 0.0147\n",
      "Epoch [6/8], Step [738/750], Loss: 0.0117\n",
      "Epoch [6/8], Step [739/750], Loss: 0.0015\n",
      "Epoch [6/8], Step [740/750], Loss: 0.0084\n",
      "Epoch [6/8], Step [741/750], Loss: 0.0137\n",
      "Epoch [6/8], Step [742/750], Loss: 0.0360\n",
      "Epoch [6/8], Step [743/750], Loss: 0.0028\n",
      "Epoch [6/8], Step [744/750], Loss: 0.1477\n",
      "Epoch [6/8], Step [745/750], Loss: 0.0259\n",
      "Epoch [6/8], Step [746/750], Loss: 0.0173\n",
      "Epoch [6/8], Step [747/750], Loss: 0.0371\n",
      "Epoch [6/8], Step [748/750], Loss: 0.0303\n",
      "Epoch [6/8], Step [749/750], Loss: 0.0548\n",
      "Epoch [6/8], Step [750/750], Loss: 0.0872\n",
      "Epoch [6/8], Tr. loss: 0.3696. Test loss: 0.2568\n",
      "\n",
      "\n",
      "Epoch [7/8], Step [1/750], Loss: 0.0063\n",
      "Epoch [7/8], Step [2/750], Loss: 0.0089\n",
      "Epoch [7/8], Step [3/750], Loss: 0.0176\n",
      "Epoch [7/8], Step [4/750], Loss: 0.0013\n",
      "Epoch [7/8], Step [5/750], Loss: 0.0137\n",
      "Epoch [7/8], Step [6/750], Loss: 0.0310\n",
      "Epoch [7/8], Step [7/750], Loss: 0.0116\n",
      "Epoch [7/8], Step [8/750], Loss: 0.0072\n",
      "Epoch [7/8], Step [9/750], Loss: 0.0103\n",
      "Epoch [7/8], Step [10/750], Loss: 0.0019\n",
      "Epoch [7/8], Step [11/750], Loss: 0.0062\n",
      "Epoch [7/8], Step [12/750], Loss: 0.0015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Step [13/750], Loss: 0.0110\n",
      "Epoch [7/8], Step [14/750], Loss: 0.0012\n",
      "Epoch [7/8], Step [15/750], Loss: 0.0056\n",
      "Epoch [7/8], Step [16/750], Loss: 0.0349\n",
      "Epoch [7/8], Step [17/750], Loss: 0.0022\n",
      "Epoch [7/8], Step [18/750], Loss: 0.0022\n",
      "Epoch [7/8], Step [19/750], Loss: 0.0013\n",
      "Epoch [7/8], Step [20/750], Loss: 0.0038\n",
      "Epoch [7/8], Step [21/750], Loss: 0.0067\n",
      "Epoch [7/8], Step [22/750], Loss: 0.0641\n",
      "Epoch [7/8], Step [23/750], Loss: 0.0016\n",
      "Epoch [7/8], Step [24/750], Loss: 0.0069\n",
      "Epoch [7/8], Step [25/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [26/750], Loss: 0.0186\n",
      "Epoch [7/8], Step [27/750], Loss: 0.0037\n",
      "Epoch [7/8], Step [28/750], Loss: 0.0014\n",
      "Epoch [7/8], Step [29/750], Loss: 0.0041\n",
      "Epoch [7/8], Step [30/750], Loss: 0.0182\n",
      "Epoch [7/8], Step [31/750], Loss: 0.0239\n",
      "Epoch [7/8], Step [32/750], Loss: 0.0022\n",
      "Epoch [7/8], Step [33/750], Loss: 0.0038\n",
      "Epoch [7/8], Step [34/750], Loss: 0.0100\n",
      "Epoch [7/8], Step [35/750], Loss: 0.0065\n",
      "Epoch [7/8], Step [36/750], Loss: 0.0087\n",
      "Epoch [7/8], Step [37/750], Loss: 0.0041\n",
      "Epoch [7/8], Step [38/750], Loss: 0.0033\n",
      "Epoch [7/8], Step [39/750], Loss: 0.0064\n",
      "Epoch [7/8], Step [40/750], Loss: 0.0020\n",
      "Epoch [7/8], Step [41/750], Loss: 0.0088\n",
      "Epoch [7/8], Step [42/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [43/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [44/750], Loss: 0.0077\n",
      "Epoch [7/8], Step [45/750], Loss: 0.0499\n",
      "Epoch [7/8], Step [46/750], Loss: 0.0456\n",
      "Epoch [7/8], Step [47/750], Loss: 0.0124\n",
      "Epoch [7/8], Step [48/750], Loss: 0.0006\n",
      "Epoch [7/8], Step [49/750], Loss: 0.0616\n",
      "Epoch [7/8], Step [50/750], Loss: 0.0014\n",
      "Epoch [7/8], Step [51/750], Loss: 0.0011\n",
      "Epoch [7/8], Step [52/750], Loss: 0.0199\n",
      "Epoch [7/8], Step [53/750], Loss: 0.0187\n",
      "Epoch [7/8], Step [54/750], Loss: 0.0129\n",
      "Epoch [7/8], Step [55/750], Loss: 0.0033\n",
      "Epoch [7/8], Step [56/750], Loss: 0.0005\n",
      "Epoch [7/8], Step [57/750], Loss: 0.0005\n",
      "Epoch [7/8], Step [58/750], Loss: 0.0009\n",
      "Epoch [7/8], Step [59/750], Loss: 0.0020\n",
      "Epoch [7/8], Step [60/750], Loss: 0.0053\n",
      "Epoch [7/8], Step [61/750], Loss: 0.0149\n",
      "Epoch [7/8], Step [62/750], Loss: 0.0374\n",
      "Epoch [7/8], Step [63/750], Loss: 0.0115\n",
      "Epoch [7/8], Step [64/750], Loss: 0.0012\n",
      "Epoch [7/8], Step [65/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [66/750], Loss: 0.0459\n",
      "Epoch [7/8], Step [67/750], Loss: 0.0580\n",
      "Epoch [7/8], Step [68/750], Loss: 0.0050\n",
      "Epoch [7/8], Step [69/750], Loss: 0.0935\n",
      "Epoch [7/8], Step [70/750], Loss: 0.0038\n",
      "Epoch [7/8], Step [71/750], Loss: 0.0062\n",
      "Epoch [7/8], Step [72/750], Loss: 0.0029\n",
      "Epoch [7/8], Step [73/750], Loss: 0.0020\n",
      "Epoch [7/8], Step [74/750], Loss: 0.0090\n",
      "Epoch [7/8], Step [75/750], Loss: 0.0125\n",
      "Epoch [7/8], Step [76/750], Loss: 0.0137\n",
      "Epoch [7/8], Step [77/750], Loss: 0.0050\n",
      "Epoch [7/8], Step [78/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [79/750], Loss: 0.0592\n",
      "Epoch [7/8], Step [80/750], Loss: 0.0049\n",
      "Epoch [7/8], Step [81/750], Loss: 0.0120\n",
      "Epoch [7/8], Step [82/750], Loss: 0.0261\n",
      "Epoch [7/8], Step [83/750], Loss: 0.0033\n",
      "Epoch [7/8], Step [84/750], Loss: 0.0229\n",
      "Epoch [7/8], Step [85/750], Loss: 0.0654\n",
      "Epoch [7/8], Step [86/750], Loss: 0.0035\n",
      "Epoch [7/8], Step [87/750], Loss: 0.0082\n",
      "Epoch [7/8], Step [88/750], Loss: 0.0035\n",
      "Epoch [7/8], Step [89/750], Loss: 0.0482\n",
      "Epoch [7/8], Step [90/750], Loss: 0.0515\n",
      "Epoch [7/8], Step [91/750], Loss: 0.1388\n",
      "Epoch [7/8], Step [92/750], Loss: 0.0075\n",
      "Epoch [7/8], Step [93/750], Loss: 0.0487\n",
      "Epoch [7/8], Step [94/750], Loss: 0.1056\n",
      "Epoch [7/8], Step [95/750], Loss: 0.0603\n",
      "Epoch [7/8], Step [96/750], Loss: 0.0183\n",
      "Epoch [7/8], Step [97/750], Loss: 0.0125\n",
      "Epoch [7/8], Step [98/750], Loss: 0.0019\n",
      "Epoch [7/8], Step [99/750], Loss: 0.0287\n",
      "Epoch [7/8], Step [100/750], Loss: 0.0723\n",
      "Epoch [7/8], Step [101/750], Loss: 0.0497\n",
      "Epoch [7/8], Step [102/750], Loss: 0.1251\n",
      "Epoch [7/8], Step [103/750], Loss: 0.0603\n",
      "Epoch [7/8], Step [104/750], Loss: 0.0059\n",
      "Epoch [7/8], Step [105/750], Loss: 0.0132\n",
      "Epoch [7/8], Step [106/750], Loss: 0.0118\n",
      "Epoch [7/8], Step [107/750], Loss: 0.0050\n",
      "Epoch [7/8], Step [108/750], Loss: 0.0458\n",
      "Epoch [7/8], Step [109/750], Loss: 0.0945\n",
      "Epoch [7/8], Step [110/750], Loss: 0.0248\n",
      "Epoch [7/8], Step [111/750], Loss: 0.0068\n",
      "Epoch [7/8], Step [112/750], Loss: 0.0111\n",
      "Epoch [7/8], Step [113/750], Loss: 0.1299\n",
      "Epoch [7/8], Step [114/750], Loss: 0.0138\n",
      "Epoch [7/8], Step [115/750], Loss: 0.0326\n",
      "Epoch [7/8], Step [116/750], Loss: 0.0030\n",
      "Epoch [7/8], Step [117/750], Loss: 0.0328\n",
      "Epoch [7/8], Step [118/750], Loss: 0.0262\n",
      "Epoch [7/8], Step [119/750], Loss: 0.0211\n",
      "Epoch [7/8], Step [120/750], Loss: 0.0416\n",
      "Epoch [7/8], Step [121/750], Loss: 0.0657\n",
      "Epoch [7/8], Step [122/750], Loss: 0.0530\n",
      "Epoch [7/8], Step [123/750], Loss: 0.0097\n",
      "Epoch [7/8], Step [124/750], Loss: 0.0025\n",
      "Epoch [7/8], Step [125/750], Loss: 0.0081\n",
      "Epoch [7/8], Step [126/750], Loss: 0.0175\n",
      "Epoch [7/8], Step [127/750], Loss: 0.0119\n",
      "Epoch [7/8], Step [128/750], Loss: 0.0399\n",
      "Epoch [7/8], Step [129/750], Loss: 0.0171\n",
      "Epoch [7/8], Step [130/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [131/750], Loss: 0.0651\n",
      "Epoch [7/8], Step [132/750], Loss: 0.0528\n",
      "Epoch [7/8], Step [133/750], Loss: 0.0117\n",
      "Epoch [7/8], Step [134/750], Loss: 0.0495\n",
      "Epoch [7/8], Step [135/750], Loss: 0.0025\n",
      "Epoch [7/8], Step [136/750], Loss: 0.0024\n",
      "Epoch [7/8], Step [137/750], Loss: 0.0064\n",
      "Epoch [7/8], Step [138/750], Loss: 0.0215\n",
      "Epoch [7/8], Step [139/750], Loss: 0.0066\n",
      "Epoch [7/8], Step [140/750], Loss: 0.0264\n",
      "Epoch [7/8], Step [141/750], Loss: 0.0068\n",
      "Epoch [7/8], Step [142/750], Loss: 0.0257\n",
      "Epoch [7/8], Step [143/750], Loss: 0.0190\n",
      "Epoch [7/8], Step [144/750], Loss: 0.0087\n",
      "Epoch [7/8], Step [145/750], Loss: 0.0405\n",
      "Epoch [7/8], Step [146/750], Loss: 0.0017\n",
      "Epoch [7/8], Step [147/750], Loss: 0.0331\n",
      "Epoch [7/8], Step [148/750], Loss: 0.0143\n",
      "Epoch [7/8], Step [149/750], Loss: 0.0008\n",
      "Epoch [7/8], Step [150/750], Loss: 0.0014\n",
      "Epoch [7/8], Step [151/750], Loss: 0.0099\n",
      "Epoch [7/8], Step [152/750], Loss: 0.0374\n",
      "Epoch [7/8], Step [153/750], Loss: 0.1370\n",
      "Epoch [7/8], Step [154/750], Loss: 0.0033\n",
      "Epoch [7/8], Step [155/750], Loss: 0.0088\n",
      "Epoch [7/8], Step [156/750], Loss: 0.0014\n",
      "Epoch [7/8], Step [157/750], Loss: 0.0033\n",
      "Epoch [7/8], Step [158/750], Loss: 0.0104\n",
      "Epoch [7/8], Step [159/750], Loss: 0.0320\n",
      "Epoch [7/8], Step [160/750], Loss: 0.1130\n",
      "Epoch [7/8], Step [161/750], Loss: 0.0075\n",
      "Epoch [7/8], Step [162/750], Loss: 0.0105\n",
      "Epoch [7/8], Step [163/750], Loss: 0.0800\n",
      "Epoch [7/8], Step [164/750], Loss: 0.0023\n",
      "Epoch [7/8], Step [165/750], Loss: 0.0208\n",
      "Epoch [7/8], Step [166/750], Loss: 0.0059\n",
      "Epoch [7/8], Step [167/750], Loss: 0.0758\n",
      "Epoch [7/8], Step [168/750], Loss: 0.0408\n",
      "Epoch [7/8], Step [169/750], Loss: 0.1127\n",
      "Epoch [7/8], Step [170/750], Loss: 0.0963\n",
      "Epoch [7/8], Step [171/750], Loss: 0.0693\n",
      "Epoch [7/8], Step [172/750], Loss: 0.0387\n",
      "Epoch [7/8], Step [173/750], Loss: 0.0185\n",
      "Epoch [7/8], Step [174/750], Loss: 0.0032\n",
      "Epoch [7/8], Step [175/750], Loss: 0.0296\n",
      "Epoch [7/8], Step [176/750], Loss: 0.0144\n",
      "Epoch [7/8], Step [177/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [178/750], Loss: 0.1486\n",
      "Epoch [7/8], Step [179/750], Loss: 0.0068\n",
      "Epoch [7/8], Step [180/750], Loss: 0.0130\n",
      "Epoch [7/8], Step [181/750], Loss: 0.0036\n",
      "Epoch [7/8], Step [182/750], Loss: 0.0259\n",
      "Epoch [7/8], Step [183/750], Loss: 0.0145\n",
      "Epoch [7/8], Step [184/750], Loss: 0.0117\n",
      "Epoch [7/8], Step [185/750], Loss: 0.0089\n",
      "Epoch [7/8], Step [186/750], Loss: 0.1273\n",
      "Epoch [7/8], Step [187/750], Loss: 0.0581\n",
      "Epoch [7/8], Step [188/750], Loss: 0.0226\n",
      "Epoch [7/8], Step [189/750], Loss: 0.0078\n",
      "Epoch [7/8], Step [190/750], Loss: 0.0749\n",
      "Epoch [7/8], Step [191/750], Loss: 0.0023\n",
      "Epoch [7/8], Step [192/750], Loss: 0.0007\n",
      "Epoch [7/8], Step [193/750], Loss: 0.0110\n",
      "Epoch [7/8], Step [194/750], Loss: 0.0079\n",
      "Epoch [7/8], Step [195/750], Loss: 0.0053\n",
      "Epoch [7/8], Step [196/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [197/750], Loss: 0.0218\n",
      "Epoch [7/8], Step [198/750], Loss: 0.0305\n",
      "Epoch [7/8], Step [199/750], Loss: 0.0019\n",
      "Epoch [7/8], Step [200/750], Loss: 0.0293\n",
      "Epoch [7/8], Step [201/750], Loss: 0.0019\n",
      "Epoch [7/8], Step [202/750], Loss: 0.0157\n",
      "Epoch [7/8], Step [203/750], Loss: 0.0043\n",
      "Epoch [7/8], Step [204/750], Loss: 0.0061\n",
      "Epoch [7/8], Step [205/750], Loss: 0.0364\n",
      "Epoch [7/8], Step [206/750], Loss: 0.0096\n",
      "Epoch [7/8], Step [207/750], Loss: 0.0297\n",
      "Epoch [7/8], Step [208/750], Loss: 0.0832\n",
      "Epoch [7/8], Step [209/750], Loss: 0.0322\n",
      "Epoch [7/8], Step [210/750], Loss: 0.0009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Step [211/750], Loss: 0.0387\n",
      "Epoch [7/8], Step [212/750], Loss: 0.0205\n",
      "Epoch [7/8], Step [213/750], Loss: 0.0361\n",
      "Epoch [7/8], Step [214/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [215/750], Loss: 0.0691\n",
      "Epoch [7/8], Step [216/750], Loss: 0.0091\n",
      "Epoch [7/8], Step [217/750], Loss: 0.0562\n",
      "Epoch [7/8], Step [218/750], Loss: 0.0011\n",
      "Epoch [7/8], Step [219/750], Loss: 0.0330\n",
      "Epoch [7/8], Step [220/750], Loss: 0.0020\n",
      "Epoch [7/8], Step [221/750], Loss: 0.0066\n",
      "Epoch [7/8], Step [222/750], Loss: 0.0023\n",
      "Epoch [7/8], Step [223/750], Loss: 0.0076\n",
      "Epoch [7/8], Step [224/750], Loss: 0.0316\n",
      "Epoch [7/8], Step [225/750], Loss: 0.0282\n",
      "Epoch [7/8], Step [226/750], Loss: 0.0294\n",
      "Epoch [7/8], Step [227/750], Loss: 0.0118\n",
      "Epoch [7/8], Step [228/750], Loss: 0.0195\n",
      "Epoch [7/8], Step [229/750], Loss: 0.0054\n",
      "Epoch [7/8], Step [230/750], Loss: 0.0541\n",
      "Epoch [7/8], Step [231/750], Loss: 0.0545\n",
      "Epoch [7/8], Step [232/750], Loss: 0.0429\n",
      "Epoch [7/8], Step [233/750], Loss: 0.0065\n",
      "Epoch [7/8], Step [234/750], Loss: 0.0015\n",
      "Epoch [7/8], Step [235/750], Loss: 0.0044\n",
      "Epoch [7/8], Step [236/750], Loss: 0.0024\n",
      "Epoch [7/8], Step [237/750], Loss: 0.0098\n",
      "Epoch [7/8], Step [238/750], Loss: 0.0152\n",
      "Epoch [7/8], Step [239/750], Loss: 0.0417\n",
      "Epoch [7/8], Step [240/750], Loss: 0.0070\n",
      "Epoch [7/8], Step [241/750], Loss: 0.0100\n",
      "Epoch [7/8], Step [242/750], Loss: 0.0421\n",
      "Epoch [7/8], Step [243/750], Loss: 0.0451\n",
      "Epoch [7/8], Step [244/750], Loss: 0.0164\n",
      "Epoch [7/8], Step [245/750], Loss: 0.0108\n",
      "Epoch [7/8], Step [246/750], Loss: 0.0301\n",
      "Epoch [7/8], Step [247/750], Loss: 0.0402\n",
      "Epoch [7/8], Step [248/750], Loss: 0.0095\n",
      "Epoch [7/8], Step [249/750], Loss: 0.0127\n",
      "Epoch [7/8], Step [250/750], Loss: 0.0016\n",
      "Epoch [7/8], Step [251/750], Loss: 0.0275\n",
      "Epoch [7/8], Step [252/750], Loss: 0.0096\n",
      "Epoch [7/8], Step [253/750], Loss: 0.0074\n",
      "Epoch [7/8], Step [254/750], Loss: 0.0237\n",
      "Epoch [7/8], Step [255/750], Loss: 0.0041\n",
      "Epoch [7/8], Step [256/750], Loss: 0.0013\n",
      "Epoch [7/8], Step [257/750], Loss: 0.0090\n",
      "Epoch [7/8], Step [258/750], Loss: 0.0032\n",
      "Epoch [7/8], Step [259/750], Loss: 0.0184\n",
      "Epoch [7/8], Step [260/750], Loss: 0.0574\n",
      "Epoch [7/8], Step [261/750], Loss: 0.0297\n",
      "Epoch [7/8], Step [262/750], Loss: 0.0061\n",
      "Epoch [7/8], Step [263/750], Loss: 0.0669\n",
      "Epoch [7/8], Step [264/750], Loss: 0.0158\n",
      "Epoch [7/8], Step [265/750], Loss: 0.0066\n",
      "Epoch [7/8], Step [266/750], Loss: 0.0007\n",
      "Epoch [7/8], Step [267/750], Loss: 0.0033\n",
      "Epoch [7/8], Step [268/750], Loss: 0.0021\n",
      "Epoch [7/8], Step [269/750], Loss: 0.0385\n",
      "Epoch [7/8], Step [270/750], Loss: 0.0868\n",
      "Epoch [7/8], Step [271/750], Loss: 0.0469\n",
      "Epoch [7/8], Step [272/750], Loss: 0.0553\n",
      "Epoch [7/8], Step [273/750], Loss: 0.0238\n",
      "Epoch [7/8], Step [274/750], Loss: 0.0111\n",
      "Epoch [7/8], Step [275/750], Loss: 0.1693\n",
      "Epoch [7/8], Step [276/750], Loss: 0.0224\n",
      "Epoch [7/8], Step [277/750], Loss: 0.0048\n",
      "Epoch [7/8], Step [278/750], Loss: 0.0775\n",
      "Epoch [7/8], Step [279/750], Loss: 0.0145\n",
      "Epoch [7/8], Step [280/750], Loss: 0.0355\n",
      "Epoch [7/8], Step [281/750], Loss: 0.0016\n",
      "Epoch [7/8], Step [282/750], Loss: 0.0021\n",
      "Epoch [7/8], Step [283/750], Loss: 0.0086\n",
      "Epoch [7/8], Step [284/750], Loss: 0.0010\n",
      "Epoch [7/8], Step [285/750], Loss: 0.0219\n",
      "Epoch [7/8], Step [286/750], Loss: 0.0228\n",
      "Epoch [7/8], Step [287/750], Loss: 0.0200\n",
      "Epoch [7/8], Step [288/750], Loss: 0.0009\n",
      "Epoch [7/8], Step [289/750], Loss: 0.0042\n",
      "Epoch [7/8], Step [290/750], Loss: 0.0031\n",
      "Epoch [7/8], Step [291/750], Loss: 0.0057\n",
      "Epoch [7/8], Step [292/750], Loss: 0.0129\n",
      "Epoch [7/8], Step [293/750], Loss: 0.0126\n",
      "Epoch [7/8], Step [294/750], Loss: 0.0130\n",
      "Epoch [7/8], Step [295/750], Loss: 0.0146\n",
      "Epoch [7/8], Step [296/750], Loss: 0.0248\n",
      "Epoch [7/8], Step [297/750], Loss: 0.0043\n",
      "Epoch [7/8], Step [298/750], Loss: 0.0498\n",
      "Epoch [7/8], Step [299/750], Loss: 0.0013\n",
      "Epoch [7/8], Step [300/750], Loss: 0.0775\n",
      "Epoch [7/8], Step [301/750], Loss: 0.0248\n",
      "Epoch [7/8], Step [302/750], Loss: 0.0782\n",
      "Epoch [7/8], Step [303/750], Loss: 0.0066\n",
      "Epoch [7/8], Step [304/750], Loss: 0.0517\n",
      "Epoch [7/8], Step [305/750], Loss: 0.0391\n",
      "Epoch [7/8], Step [306/750], Loss: 0.1084\n",
      "Epoch [7/8], Step [307/750], Loss: 0.0036\n",
      "Epoch [7/8], Step [308/750], Loss: 0.0130\n",
      "Epoch [7/8], Step [309/750], Loss: 0.0061\n",
      "Epoch [7/8], Step [310/750], Loss: 0.0468\n",
      "Epoch [7/8], Step [311/750], Loss: 0.0076\n",
      "Epoch [7/8], Step [312/750], Loss: 0.0325\n",
      "Epoch [7/8], Step [313/750], Loss: 0.0131\n",
      "Epoch [7/8], Step [314/750], Loss: 0.0059\n",
      "Epoch [7/8], Step [315/750], Loss: 0.0062\n",
      "Epoch [7/8], Step [316/750], Loss: 0.0092\n",
      "Epoch [7/8], Step [317/750], Loss: 0.0017\n",
      "Epoch [7/8], Step [318/750], Loss: 0.0580\n",
      "Epoch [7/8], Step [319/750], Loss: 0.0158\n",
      "Epoch [7/8], Step [320/750], Loss: 0.0311\n",
      "Epoch [7/8], Step [321/750], Loss: 0.0066\n",
      "Epoch [7/8], Step [322/750], Loss: 0.0021\n",
      "Epoch [7/8], Step [323/750], Loss: 0.0022\n",
      "Epoch [7/8], Step [324/750], Loss: 0.0388\n",
      "Epoch [7/8], Step [325/750], Loss: 0.0770\n",
      "Epoch [7/8], Step [326/750], Loss: 0.0045\n",
      "Epoch [7/8], Step [327/750], Loss: 0.0019\n",
      "Epoch [7/8], Step [328/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [329/750], Loss: 0.0185\n",
      "Epoch [7/8], Step [330/750], Loss: 0.0021\n",
      "Epoch [7/8], Step [331/750], Loss: 0.0153\n",
      "Epoch [7/8], Step [332/750], Loss: 0.0016\n",
      "Epoch [7/8], Step [333/750], Loss: 0.0980\n",
      "Epoch [7/8], Step [334/750], Loss: 0.0604\n",
      "Epoch [7/8], Step [335/750], Loss: 0.0088\n",
      "Epoch [7/8], Step [336/750], Loss: 0.0247\n",
      "Epoch [7/8], Step [337/750], Loss: 0.0046\n",
      "Epoch [7/8], Step [338/750], Loss: 0.0636\n",
      "Epoch [7/8], Step [339/750], Loss: 0.0400\n",
      "Epoch [7/8], Step [340/750], Loss: 0.0056\n",
      "Epoch [7/8], Step [341/750], Loss: 0.0259\n",
      "Epoch [7/8], Step [342/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [343/750], Loss: 0.0745\n",
      "Epoch [7/8], Step [344/750], Loss: 0.1236\n",
      "Epoch [7/8], Step [345/750], Loss: 0.0820\n",
      "Epoch [7/8], Step [346/750], Loss: 0.0574\n",
      "Epoch [7/8], Step [347/750], Loss: 0.0172\n",
      "Epoch [7/8], Step [348/750], Loss: 0.0224\n",
      "Epoch [7/8], Step [349/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [350/750], Loss: 0.1226\n",
      "Epoch [7/8], Step [351/750], Loss: 0.0864\n",
      "Epoch [7/8], Step [352/750], Loss: 0.0274\n",
      "Epoch [7/8], Step [353/750], Loss: 0.0365\n",
      "Epoch [7/8], Step [354/750], Loss: 0.0140\n",
      "Epoch [7/8], Step [355/750], Loss: 0.0972\n",
      "Epoch [7/8], Step [356/750], Loss: 0.0094\n",
      "Epoch [7/8], Step [357/750], Loss: 0.0276\n",
      "Epoch [7/8], Step [358/750], Loss: 0.1317\n",
      "Epoch [7/8], Step [359/750], Loss: 0.0048\n",
      "Epoch [7/8], Step [360/750], Loss: 0.0574\n",
      "Epoch [7/8], Step [361/750], Loss: 0.0103\n",
      "Epoch [7/8], Step [362/750], Loss: 0.0112\n",
      "Epoch [7/8], Step [363/750], Loss: 0.0226\n",
      "Epoch [7/8], Step [364/750], Loss: 0.0134\n",
      "Epoch [7/8], Step [365/750], Loss: 0.1329\n",
      "Epoch [7/8], Step [366/750], Loss: 0.0029\n",
      "Epoch [7/8], Step [367/750], Loss: 0.0496\n",
      "Epoch [7/8], Step [368/750], Loss: 0.0085\n",
      "Epoch [7/8], Step [369/750], Loss: 0.0036\n",
      "Epoch [7/8], Step [370/750], Loss: 0.0042\n",
      "Epoch [7/8], Step [371/750], Loss: 0.0189\n",
      "Epoch [7/8], Step [372/750], Loss: 0.0041\n",
      "Epoch [7/8], Step [373/750], Loss: 0.0548\n",
      "Epoch [7/8], Step [374/750], Loss: 0.0171\n",
      "Epoch [7/8], Step [375/750], Loss: 0.0149\n",
      "Epoch [7/8], Step [376/750], Loss: 0.0373\n",
      "Epoch [7/8], Step [377/750], Loss: 0.0032\n",
      "Epoch [7/8], Step [378/750], Loss: 0.0629\n",
      "Epoch [7/8], Step [379/750], Loss: 0.0175\n",
      "Epoch [7/8], Step [380/750], Loss: 0.0021\n",
      "Epoch [7/8], Step [381/750], Loss: 0.0273\n",
      "Epoch [7/8], Step [382/750], Loss: 0.0306\n",
      "Epoch [7/8], Step [383/750], Loss: 0.0230\n",
      "Epoch [7/8], Step [384/750], Loss: 0.0866\n",
      "Epoch [7/8], Step [385/750], Loss: 0.1756\n",
      "Epoch [7/8], Step [386/750], Loss: 0.0157\n",
      "Epoch [7/8], Step [387/750], Loss: 0.0052\n",
      "Epoch [7/8], Step [388/750], Loss: 0.0097\n",
      "Epoch [7/8], Step [389/750], Loss: 0.0351\n",
      "Epoch [7/8], Step [390/750], Loss: 0.0118\n",
      "Epoch [7/8], Step [391/750], Loss: 0.0047\n",
      "Epoch [7/8], Step [392/750], Loss: 0.1019\n",
      "Epoch [7/8], Step [393/750], Loss: 0.1234\n",
      "Epoch [7/8], Step [394/750], Loss: 0.0076\n",
      "Epoch [7/8], Step [395/750], Loss: 0.0089\n",
      "Epoch [7/8], Step [396/750], Loss: 0.0110\n",
      "Epoch [7/8], Step [397/750], Loss: 0.0052\n",
      "Epoch [7/8], Step [398/750], Loss: 0.0131\n",
      "Epoch [7/8], Step [399/750], Loss: 0.0151\n",
      "Epoch [7/8], Step [400/750], Loss: 0.0585\n",
      "Epoch [7/8], Step [401/750], Loss: 0.0619\n",
      "Epoch [7/8], Step [402/750], Loss: 0.0073\n",
      "Epoch [7/8], Step [403/750], Loss: 0.0414\n",
      "Epoch [7/8], Step [404/750], Loss: 0.0623\n",
      "Epoch [7/8], Step [405/750], Loss: 0.0520\n",
      "Epoch [7/8], Step [406/750], Loss: 0.0062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Step [407/750], Loss: 0.0103\n",
      "Epoch [7/8], Step [408/750], Loss: 0.0169\n",
      "Epoch [7/8], Step [409/750], Loss: 0.0258\n",
      "Epoch [7/8], Step [410/750], Loss: 0.0683\n",
      "Epoch [7/8], Step [411/750], Loss: 0.0367\n",
      "Epoch [7/8], Step [412/750], Loss: 0.0157\n",
      "Epoch [7/8], Step [413/750], Loss: 0.0511\n",
      "Epoch [7/8], Step [414/750], Loss: 0.0166\n",
      "Epoch [7/8], Step [415/750], Loss: 0.0106\n",
      "Epoch [7/8], Step [416/750], Loss: 0.0920\n",
      "Epoch [7/8], Step [417/750], Loss: 0.0038\n",
      "Epoch [7/8], Step [418/750], Loss: 0.0307\n",
      "Epoch [7/8], Step [419/750], Loss: 0.0399\n",
      "Epoch [7/8], Step [420/750], Loss: 0.0045\n",
      "Epoch [7/8], Step [421/750], Loss: 0.0048\n",
      "Epoch [7/8], Step [422/750], Loss: 0.0097\n",
      "Epoch [7/8], Step [423/750], Loss: 0.0094\n",
      "Epoch [7/8], Step [424/750], Loss: 0.1687\n",
      "Epoch [7/8], Step [425/750], Loss: 0.0132\n",
      "Epoch [7/8], Step [426/750], Loss: 0.0086\n",
      "Epoch [7/8], Step [427/750], Loss: 0.0607\n",
      "Epoch [7/8], Step [428/750], Loss: 0.0313\n",
      "Epoch [7/8], Step [429/750], Loss: 0.0506\n",
      "Epoch [7/8], Step [430/750], Loss: 0.0166\n",
      "Epoch [7/8], Step [431/750], Loss: 0.0257\n",
      "Epoch [7/8], Step [432/750], Loss: 0.0321\n",
      "Epoch [7/8], Step [433/750], Loss: 0.0058\n",
      "Epoch [7/8], Step [434/750], Loss: 0.0364\n",
      "Epoch [7/8], Step [435/750], Loss: 0.0411\n",
      "Epoch [7/8], Step [436/750], Loss: 0.1044\n",
      "Epoch [7/8], Step [437/750], Loss: 0.0421\n",
      "Epoch [7/8], Step [438/750], Loss: 0.0081\n",
      "Epoch [7/8], Step [439/750], Loss: 0.0057\n",
      "Epoch [7/8], Step [440/750], Loss: 0.0392\n",
      "Epoch [7/8], Step [441/750], Loss: 0.1081\n",
      "Epoch [7/8], Step [442/750], Loss: 0.0071\n",
      "Epoch [7/8], Step [443/750], Loss: 0.0667\n",
      "Epoch [7/8], Step [444/750], Loss: 0.0028\n",
      "Epoch [7/8], Step [445/750], Loss: 0.0444\n",
      "Epoch [7/8], Step [446/750], Loss: 0.0332\n",
      "Epoch [7/8], Step [447/750], Loss: 0.0165\n",
      "Epoch [7/8], Step [448/750], Loss: 0.0063\n",
      "Epoch [7/8], Step [449/750], Loss: 0.1126\n",
      "Epoch [7/8], Step [450/750], Loss: 0.0077\n",
      "Epoch [7/8], Step [451/750], Loss: 0.0032\n",
      "Epoch [7/8], Step [452/750], Loss: 0.0084\n",
      "Epoch [7/8], Step [453/750], Loss: 0.0303\n",
      "Epoch [7/8], Step [454/750], Loss: 0.0076\n",
      "Epoch [7/8], Step [455/750], Loss: 0.0179\n",
      "Epoch [7/8], Step [456/750], Loss: 0.0858\n",
      "Epoch [7/8], Step [457/750], Loss: 0.0133\n",
      "Epoch [7/8], Step [458/750], Loss: 0.1240\n",
      "Epoch [7/8], Step [459/750], Loss: 0.0364\n",
      "Epoch [7/8], Step [460/750], Loss: 0.0016\n",
      "Epoch [7/8], Step [461/750], Loss: 0.0306\n",
      "Epoch [7/8], Step [462/750], Loss: 0.0169\n",
      "Epoch [7/8], Step [463/750], Loss: 0.0079\n",
      "Epoch [7/8], Step [464/750], Loss: 0.0039\n",
      "Epoch [7/8], Step [465/750], Loss: 0.0492\n",
      "Epoch [7/8], Step [466/750], Loss: 0.0580\n",
      "Epoch [7/8], Step [467/750], Loss: 0.0237\n",
      "Epoch [7/8], Step [468/750], Loss: 0.0106\n",
      "Epoch [7/8], Step [469/750], Loss: 0.0682\n",
      "Epoch [7/8], Step [470/750], Loss: 0.0045\n",
      "Epoch [7/8], Step [471/750], Loss: 0.0142\n",
      "Epoch [7/8], Step [472/750], Loss: 0.0023\n",
      "Epoch [7/8], Step [473/750], Loss: 0.0652\n",
      "Epoch [7/8], Step [474/750], Loss: 0.0089\n",
      "Epoch [7/8], Step [475/750], Loss: 0.0064\n",
      "Epoch [7/8], Step [476/750], Loss: 0.0146\n",
      "Epoch [7/8], Step [477/750], Loss: 0.0095\n",
      "Epoch [7/8], Step [478/750], Loss: 0.0211\n",
      "Epoch [7/8], Step [479/750], Loss: 0.0228\n",
      "Epoch [7/8], Step [480/750], Loss: 0.0089\n",
      "Epoch [7/8], Step [481/750], Loss: 0.0025\n",
      "Epoch [7/8], Step [482/750], Loss: 0.0376\n",
      "Epoch [7/8], Step [483/750], Loss: 0.0134\n",
      "Epoch [7/8], Step [484/750], Loss: 0.0022\n",
      "Epoch [7/8], Step [485/750], Loss: 0.0163\n",
      "Epoch [7/8], Step [486/750], Loss: 0.0038\n",
      "Epoch [7/8], Step [487/750], Loss: 0.0374\n",
      "Epoch [7/8], Step [488/750], Loss: 0.0543\n",
      "Epoch [7/8], Step [489/750], Loss: 0.0336\n",
      "Epoch [7/8], Step [490/750], Loss: 0.0027\n",
      "Epoch [7/8], Step [491/750], Loss: 0.0013\n",
      "Epoch [7/8], Step [492/750], Loss: 0.0086\n",
      "Epoch [7/8], Step [493/750], Loss: 0.0393\n",
      "Epoch [7/8], Step [494/750], Loss: 0.1026\n",
      "Epoch [7/8], Step [495/750], Loss: 0.0396\n",
      "Epoch [7/8], Step [496/750], Loss: 0.0027\n",
      "Epoch [7/8], Step [497/750], Loss: 0.0225\n",
      "Epoch [7/8], Step [498/750], Loss: 0.0520\n",
      "Epoch [7/8], Step [499/750], Loss: 0.0476\n",
      "Epoch [7/8], Step [500/750], Loss: 0.0308\n",
      "Epoch [7/8], Step [501/750], Loss: 0.0049\n",
      "Epoch [7/8], Step [502/750], Loss: 0.0057\n",
      "Epoch [7/8], Step [503/750], Loss: 0.0252\n",
      "Epoch [7/8], Step [504/750], Loss: 0.0042\n",
      "Epoch [7/8], Step [505/750], Loss: 0.0128\n",
      "Epoch [7/8], Step [506/750], Loss: 0.0461\n",
      "Epoch [7/8], Step [507/750], Loss: 0.0291\n",
      "Epoch [7/8], Step [508/750], Loss: 0.0129\n",
      "Epoch [7/8], Step [509/750], Loss: 0.0075\n",
      "Epoch [7/8], Step [510/750], Loss: 0.0030\n",
      "Epoch [7/8], Step [511/750], Loss: 0.0398\n",
      "Epoch [7/8], Step [512/750], Loss: 0.0011\n",
      "Epoch [7/8], Step [513/750], Loss: 0.0015\n",
      "Epoch [7/8], Step [514/750], Loss: 0.0046\n",
      "Epoch [7/8], Step [515/750], Loss: 0.0035\n",
      "Epoch [7/8], Step [516/750], Loss: 0.0683\n",
      "Epoch [7/8], Step [517/750], Loss: 0.0314\n",
      "Epoch [7/8], Step [518/750], Loss: 0.0098\n",
      "Epoch [7/8], Step [519/750], Loss: 0.0262\n",
      "Epoch [7/8], Step [520/750], Loss: 0.0454\n",
      "Epoch [7/8], Step [521/750], Loss: 0.0012\n",
      "Epoch [7/8], Step [522/750], Loss: 0.0063\n",
      "Epoch [7/8], Step [523/750], Loss: 0.0024\n",
      "Epoch [7/8], Step [524/750], Loss: 0.0013\n",
      "Epoch [7/8], Step [525/750], Loss: 0.0435\n",
      "Epoch [7/8], Step [526/750], Loss: 0.0069\n",
      "Epoch [7/8], Step [527/750], Loss: 0.0263\n",
      "Epoch [7/8], Step [528/750], Loss: 0.0028\n",
      "Epoch [7/8], Step [529/750], Loss: 0.0045\n",
      "Epoch [7/8], Step [530/750], Loss: 0.0205\n",
      "Epoch [7/8], Step [531/750], Loss: 0.0684\n",
      "Epoch [7/8], Step [532/750], Loss: 0.0245\n",
      "Epoch [7/8], Step [533/750], Loss: 0.0096\n",
      "Epoch [7/8], Step [534/750], Loss: 0.0158\n",
      "Epoch [7/8], Step [535/750], Loss: 0.0689\n",
      "Epoch [7/8], Step [536/750], Loss: 0.0517\n",
      "Epoch [7/8], Step [537/750], Loss: 0.1170\n",
      "Epoch [7/8], Step [538/750], Loss: 0.0355\n",
      "Epoch [7/8], Step [539/750], Loss: 0.0343\n",
      "Epoch [7/8], Step [540/750], Loss: 0.0108\n",
      "Epoch [7/8], Step [541/750], Loss: 0.0329\n",
      "Epoch [7/8], Step [542/750], Loss: 0.0236\n",
      "Epoch [7/8], Step [543/750], Loss: 0.0454\n",
      "Epoch [7/8], Step [544/750], Loss: 0.0034\n",
      "Epoch [7/8], Step [545/750], Loss: 0.0010\n",
      "Epoch [7/8], Step [546/750], Loss: 0.0101\n",
      "Epoch [7/8], Step [547/750], Loss: 0.0099\n",
      "Epoch [7/8], Step [548/750], Loss: 0.0037\n",
      "Epoch [7/8], Step [549/750], Loss: 0.0357\n",
      "Epoch [7/8], Step [550/750], Loss: 0.0334\n",
      "Epoch [7/8], Step [551/750], Loss: 0.0101\n",
      "Epoch [7/8], Step [552/750], Loss: 0.0321\n",
      "Epoch [7/8], Step [553/750], Loss: 0.0107\n",
      "Epoch [7/8], Step [554/750], Loss: 0.0695\n",
      "Epoch [7/8], Step [555/750], Loss: 0.0766\n",
      "Epoch [7/8], Step [556/750], Loss: 0.1824\n",
      "Epoch [7/8], Step [557/750], Loss: 0.0392\n",
      "Epoch [7/8], Step [558/750], Loss: 0.0025\n",
      "Epoch [7/8], Step [559/750], Loss: 0.1483\n",
      "Epoch [7/8], Step [560/750], Loss: 0.0144\n",
      "Epoch [7/8], Step [561/750], Loss: 0.0070\n",
      "Epoch [7/8], Step [562/750], Loss: 0.0070\n",
      "Epoch [7/8], Step [563/750], Loss: 0.0599\n",
      "Epoch [7/8], Step [564/750], Loss: 0.0748\n",
      "Epoch [7/8], Step [565/750], Loss: 0.0438\n",
      "Epoch [7/8], Step [566/750], Loss: 0.0161\n",
      "Epoch [7/8], Step [567/750], Loss: 0.0324\n",
      "Epoch [7/8], Step [568/750], Loss: 0.0115\n",
      "Epoch [7/8], Step [569/750], Loss: 0.0630\n",
      "Epoch [7/8], Step [570/750], Loss: 0.0185\n",
      "Epoch [7/8], Step [571/750], Loss: 0.0066\n",
      "Epoch [7/8], Step [572/750], Loss: 0.1235\n",
      "Epoch [7/8], Step [573/750], Loss: 0.0064\n",
      "Epoch [7/8], Step [574/750], Loss: 0.0085\n",
      "Epoch [7/8], Step [575/750], Loss: 0.0036\n",
      "Epoch [7/8], Step [576/750], Loss: 0.1201\n",
      "Epoch [7/8], Step [577/750], Loss: 0.0311\n",
      "Epoch [7/8], Step [578/750], Loss: 0.0762\n",
      "Epoch [7/8], Step [579/750], Loss: 0.0169\n",
      "Epoch [7/8], Step [580/750], Loss: 0.0025\n",
      "Epoch [7/8], Step [581/750], Loss: 0.0024\n",
      "Epoch [7/8], Step [582/750], Loss: 0.0115\n",
      "Epoch [7/8], Step [583/750], Loss: 0.0116\n",
      "Epoch [7/8], Step [584/750], Loss: 0.1252\n",
      "Epoch [7/8], Step [585/750], Loss: 0.0475\n",
      "Epoch [7/8], Step [586/750], Loss: 0.0120\n",
      "Epoch [7/8], Step [587/750], Loss: 0.0072\n",
      "Epoch [7/8], Step [588/750], Loss: 0.1144\n",
      "Epoch [7/8], Step [589/750], Loss: 0.0288\n",
      "Epoch [7/8], Step [590/750], Loss: 0.0063\n",
      "Epoch [7/8], Step [591/750], Loss: 0.0123\n",
      "Epoch [7/8], Step [592/750], Loss: 0.1751\n",
      "Epoch [7/8], Step [593/750], Loss: 0.0150\n",
      "Epoch [7/8], Step [594/750], Loss: 0.0270\n",
      "Epoch [7/8], Step [595/750], Loss: 0.0735\n",
      "Epoch [7/8], Step [596/750], Loss: 0.0037\n",
      "Epoch [7/8], Step [597/750], Loss: 0.0224\n",
      "Epoch [7/8], Step [598/750], Loss: 0.0049\n",
      "Epoch [7/8], Step [599/750], Loss: 0.0287\n",
      "Epoch [7/8], Step [600/750], Loss: 0.1334\n",
      "Epoch [7/8], Step [601/750], Loss: 0.0102\n",
      "Epoch [7/8], Step [602/750], Loss: 0.0755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Step [603/750], Loss: 0.0095\n",
      "Epoch [7/8], Step [604/750], Loss: 0.0149\n",
      "Epoch [7/8], Step [605/750], Loss: 0.0026\n",
      "Epoch [7/8], Step [606/750], Loss: 0.0259\n",
      "Epoch [7/8], Step [607/750], Loss: 0.0362\n",
      "Epoch [7/8], Step [608/750], Loss: 0.0091\n",
      "Epoch [7/8], Step [609/750], Loss: 0.0736\n",
      "Epoch [7/8], Step [610/750], Loss: 0.0174\n",
      "Epoch [7/8], Step [611/750], Loss: 0.0245\n",
      "Epoch [7/8], Step [612/750], Loss: 0.1610\n",
      "Epoch [7/8], Step [613/750], Loss: 0.0138\n",
      "Epoch [7/8], Step [614/750], Loss: 0.0143\n",
      "Epoch [7/8], Step [615/750], Loss: 0.0510\n",
      "Epoch [7/8], Step [616/750], Loss: 0.0175\n",
      "Epoch [7/8], Step [617/750], Loss: 0.0638\n",
      "Epoch [7/8], Step [618/750], Loss: 0.0030\n",
      "Epoch [7/8], Step [619/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [620/750], Loss: 0.0293\n",
      "Epoch [7/8], Step [621/750], Loss: 0.0046\n",
      "Epoch [7/8], Step [622/750], Loss: 0.0255\n",
      "Epoch [7/8], Step [623/750], Loss: 0.0059\n",
      "Epoch [7/8], Step [624/750], Loss: 0.0262\n",
      "Epoch [7/8], Step [625/750], Loss: 0.0034\n",
      "Epoch [7/8], Step [626/750], Loss: 0.0064\n",
      "Epoch [7/8], Step [627/750], Loss: 0.2574\n",
      "Epoch [7/8], Step [628/750], Loss: 0.0211\n",
      "Epoch [7/8], Step [629/750], Loss: 0.0208\n",
      "Epoch [7/8], Step [630/750], Loss: 0.0079\n",
      "Epoch [7/8], Step [631/750], Loss: 0.0207\n",
      "Epoch [7/8], Step [632/750], Loss: 0.0012\n",
      "Epoch [7/8], Step [633/750], Loss: 0.0300\n",
      "Epoch [7/8], Step [634/750], Loss: 0.0066\n",
      "Epoch [7/8], Step [635/750], Loss: 0.0293\n",
      "Epoch [7/8], Step [636/750], Loss: 0.0140\n",
      "Epoch [7/8], Step [637/750], Loss: 0.0656\n",
      "Epoch [7/8], Step [638/750], Loss: 0.1208\n",
      "Epoch [7/8], Step [639/750], Loss: 0.0021\n",
      "Epoch [7/8], Step [640/750], Loss: 0.0135\n",
      "Epoch [7/8], Step [641/750], Loss: 0.0009\n",
      "Epoch [7/8], Step [642/750], Loss: 0.0075\n",
      "Epoch [7/8], Step [643/750], Loss: 0.0271\n",
      "Epoch [7/8], Step [644/750], Loss: 0.0073\n",
      "Epoch [7/8], Step [645/750], Loss: 0.0010\n",
      "Epoch [7/8], Step [646/750], Loss: 0.1363\n",
      "Epoch [7/8], Step [647/750], Loss: 0.0067\n",
      "Epoch [7/8], Step [648/750], Loss: 0.0168\n",
      "Epoch [7/8], Step [649/750], Loss: 0.0083\n",
      "Epoch [7/8], Step [650/750], Loss: 0.0173\n",
      "Epoch [7/8], Step [651/750], Loss: 0.0020\n",
      "Epoch [7/8], Step [652/750], Loss: 0.0860\n",
      "Epoch [7/8], Step [653/750], Loss: 0.0020\n",
      "Epoch [7/8], Step [654/750], Loss: 0.0117\n",
      "Epoch [7/8], Step [655/750], Loss: 0.0098\n",
      "Epoch [7/8], Step [656/750], Loss: 0.0044\n",
      "Epoch [7/8], Step [657/750], Loss: 0.0761\n",
      "Epoch [7/8], Step [658/750], Loss: 0.0306\n",
      "Epoch [7/8], Step [659/750], Loss: 0.0059\n",
      "Epoch [7/8], Step [660/750], Loss: 0.0182\n",
      "Epoch [7/8], Step [661/750], Loss: 0.0161\n",
      "Epoch [7/8], Step [662/750], Loss: 0.0054\n",
      "Epoch [7/8], Step [663/750], Loss: 0.0011\n",
      "Epoch [7/8], Step [664/750], Loss: 0.0077\n",
      "Epoch [7/8], Step [665/750], Loss: 0.0054\n",
      "Epoch [7/8], Step [666/750], Loss: 0.0062\n",
      "Epoch [7/8], Step [667/750], Loss: 0.0055\n",
      "Epoch [7/8], Step [668/750], Loss: 0.0028\n",
      "Epoch [7/8], Step [669/750], Loss: 0.0261\n",
      "Epoch [7/8], Step [670/750], Loss: 0.0015\n",
      "Epoch [7/8], Step [671/750], Loss: 0.0058\n",
      "Epoch [7/8], Step [672/750], Loss: 0.0010\n",
      "Epoch [7/8], Step [673/750], Loss: 0.0301\n",
      "Epoch [7/8], Step [674/750], Loss: 0.0197\n",
      "Epoch [7/8], Step [675/750], Loss: 0.0025\n",
      "Epoch [7/8], Step [676/750], Loss: 0.0023\n",
      "Epoch [7/8], Step [677/750], Loss: 0.0044\n",
      "Epoch [7/8], Step [678/750], Loss: 0.0059\n",
      "Epoch [7/8], Step [679/750], Loss: 0.0482\n",
      "Epoch [7/8], Step [680/750], Loss: 0.0089\n",
      "Epoch [7/8], Step [681/750], Loss: 0.0631\n",
      "Epoch [7/8], Step [682/750], Loss: 0.0318\n",
      "Epoch [7/8], Step [683/750], Loss: 0.1288\n",
      "Epoch [7/8], Step [684/750], Loss: 0.0251\n",
      "Epoch [7/8], Step [685/750], Loss: 0.0014\n",
      "Epoch [7/8], Step [686/750], Loss: 0.0100\n",
      "Epoch [7/8], Step [687/750], Loss: 0.0016\n",
      "Epoch [7/8], Step [688/750], Loss: 0.0076\n",
      "Epoch [7/8], Step [689/750], Loss: 0.0215\n",
      "Epoch [7/8], Step [690/750], Loss: 0.0260\n",
      "Epoch [7/8], Step [691/750], Loss: 0.0845\n",
      "Epoch [7/8], Step [692/750], Loss: 0.0072\n",
      "Epoch [7/8], Step [693/750], Loss: 0.1023\n",
      "Epoch [7/8], Step [694/750], Loss: 0.0008\n",
      "Epoch [7/8], Step [695/750], Loss: 0.0059\n",
      "Epoch [7/8], Step [696/750], Loss: 0.0796\n",
      "Epoch [7/8], Step [697/750], Loss: 0.0135\n",
      "Epoch [7/8], Step [698/750], Loss: 0.0102\n",
      "Epoch [7/8], Step [699/750], Loss: 0.0013\n",
      "Epoch [7/8], Step [700/750], Loss: 0.0079\n",
      "Epoch [7/8], Step [701/750], Loss: 0.1030\n",
      "Epoch [7/8], Step [702/750], Loss: 0.0030\n",
      "Epoch [7/8], Step [703/750], Loss: 0.0025\n",
      "Epoch [7/8], Step [704/750], Loss: 0.0069\n",
      "Epoch [7/8], Step [705/750], Loss: 0.0380\n",
      "Epoch [7/8], Step [706/750], Loss: 0.0045\n",
      "Epoch [7/8], Step [707/750], Loss: 0.0036\n",
      "Epoch [7/8], Step [708/750], Loss: 0.0071\n",
      "Epoch [7/8], Step [709/750], Loss: 0.0126\n",
      "Epoch [7/8], Step [710/750], Loss: 0.0396\n",
      "Epoch [7/8], Step [711/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [712/750], Loss: 0.0019\n",
      "Epoch [7/8], Step [713/750], Loss: 0.0050\n",
      "Epoch [7/8], Step [714/750], Loss: 0.0136\n",
      "Epoch [7/8], Step [715/750], Loss: 0.0332\n",
      "Epoch [7/8], Step [716/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [717/750], Loss: 0.0164\n",
      "Epoch [7/8], Step [718/750], Loss: 0.0069\n",
      "Epoch [7/8], Step [719/750], Loss: 0.0048\n",
      "Epoch [7/8], Step [720/750], Loss: 0.0661\n",
      "Epoch [7/8], Step [721/750], Loss: 0.0400\n",
      "Epoch [7/8], Step [722/750], Loss: 0.0135\n",
      "Epoch [7/8], Step [723/750], Loss: 0.0023\n",
      "Epoch [7/8], Step [724/750], Loss: 0.0081\n",
      "Epoch [7/8], Step [725/750], Loss: 0.0426\n",
      "Epoch [7/8], Step [726/750], Loss: 0.0015\n",
      "Epoch [7/8], Step [727/750], Loss: 0.0786\n",
      "Epoch [7/8], Step [728/750], Loss: 0.0088\n",
      "Epoch [7/8], Step [729/750], Loss: 0.0010\n",
      "Epoch [7/8], Step [730/750], Loss: 0.0059\n",
      "Epoch [7/8], Step [731/750], Loss: 0.0857\n",
      "Epoch [7/8], Step [732/750], Loss: 0.0032\n",
      "Epoch [7/8], Step [733/750], Loss: 0.0056\n",
      "Epoch [7/8], Step [734/750], Loss: 0.1171\n",
      "Epoch [7/8], Step [735/750], Loss: 0.0332\n",
      "Epoch [7/8], Step [736/750], Loss: 0.0072\n",
      "Epoch [7/8], Step [737/750], Loss: 0.0101\n",
      "Epoch [7/8], Step [738/750], Loss: 0.0016\n",
      "Epoch [7/8], Step [739/750], Loss: 0.0039\n",
      "Epoch [7/8], Step [740/750], Loss: 0.0049\n",
      "Epoch [7/8], Step [741/750], Loss: 0.0120\n",
      "Epoch [7/8], Step [742/750], Loss: 0.0241\n",
      "Epoch [7/8], Step [743/750], Loss: 0.0270\n",
      "Epoch [7/8], Step [744/750], Loss: 0.0265\n",
      "Epoch [7/8], Step [745/750], Loss: 0.0172\n",
      "Epoch [7/8], Step [746/750], Loss: 0.0008\n",
      "Epoch [7/8], Step [747/750], Loss: 0.0025\n",
      "Epoch [7/8], Step [748/750], Loss: 0.0027\n",
      "Epoch [7/8], Step [749/750], Loss: 0.0023\n",
      "Epoch [7/8], Step [750/750], Loss: 0.0083\n",
      "Epoch [7/8], Tr. loss: 0.3968. Test loss: 0.2895\n",
      "\n",
      "\n",
      "Epoch [8/8], Step [1/750], Loss: 0.0235\n",
      "Epoch [8/8], Step [2/750], Loss: 0.0025\n",
      "Epoch [8/8], Step [3/750], Loss: 0.0123\n",
      "Epoch [8/8], Step [4/750], Loss: 0.0040\n",
      "Epoch [8/8], Step [5/750], Loss: 0.0628\n",
      "Epoch [8/8], Step [6/750], Loss: 0.0030\n",
      "Epoch [8/8], Step [7/750], Loss: 0.0068\n",
      "Epoch [8/8], Step [8/750], Loss: 0.0028\n",
      "Epoch [8/8], Step [9/750], Loss: 0.0032\n",
      "Epoch [8/8], Step [10/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [11/750], Loss: 0.0458\n",
      "Epoch [8/8], Step [12/750], Loss: 0.0595\n",
      "Epoch [8/8], Step [13/750], Loss: 0.0026\n",
      "Epoch [8/8], Step [14/750], Loss: 0.0236\n",
      "Epoch [8/8], Step [15/750], Loss: 0.0143\n",
      "Epoch [8/8], Step [16/750], Loss: 0.0366\n",
      "Epoch [8/8], Step [17/750], Loss: 0.0269\n",
      "Epoch [8/8], Step [18/750], Loss: 0.0045\n",
      "Epoch [8/8], Step [19/750], Loss: 0.0581\n",
      "Epoch [8/8], Step [20/750], Loss: 0.0334\n",
      "Epoch [8/8], Step [21/750], Loss: 0.0095\n",
      "Epoch [8/8], Step [22/750], Loss: 0.0010\n",
      "Epoch [8/8], Step [23/750], Loss: 0.0009\n",
      "Epoch [8/8], Step [24/750], Loss: 0.0076\n",
      "Epoch [8/8], Step [25/750], Loss: 0.0040\n",
      "Epoch [8/8], Step [26/750], Loss: 0.0765\n",
      "Epoch [8/8], Step [27/750], Loss: 0.0083\n",
      "Epoch [8/8], Step [28/750], Loss: 0.0500\n",
      "Epoch [8/8], Step [29/750], Loss: 0.0091\n",
      "Epoch [8/8], Step [30/750], Loss: 0.0084\n",
      "Epoch [8/8], Step [31/750], Loss: 0.0030\n",
      "Epoch [8/8], Step [32/750], Loss: 0.0681\n",
      "Epoch [8/8], Step [33/750], Loss: 0.0103\n",
      "Epoch [8/8], Step [34/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [35/750], Loss: 0.0014\n",
      "Epoch [8/8], Step [36/750], Loss: 0.0045\n",
      "Epoch [8/8], Step [37/750], Loss: 0.0765\n",
      "Epoch [8/8], Step [38/750], Loss: 0.0169\n",
      "Epoch [8/8], Step [39/750], Loss: 0.0034\n",
      "Epoch [8/8], Step [40/750], Loss: 0.0057\n",
      "Epoch [8/8], Step [41/750], Loss: 0.0156\n",
      "Epoch [8/8], Step [42/750], Loss: 0.0072\n",
      "Epoch [8/8], Step [43/750], Loss: 0.0024\n",
      "Epoch [8/8], Step [44/750], Loss: 0.0133\n",
      "Epoch [8/8], Step [45/750], Loss: 0.0789\n",
      "Epoch [8/8], Step [46/750], Loss: 0.0564\n",
      "Epoch [8/8], Step [47/750], Loss: 0.0966\n",
      "Epoch [8/8], Step [48/750], Loss: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Step [49/750], Loss: 0.0056\n",
      "Epoch [8/8], Step [50/750], Loss: 0.0735\n",
      "Epoch [8/8], Step [51/750], Loss: 0.0473\n",
      "Epoch [8/8], Step [52/750], Loss: 0.0226\n",
      "Epoch [8/8], Step [53/750], Loss: 0.0190\n",
      "Epoch [8/8], Step [54/750], Loss: 0.0111\n",
      "Epoch [8/8], Step [55/750], Loss: 0.0602\n",
      "Epoch [8/8], Step [56/750], Loss: 0.0020\n",
      "Epoch [8/8], Step [57/750], Loss: 0.0328\n",
      "Epoch [8/8], Step [58/750], Loss: 0.0053\n",
      "Epoch [8/8], Step [59/750], Loss: 0.0058\n",
      "Epoch [8/8], Step [60/750], Loss: 0.0015\n",
      "Epoch [8/8], Step [61/750], Loss: 0.0310\n",
      "Epoch [8/8], Step [62/750], Loss: 0.0043\n",
      "Epoch [8/8], Step [63/750], Loss: 0.0023\n",
      "Epoch [8/8], Step [64/750], Loss: 0.0583\n",
      "Epoch [8/8], Step [65/750], Loss: 0.0522\n",
      "Epoch [8/8], Step [66/750], Loss: 0.0720\n",
      "Epoch [8/8], Step [67/750], Loss: 0.0101\n",
      "Epoch [8/8], Step [68/750], Loss: 0.0036\n",
      "Epoch [8/8], Step [69/750], Loss: 0.0583\n",
      "Epoch [8/8], Step [70/750], Loss: 0.0146\n",
      "Epoch [8/8], Step [71/750], Loss: 0.0019\n",
      "Epoch [8/8], Step [72/750], Loss: 0.0172\n",
      "Epoch [8/8], Step [73/750], Loss: 0.0044\n",
      "Epoch [8/8], Step [74/750], Loss: 0.0583\n",
      "Epoch [8/8], Step [75/750], Loss: 0.0303\n",
      "Epoch [8/8], Step [76/750], Loss: 0.0170\n",
      "Epoch [8/8], Step [77/750], Loss: 0.0024\n",
      "Epoch [8/8], Step [78/750], Loss: 0.0831\n",
      "Epoch [8/8], Step [79/750], Loss: 0.0285\n",
      "Epoch [8/8], Step [80/750], Loss: 0.0042\n",
      "Epoch [8/8], Step [81/750], Loss: 0.0067\n",
      "Epoch [8/8], Step [82/750], Loss: 0.0079\n",
      "Epoch [8/8], Step [83/750], Loss: 0.0713\n",
      "Epoch [8/8], Step [84/750], Loss: 0.0095\n",
      "Epoch [8/8], Step [85/750], Loss: 0.0547\n",
      "Epoch [8/8], Step [86/750], Loss: 0.0221\n",
      "Epoch [8/8], Step [87/750], Loss: 0.0011\n",
      "Epoch [8/8], Step [88/750], Loss: 0.0297\n",
      "Epoch [8/8], Step [89/750], Loss: 0.0033\n",
      "Epoch [8/8], Step [90/750], Loss: 0.0054\n",
      "Epoch [8/8], Step [91/750], Loss: 0.0762\n",
      "Epoch [8/8], Step [92/750], Loss: 0.0176\n",
      "Epoch [8/8], Step [93/750], Loss: 0.0537\n",
      "Epoch [8/8], Step [94/750], Loss: 0.0135\n",
      "Epoch [8/8], Step [95/750], Loss: 0.0043\n",
      "Epoch [8/8], Step [96/750], Loss: 0.0054\n",
      "Epoch [8/8], Step [97/750], Loss: 0.0184\n",
      "Epoch [8/8], Step [98/750], Loss: 0.0205\n",
      "Epoch [8/8], Step [99/750], Loss: 0.1147\n",
      "Epoch [8/8], Step [100/750], Loss: 0.0212\n",
      "Epoch [8/8], Step [101/750], Loss: 0.0120\n",
      "Epoch [8/8], Step [102/750], Loss: 0.0059\n",
      "Epoch [8/8], Step [103/750], Loss: 0.0065\n",
      "Epoch [8/8], Step [104/750], Loss: 0.0082\n",
      "Epoch [8/8], Step [105/750], Loss: 0.0058\n",
      "Epoch [8/8], Step [106/750], Loss: 0.1614\n",
      "Epoch [8/8], Step [107/750], Loss: 0.1320\n",
      "Epoch [8/8], Step [108/750], Loss: 0.0087\n",
      "Epoch [8/8], Step [109/750], Loss: 0.0332\n",
      "Epoch [8/8], Step [110/750], Loss: 0.0102\n",
      "Epoch [8/8], Step [111/750], Loss: 0.0363\n",
      "Epoch [8/8], Step [112/750], Loss: 0.0096\n",
      "Epoch [8/8], Step [113/750], Loss: 0.0009\n",
      "Epoch [8/8], Step [114/750], Loss: 0.0071\n",
      "Epoch [8/8], Step [115/750], Loss: 0.0103\n",
      "Epoch [8/8], Step [116/750], Loss: 0.0422\n",
      "Epoch [8/8], Step [117/750], Loss: 0.0270\n",
      "Epoch [8/8], Step [118/750], Loss: 0.1335\n",
      "Epoch [8/8], Step [119/750], Loss: 0.0296\n",
      "Epoch [8/8], Step [120/750], Loss: 0.0825\n",
      "Epoch [8/8], Step [121/750], Loss: 0.0058\n",
      "Epoch [8/8], Step [122/750], Loss: 0.0133\n",
      "Epoch [8/8], Step [123/750], Loss: 0.0038\n",
      "Epoch [8/8], Step [124/750], Loss: 0.0555\n",
      "Epoch [8/8], Step [125/750], Loss: 0.0370\n",
      "Epoch [8/8], Step [126/750], Loss: 0.0565\n",
      "Epoch [8/8], Step [127/750], Loss: 0.0269\n",
      "Epoch [8/8], Step [128/750], Loss: 0.0068\n",
      "Epoch [8/8], Step [129/750], Loss: 0.0031\n",
      "Epoch [8/8], Step [130/750], Loss: 0.0060\n",
      "Epoch [8/8], Step [131/750], Loss: 0.0111\n",
      "Epoch [8/8], Step [132/750], Loss: 0.0315\n",
      "Epoch [8/8], Step [133/750], Loss: 0.0159\n",
      "Epoch [8/8], Step [134/750], Loss: 0.0028\n",
      "Epoch [8/8], Step [135/750], Loss: 0.0425\n",
      "Epoch [8/8], Step [136/750], Loss: 0.0505\n",
      "Epoch [8/8], Step [137/750], Loss: 0.0760\n",
      "Epoch [8/8], Step [138/750], Loss: 0.0447\n",
      "Epoch [8/8], Step [139/750], Loss: 0.0058\n",
      "Epoch [8/8], Step [140/750], Loss: 0.0039\n",
      "Epoch [8/8], Step [141/750], Loss: 0.0096\n",
      "Epoch [8/8], Step [142/750], Loss: 0.0392\n",
      "Epoch [8/8], Step [143/750], Loss: 0.0248\n",
      "Epoch [8/8], Step [144/750], Loss: 0.1224\n",
      "Epoch [8/8], Step [145/750], Loss: 0.0349\n",
      "Epoch [8/8], Step [146/750], Loss: 0.0017\n",
      "Epoch [8/8], Step [147/750], Loss: 0.0223\n",
      "Epoch [8/8], Step [148/750], Loss: 0.0206\n",
      "Epoch [8/8], Step [149/750], Loss: 0.0018\n",
      "Epoch [8/8], Step [150/750], Loss: 0.1262\n",
      "Epoch [8/8], Step [151/750], Loss: 0.0192\n",
      "Epoch [8/8], Step [152/750], Loss: 0.0058\n",
      "Epoch [8/8], Step [153/750], Loss: 0.0283\n",
      "Epoch [8/8], Step [154/750], Loss: 0.0070\n",
      "Epoch [8/8], Step [155/750], Loss: 0.0030\n",
      "Epoch [8/8], Step [156/750], Loss: 0.0444\n",
      "Epoch [8/8], Step [157/750], Loss: 0.0065\n",
      "Epoch [8/8], Step [158/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [159/750], Loss: 0.0150\n",
      "Epoch [8/8], Step [160/750], Loss: 0.0148\n",
      "Epoch [8/8], Step [161/750], Loss: 0.0466\n",
      "Epoch [8/8], Step [162/750], Loss: 0.0153\n",
      "Epoch [8/8], Step [163/750], Loss: 0.0237\n",
      "Epoch [8/8], Step [164/750], Loss: 0.0078\n",
      "Epoch [8/8], Step [165/750], Loss: 0.0237\n",
      "Epoch [8/8], Step [166/750], Loss: 0.0067\n",
      "Epoch [8/8], Step [167/750], Loss: 0.0030\n",
      "Epoch [8/8], Step [168/750], Loss: 0.0005\n",
      "Epoch [8/8], Step [169/750], Loss: 0.0144\n",
      "Epoch [8/8], Step [170/750], Loss: 0.0593\n",
      "Epoch [8/8], Step [171/750], Loss: 0.0048\n",
      "Epoch [8/8], Step [172/750], Loss: 0.0613\n",
      "Epoch [8/8], Step [173/750], Loss: 0.0643\n",
      "Epoch [8/8], Step [174/750], Loss: 0.0015\n",
      "Epoch [8/8], Step [175/750], Loss: 0.0037\n",
      "Epoch [8/8], Step [176/750], Loss: 0.0040\n",
      "Epoch [8/8], Step [177/750], Loss: 0.0112\n",
      "Epoch [8/8], Step [178/750], Loss: 0.0417\n",
      "Epoch [8/8], Step [179/750], Loss: 0.0366\n",
      "Epoch [8/8], Step [180/750], Loss: 0.0504\n",
      "Epoch [8/8], Step [181/750], Loss: 0.0020\n",
      "Epoch [8/8], Step [182/750], Loss: 0.0243\n",
      "Epoch [8/8], Step [183/750], Loss: 0.0044\n",
      "Epoch [8/8], Step [184/750], Loss: 0.0118\n",
      "Epoch [8/8], Step [185/750], Loss: 0.0335\n",
      "Epoch [8/8], Step [186/750], Loss: 0.0392\n",
      "Epoch [8/8], Step [187/750], Loss: 0.0010\n",
      "Epoch [8/8], Step [188/750], Loss: 0.0013\n",
      "Epoch [8/8], Step [189/750], Loss: 0.0026\n",
      "Epoch [8/8], Step [190/750], Loss: 0.0013\n",
      "Epoch [8/8], Step [191/750], Loss: 0.0548\n",
      "Epoch [8/8], Step [192/750], Loss: 0.0037\n",
      "Epoch [8/8], Step [193/750], Loss: 0.0025\n",
      "Epoch [8/8], Step [194/750], Loss: 0.0099\n",
      "Epoch [8/8], Step [195/750], Loss: 0.0032\n",
      "Epoch [8/8], Step [196/750], Loss: 0.0039\n",
      "Epoch [8/8], Step [197/750], Loss: 0.0010\n",
      "Epoch [8/8], Step [198/750], Loss: 0.0229\n",
      "Epoch [8/8], Step [199/750], Loss: 0.0010\n",
      "Epoch [8/8], Step [200/750], Loss: 0.0034\n",
      "Epoch [8/8], Step [201/750], Loss: 0.0107\n",
      "Epoch [8/8], Step [202/750], Loss: 0.0139\n",
      "Epoch [8/8], Step [203/750], Loss: 0.0027\n",
      "Epoch [8/8], Step [204/750], Loss: 0.0137\n",
      "Epoch [8/8], Step [205/750], Loss: 0.0101\n",
      "Epoch [8/8], Step [206/750], Loss: 0.0152\n",
      "Epoch [8/8], Step [207/750], Loss: 0.0927\n",
      "Epoch [8/8], Step [208/750], Loss: 0.0059\n",
      "Epoch [8/8], Step [209/750], Loss: 0.0083\n",
      "Epoch [8/8], Step [210/750], Loss: 0.0162\n",
      "Epoch [8/8], Step [211/750], Loss: 0.0164\n",
      "Epoch [8/8], Step [212/750], Loss: 0.0089\n",
      "Epoch [8/8], Step [213/750], Loss: 0.0050\n",
      "Epoch [8/8], Step [214/750], Loss: 0.0017\n",
      "Epoch [8/8], Step [215/750], Loss: 0.0012\n",
      "Epoch [8/8], Step [216/750], Loss: 0.0128\n",
      "Epoch [8/8], Step [217/750], Loss: 0.0020\n",
      "Epoch [8/8], Step [218/750], Loss: 0.0071\n",
      "Epoch [8/8], Step [219/750], Loss: 0.0114\n",
      "Epoch [8/8], Step [220/750], Loss: 0.0120\n",
      "Epoch [8/8], Step [221/750], Loss: 0.1023\n",
      "Epoch [8/8], Step [222/750], Loss: 0.0173\n",
      "Epoch [8/8], Step [223/750], Loss: 0.0591\n",
      "Epoch [8/8], Step [224/750], Loss: 0.0013\n",
      "Epoch [8/8], Step [225/750], Loss: 0.0480\n",
      "Epoch [8/8], Step [226/750], Loss: 0.0418\n",
      "Epoch [8/8], Step [227/750], Loss: 0.0146\n",
      "Epoch [8/8], Step [228/750], Loss: 0.0031\n",
      "Epoch [8/8], Step [229/750], Loss: 0.0124\n",
      "Epoch [8/8], Step [230/750], Loss: 0.0502\n",
      "Epoch [8/8], Step [231/750], Loss: 0.0458\n",
      "Epoch [8/8], Step [232/750], Loss: 0.0031\n",
      "Epoch [8/8], Step [233/750], Loss: 0.0505\n",
      "Epoch [8/8], Step [234/750], Loss: 0.0033\n",
      "Epoch [8/8], Step [235/750], Loss: 0.0381\n",
      "Epoch [8/8], Step [236/750], Loss: 0.0336\n",
      "Epoch [8/8], Step [237/750], Loss: 0.0049\n",
      "Epoch [8/8], Step [238/750], Loss: 0.0040\n",
      "Epoch [8/8], Step [239/750], Loss: 0.0209\n",
      "Epoch [8/8], Step [240/750], Loss: 0.0177\n",
      "Epoch [8/8], Step [241/750], Loss: 0.0069\n",
      "Epoch [8/8], Step [242/750], Loss: 0.0292\n",
      "Epoch [8/8], Step [243/750], Loss: 0.0037\n",
      "Epoch [8/8], Step [244/750], Loss: 0.0592\n",
      "Epoch [8/8], Step [245/750], Loss: 0.0026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Step [246/750], Loss: 0.0094\n",
      "Epoch [8/8], Step [247/750], Loss: 0.0100\n",
      "Epoch [8/8], Step [248/750], Loss: 0.0073\n",
      "Epoch [8/8], Step [249/750], Loss: 0.0234\n",
      "Epoch [8/8], Step [250/750], Loss: 0.0257\n",
      "Epoch [8/8], Step [251/750], Loss: 0.0241\n",
      "Epoch [8/8], Step [252/750], Loss: 0.0195\n",
      "Epoch [8/8], Step [253/750], Loss: 0.0932\n",
      "Epoch [8/8], Step [254/750], Loss: 0.0096\n",
      "Epoch [8/8], Step [255/750], Loss: 0.0366\n",
      "Epoch [8/8], Step [256/750], Loss: 0.0048\n",
      "Epoch [8/8], Step [257/750], Loss: 0.0062\n",
      "Epoch [8/8], Step [258/750], Loss: 0.0182\n",
      "Epoch [8/8], Step [259/750], Loss: 0.0142\n",
      "Epoch [8/8], Step [260/750], Loss: 0.0203\n",
      "Epoch [8/8], Step [261/750], Loss: 0.0063\n",
      "Epoch [8/8], Step [262/750], Loss: 0.0452\n",
      "Epoch [8/8], Step [263/750], Loss: 0.0475\n",
      "Epoch [8/8], Step [264/750], Loss: 0.0063\n",
      "Epoch [8/8], Step [265/750], Loss: 0.0053\n",
      "Epoch [8/8], Step [266/750], Loss: 0.0027\n",
      "Epoch [8/8], Step [267/750], Loss: 0.0081\n",
      "Epoch [8/8], Step [268/750], Loss: 0.0028\n",
      "Epoch [8/8], Step [269/750], Loss: 0.0072\n",
      "Epoch [8/8], Step [270/750], Loss: 0.0259\n",
      "Epoch [8/8], Step [271/750], Loss: 0.0060\n",
      "Epoch [8/8], Step [272/750], Loss: 0.1322\n",
      "Epoch [8/8], Step [273/750], Loss: 0.0020\n",
      "Epoch [8/8], Step [274/750], Loss: 0.0060\n",
      "Epoch [8/8], Step [275/750], Loss: 0.0163\n",
      "Epoch [8/8], Step [276/750], Loss: 0.0052\n",
      "Epoch [8/8], Step [277/750], Loss: 0.0026\n",
      "Epoch [8/8], Step [278/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [279/750], Loss: 0.0289\n",
      "Epoch [8/8], Step [280/750], Loss: 0.0050\n",
      "Epoch [8/8], Step [281/750], Loss: 0.1083\n",
      "Epoch [8/8], Step [282/750], Loss: 0.0381\n",
      "Epoch [8/8], Step [283/750], Loss: 0.0085\n",
      "Epoch [8/8], Step [284/750], Loss: 0.0049\n",
      "Epoch [8/8], Step [285/750], Loss: 0.0020\n",
      "Epoch [8/8], Step [286/750], Loss: 0.0009\n",
      "Epoch [8/8], Step [287/750], Loss: 0.0082\n",
      "Epoch [8/8], Step [288/750], Loss: 0.0058\n",
      "Epoch [8/8], Step [289/750], Loss: 0.0061\n",
      "Epoch [8/8], Step [290/750], Loss: 0.0205\n",
      "Epoch [8/8], Step [291/750], Loss: 0.0023\n",
      "Epoch [8/8], Step [292/750], Loss: 0.0006\n",
      "Epoch [8/8], Step [293/750], Loss: 0.0330\n",
      "Epoch [8/8], Step [294/750], Loss: 0.0019\n",
      "Epoch [8/8], Step [295/750], Loss: 0.0015\n",
      "Epoch [8/8], Step [296/750], Loss: 0.0065\n",
      "Epoch [8/8], Step [297/750], Loss: 0.0023\n",
      "Epoch [8/8], Step [298/750], Loss: 0.0022\n",
      "Epoch [8/8], Step [299/750], Loss: 0.0302\n",
      "Epoch [8/8], Step [300/750], Loss: 0.0243\n",
      "Epoch [8/8], Step [301/750], Loss: 0.0177\n",
      "Epoch [8/8], Step [302/750], Loss: 0.0524\n",
      "Epoch [8/8], Step [303/750], Loss: 0.0142\n",
      "Epoch [8/8], Step [304/750], Loss: 0.0074\n",
      "Epoch [8/8], Step [305/750], Loss: 0.0021\n",
      "Epoch [8/8], Step [306/750], Loss: 0.0176\n",
      "Epoch [8/8], Step [307/750], Loss: 0.0230\n",
      "Epoch [8/8], Step [308/750], Loss: 0.0107\n",
      "Epoch [8/8], Step [309/750], Loss: 0.0043\n",
      "Epoch [8/8], Step [310/750], Loss: 0.0088\n",
      "Epoch [8/8], Step [311/750], Loss: 0.0041\n",
      "Epoch [8/8], Step [312/750], Loss: 0.0034\n",
      "Epoch [8/8], Step [313/750], Loss: 0.0018\n",
      "Epoch [8/8], Step [314/750], Loss: 0.0059\n",
      "Epoch [8/8], Step [315/750], Loss: 0.0499\n",
      "Epoch [8/8], Step [316/750], Loss: 0.0095\n",
      "Epoch [8/8], Step [317/750], Loss: 0.0099\n",
      "Epoch [8/8], Step [318/750], Loss: 0.0028\n",
      "Epoch [8/8], Step [319/750], Loss: 0.0243\n",
      "Epoch [8/8], Step [320/750], Loss: 0.1214\n",
      "Epoch [8/8], Step [321/750], Loss: 0.0287\n",
      "Epoch [8/8], Step [322/750], Loss: 0.0205\n",
      "Epoch [8/8], Step [323/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [324/750], Loss: 0.0080\n",
      "Epoch [8/8], Step [325/750], Loss: 0.0354\n",
      "Epoch [8/8], Step [326/750], Loss: 0.0779\n",
      "Epoch [8/8], Step [327/750], Loss: 0.0063\n",
      "Epoch [8/8], Step [328/750], Loss: 0.0039\n",
      "Epoch [8/8], Step [329/750], Loss: 0.0230\n",
      "Epoch [8/8], Step [330/750], Loss: 0.0117\n",
      "Epoch [8/8], Step [331/750], Loss: 0.0104\n",
      "Epoch [8/8], Step [332/750], Loss: 0.0061\n",
      "Epoch [8/8], Step [333/750], Loss: 0.0431\n",
      "Epoch [8/8], Step [334/750], Loss: 0.0013\n",
      "Epoch [8/8], Step [335/750], Loss: 0.0448\n",
      "Epoch [8/8], Step [336/750], Loss: 0.0060\n",
      "Epoch [8/8], Step [337/750], Loss: 0.0053\n",
      "Epoch [8/8], Step [338/750], Loss: 0.0075\n",
      "Epoch [8/8], Step [339/750], Loss: 0.0415\n",
      "Epoch [8/8], Step [340/750], Loss: 0.0011\n",
      "Epoch [8/8], Step [341/750], Loss: 0.0055\n",
      "Epoch [8/8], Step [342/750], Loss: 0.0104\n",
      "Epoch [8/8], Step [343/750], Loss: 0.0059\n",
      "Epoch [8/8], Step [344/750], Loss: 0.0095\n",
      "Epoch [8/8], Step [345/750], Loss: 0.0021\n",
      "Epoch [8/8], Step [346/750], Loss: 0.1327\n",
      "Epoch [8/8], Step [347/750], Loss: 0.0245\n",
      "Epoch [8/8], Step [348/750], Loss: 0.0022\n",
      "Epoch [8/8], Step [349/750], Loss: 0.0180\n",
      "Epoch [8/8], Step [350/750], Loss: 0.0353\n",
      "Epoch [8/8], Step [351/750], Loss: 0.0225\n",
      "Epoch [8/8], Step [352/750], Loss: 0.0011\n",
      "Epoch [8/8], Step [353/750], Loss: 0.0018\n",
      "Epoch [8/8], Step [354/750], Loss: 0.0279\n",
      "Epoch [8/8], Step [355/750], Loss: 0.0261\n",
      "Epoch [8/8], Step [356/750], Loss: 0.0179\n",
      "Epoch [8/8], Step [357/750], Loss: 0.0882\n",
      "Epoch [8/8], Step [358/750], Loss: 0.0070\n",
      "Epoch [8/8], Step [359/750], Loss: 0.0376\n",
      "Epoch [8/8], Step [360/750], Loss: 0.0108\n",
      "Epoch [8/8], Step [361/750], Loss: 0.0212\n",
      "Epoch [8/8], Step [362/750], Loss: 0.0014\n",
      "Epoch [8/8], Step [363/750], Loss: 0.0130\n",
      "Epoch [8/8], Step [364/750], Loss: 0.0240\n",
      "Epoch [8/8], Step [365/750], Loss: 0.0248\n",
      "Epoch [8/8], Step [366/750], Loss: 0.0345\n",
      "Epoch [8/8], Step [367/750], Loss: 0.0377\n",
      "Epoch [8/8], Step [368/750], Loss: 0.0404\n",
      "Epoch [8/8], Step [369/750], Loss: 0.0264\n",
      "Epoch [8/8], Step [370/750], Loss: 0.0841\n",
      "Epoch [8/8], Step [371/750], Loss: 0.0095\n",
      "Epoch [8/8], Step [372/750], Loss: 0.0032\n",
      "Epoch [8/8], Step [373/750], Loss: 0.0398\n",
      "Epoch [8/8], Step [374/750], Loss: 0.0757\n",
      "Epoch [8/8], Step [375/750], Loss: 0.1408\n",
      "Epoch [8/8], Step [376/750], Loss: 0.0371\n",
      "Epoch [8/8], Step [377/750], Loss: 0.0303\n",
      "Epoch [8/8], Step [378/750], Loss: 0.0298\n",
      "Epoch [8/8], Step [379/750], Loss: 0.0489\n",
      "Epoch [8/8], Step [380/750], Loss: 0.0054\n",
      "Epoch [8/8], Step [381/750], Loss: 0.0048\n",
      "Epoch [8/8], Step [382/750], Loss: 0.0047\n",
      "Epoch [8/8], Step [383/750], Loss: 0.0514\n",
      "Epoch [8/8], Step [384/750], Loss: 0.0110\n",
      "Epoch [8/8], Step [385/750], Loss: 0.0388\n",
      "Epoch [8/8], Step [386/750], Loss: 0.0168\n",
      "Epoch [8/8], Step [387/750], Loss: 0.0027\n",
      "Epoch [8/8], Step [388/750], Loss: 0.1044\n",
      "Epoch [8/8], Step [389/750], Loss: 0.0919\n",
      "Epoch [8/8], Step [390/750], Loss: 0.0067\n",
      "Epoch [8/8], Step [391/750], Loss: 0.0513\n",
      "Epoch [8/8], Step [392/750], Loss: 0.0351\n",
      "Epoch [8/8], Step [393/750], Loss: 0.0013\n",
      "Epoch [8/8], Step [394/750], Loss: 0.0068\n",
      "Epoch [8/8], Step [395/750], Loss: 0.0032\n",
      "Epoch [8/8], Step [396/750], Loss: 0.0745\n",
      "Epoch [8/8], Step [397/750], Loss: 0.0085\n",
      "Epoch [8/8], Step [398/750], Loss: 0.0193\n",
      "Epoch [8/8], Step [399/750], Loss: 0.0067\n",
      "Epoch [8/8], Step [400/750], Loss: 0.0076\n",
      "Epoch [8/8], Step [401/750], Loss: 0.0049\n",
      "Epoch [8/8], Step [402/750], Loss: 0.0242\n",
      "Epoch [8/8], Step [403/750], Loss: 0.0354\n",
      "Epoch [8/8], Step [404/750], Loss: 0.0125\n",
      "Epoch [8/8], Step [405/750], Loss: 0.0252\n",
      "Epoch [8/8], Step [406/750], Loss: 0.1172\n",
      "Epoch [8/8], Step [407/750], Loss: 0.0155\n",
      "Epoch [8/8], Step [408/750], Loss: 0.1166\n",
      "Epoch [8/8], Step [409/750], Loss: 0.1157\n",
      "Epoch [8/8], Step [410/750], Loss: 0.0037\n",
      "Epoch [8/8], Step [411/750], Loss: 0.0018\n",
      "Epoch [8/8], Step [412/750], Loss: 0.1657\n",
      "Epoch [8/8], Step [413/750], Loss: 0.0036\n",
      "Epoch [8/8], Step [414/750], Loss: 0.0181\n",
      "Epoch [8/8], Step [415/750], Loss: 0.0146\n",
      "Epoch [8/8], Step [416/750], Loss: 0.0150\n",
      "Epoch [8/8], Step [417/750], Loss: 0.0139\n",
      "Epoch [8/8], Step [418/750], Loss: 0.0048\n",
      "Epoch [8/8], Step [419/750], Loss: 0.0645\n",
      "Epoch [8/8], Step [420/750], Loss: 0.0104\n",
      "Epoch [8/8], Step [421/750], Loss: 0.0212\n",
      "Epoch [8/8], Step [422/750], Loss: 0.0148\n",
      "Epoch [8/8], Step [423/750], Loss: 0.0400\n",
      "Epoch [8/8], Step [424/750], Loss: 0.0021\n",
      "Epoch [8/8], Step [425/750], Loss: 0.0232\n",
      "Epoch [8/8], Step [426/750], Loss: 0.0222\n",
      "Epoch [8/8], Step [427/750], Loss: 0.0027\n",
      "Epoch [8/8], Step [428/750], Loss: 0.0013\n",
      "Epoch [8/8], Step [429/750], Loss: 0.0438\n",
      "Epoch [8/8], Step [430/750], Loss: 0.0091\n",
      "Epoch [8/8], Step [431/750], Loss: 0.0382\n",
      "Epoch [8/8], Step [432/750], Loss: 0.0793\n",
      "Epoch [8/8], Step [433/750], Loss: 0.0025\n",
      "Epoch [8/8], Step [434/750], Loss: 0.0035\n",
      "Epoch [8/8], Step [435/750], Loss: 0.0024\n",
      "Epoch [8/8], Step [436/750], Loss: 0.0024\n",
      "Epoch [8/8], Step [437/750], Loss: 0.0008\n",
      "Epoch [8/8], Step [438/750], Loss: 0.0931\n",
      "Epoch [8/8], Step [439/750], Loss: 0.0046\n",
      "Epoch [8/8], Step [440/750], Loss: 0.0492\n",
      "Epoch [8/8], Step [441/750], Loss: 0.0024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Step [442/750], Loss: 0.0015\n",
      "Epoch [8/8], Step [443/750], Loss: 0.0173\n",
      "Epoch [8/8], Step [444/750], Loss: 0.0022\n",
      "Epoch [8/8], Step [445/750], Loss: 0.0218\n",
      "Epoch [8/8], Step [446/750], Loss: 0.0527\n",
      "Epoch [8/8], Step [447/750], Loss: 0.0218\n",
      "Epoch [8/8], Step [448/750], Loss: 0.0156\n",
      "Epoch [8/8], Step [449/750], Loss: 0.1149\n",
      "Epoch [8/8], Step [450/750], Loss: 0.0361\n",
      "Epoch [8/8], Step [451/750], Loss: 0.0086\n",
      "Epoch [8/8], Step [452/750], Loss: 0.0267\n",
      "Epoch [8/8], Step [453/750], Loss: 0.0287\n",
      "Epoch [8/8], Step [454/750], Loss: 0.0272\n",
      "Epoch [8/8], Step [455/750], Loss: 0.0013\n",
      "Epoch [8/8], Step [456/750], Loss: 0.0105\n",
      "Epoch [8/8], Step [457/750], Loss: 0.0222\n",
      "Epoch [8/8], Step [458/750], Loss: 0.1350\n",
      "Epoch [8/8], Step [459/750], Loss: 0.0167\n",
      "Epoch [8/8], Step [460/750], Loss: 0.0824\n",
      "Epoch [8/8], Step [461/750], Loss: 0.0028\n",
      "Epoch [8/8], Step [462/750], Loss: 0.0600\n",
      "Epoch [8/8], Step [463/750], Loss: 0.0013\n",
      "Epoch [8/8], Step [464/750], Loss: 0.0194\n",
      "Epoch [8/8], Step [465/750], Loss: 0.0418\n",
      "Epoch [8/8], Step [466/750], Loss: 0.0430\n",
      "Epoch [8/8], Step [467/750], Loss: 0.0209\n",
      "Epoch [8/8], Step [468/750], Loss: 0.0642\n",
      "Epoch [8/8], Step [469/750], Loss: 0.0156\n",
      "Epoch [8/8], Step [470/750], Loss: 0.0509\n",
      "Epoch [8/8], Step [471/750], Loss: 0.0330\n",
      "Epoch [8/8], Step [472/750], Loss: 0.1642\n",
      "Epoch [8/8], Step [473/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [474/750], Loss: 0.0869\n",
      "Epoch [8/8], Step [475/750], Loss: 0.0621\n",
      "Epoch [8/8], Step [476/750], Loss: 0.0217\n",
      "Epoch [8/8], Step [477/750], Loss: 0.0747\n",
      "Epoch [8/8], Step [478/750], Loss: 0.0189\n",
      "Epoch [8/8], Step [479/750], Loss: 0.0034\n",
      "Epoch [8/8], Step [480/750], Loss: 0.0523\n",
      "Epoch [8/8], Step [481/750], Loss: 0.0558\n",
      "Epoch [8/8], Step [482/750], Loss: 0.0128\n",
      "Epoch [8/8], Step [483/750], Loss: 0.0402\n",
      "Epoch [8/8], Step [484/750], Loss: 0.0101\n",
      "Epoch [8/8], Step [485/750], Loss: 0.0231\n",
      "Epoch [8/8], Step [486/750], Loss: 0.0212\n",
      "Epoch [8/8], Step [487/750], Loss: 0.0069\n",
      "Epoch [8/8], Step [488/750], Loss: 0.0154\n",
      "Epoch [8/8], Step [489/750], Loss: 0.0129\n",
      "Epoch [8/8], Step [490/750], Loss: 0.0327\n",
      "Epoch [8/8], Step [491/750], Loss: 0.0160\n",
      "Epoch [8/8], Step [492/750], Loss: 0.0034\n",
      "Epoch [8/8], Step [493/750], Loss: 0.0081\n",
      "Epoch [8/8], Step [494/750], Loss: 0.0149\n",
      "Epoch [8/8], Step [495/750], Loss: 0.0039\n",
      "Epoch [8/8], Step [496/750], Loss: 0.0208\n",
      "Epoch [8/8], Step [497/750], Loss: 0.0156\n",
      "Epoch [8/8], Step [498/750], Loss: 0.0718\n",
      "Epoch [8/8], Step [499/750], Loss: 0.0015\n",
      "Epoch [8/8], Step [500/750], Loss: 0.0051\n",
      "Epoch [8/8], Step [501/750], Loss: 0.0049\n",
      "Epoch [8/8], Step [502/750], Loss: 0.0164\n",
      "Epoch [8/8], Step [503/750], Loss: 0.1086\n",
      "Epoch [8/8], Step [504/750], Loss: 0.0047\n",
      "Epoch [8/8], Step [505/750], Loss: 0.0805\n",
      "Epoch [8/8], Step [506/750], Loss: 0.0093\n",
      "Epoch [8/8], Step [507/750], Loss: 0.0210\n",
      "Epoch [8/8], Step [508/750], Loss: 0.0150\n",
      "Epoch [8/8], Step [509/750], Loss: 0.0105\n",
      "Epoch [8/8], Step [510/750], Loss: 0.0958\n",
      "Epoch [8/8], Step [511/750], Loss: 0.0056\n",
      "Epoch [8/8], Step [512/750], Loss: 0.0487\n",
      "Epoch [8/8], Step [513/750], Loss: 0.0119\n",
      "Epoch [8/8], Step [514/750], Loss: 0.0086\n",
      "Epoch [8/8], Step [515/750], Loss: 0.0563\n",
      "Epoch [8/8], Step [516/750], Loss: 0.0100\n",
      "Epoch [8/8], Step [517/750], Loss: 0.0268\n",
      "Epoch [8/8], Step [518/750], Loss: 0.0043\n",
      "Epoch [8/8], Step [519/750], Loss: 0.0147\n",
      "Epoch [8/8], Step [520/750], Loss: 0.0166\n",
      "Epoch [8/8], Step [521/750], Loss: 0.0048\n",
      "Epoch [8/8], Step [522/750], Loss: 0.0805\n",
      "Epoch [8/8], Step [523/750], Loss: 0.0196\n",
      "Epoch [8/8], Step [524/750], Loss: 0.0234\n",
      "Epoch [8/8], Step [525/750], Loss: 0.0308\n",
      "Epoch [8/8], Step [526/750], Loss: 0.0117\n",
      "Epoch [8/8], Step [527/750], Loss: 0.0185\n",
      "Epoch [8/8], Step [528/750], Loss: 0.0462\n",
      "Epoch [8/8], Step [529/750], Loss: 0.0098\n",
      "Epoch [8/8], Step [530/750], Loss: 0.0044\n",
      "Epoch [8/8], Step [531/750], Loss: 0.0060\n",
      "Epoch [8/8], Step [532/750], Loss: 0.0110\n",
      "Epoch [8/8], Step [533/750], Loss: 0.0034\n",
      "Epoch [8/8], Step [534/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [535/750], Loss: 0.0096\n",
      "Epoch [8/8], Step [536/750], Loss: 0.1102\n",
      "Epoch [8/8], Step [537/750], Loss: 0.0144\n",
      "Epoch [8/8], Step [538/750], Loss: 0.0015\n",
      "Epoch [8/8], Step [539/750], Loss: 0.0046\n",
      "Epoch [8/8], Step [540/750], Loss: 0.0582\n",
      "Epoch [8/8], Step [541/750], Loss: 0.0935\n",
      "Epoch [8/8], Step [542/750], Loss: 0.0363\n",
      "Epoch [8/8], Step [543/750], Loss: 0.0273\n",
      "Epoch [8/8], Step [544/750], Loss: 0.0679\n",
      "Epoch [8/8], Step [545/750], Loss: 0.0034\n",
      "Epoch [8/8], Step [546/750], Loss: 0.0021\n",
      "Epoch [8/8], Step [547/750], Loss: 0.0731\n",
      "Epoch [8/8], Step [548/750], Loss: 0.0325\n",
      "Epoch [8/8], Step [549/750], Loss: 0.0295\n",
      "Epoch [8/8], Step [550/750], Loss: 0.0168\n",
      "Epoch [8/8], Step [551/750], Loss: 0.0072\n",
      "Epoch [8/8], Step [552/750], Loss: 0.0170\n",
      "Epoch [8/8], Step [553/750], Loss: 0.0048\n",
      "Epoch [8/8], Step [554/750], Loss: 0.0734\n",
      "Epoch [8/8], Step [555/750], Loss: 0.0866\n",
      "Epoch [8/8], Step [556/750], Loss: 0.0109\n",
      "Epoch [8/8], Step [557/750], Loss: 0.0126\n",
      "Epoch [8/8], Step [558/750], Loss: 0.0707\n",
      "Epoch [8/8], Step [559/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [560/750], Loss: 0.0560\n",
      "Epoch [8/8], Step [561/750], Loss: 0.0384\n",
      "Epoch [8/8], Step [562/750], Loss: 0.0444\n",
      "Epoch [8/8], Step [563/750], Loss: 0.0063\n",
      "Epoch [8/8], Step [564/750], Loss: 0.0099\n",
      "Epoch [8/8], Step [565/750], Loss: 0.0102\n",
      "Epoch [8/8], Step [566/750], Loss: 0.0163\n",
      "Epoch [8/8], Step [567/750], Loss: 0.0028\n",
      "Epoch [8/8], Step [568/750], Loss: 0.0021\n",
      "Epoch [8/8], Step [569/750], Loss: 0.1432\n",
      "Epoch [8/8], Step [570/750], Loss: 0.0223\n",
      "Epoch [8/8], Step [571/750], Loss: 0.0511\n",
      "Epoch [8/8], Step [572/750], Loss: 0.0152\n",
      "Epoch [8/8], Step [573/750], Loss: 0.0064\n",
      "Epoch [8/8], Step [574/750], Loss: 0.0318\n",
      "Epoch [8/8], Step [575/750], Loss: 0.0025\n",
      "Epoch [8/8], Step [576/750], Loss: 0.0025\n",
      "Epoch [8/8], Step [577/750], Loss: 0.0588\n",
      "Epoch [8/8], Step [578/750], Loss: 0.0125\n",
      "Epoch [8/8], Step [579/750], Loss: 0.0006\n",
      "Epoch [8/8], Step [580/750], Loss: 0.0522\n",
      "Epoch [8/8], Step [581/750], Loss: 0.0074\n",
      "Epoch [8/8], Step [582/750], Loss: 0.0067\n",
      "Epoch [8/8], Step [583/750], Loss: 0.0068\n",
      "Epoch [8/8], Step [584/750], Loss: 0.0410\n",
      "Epoch [8/8], Step [585/750], Loss: 0.0683\n",
      "Epoch [8/8], Step [586/750], Loss: 0.0091\n",
      "Epoch [8/8], Step [587/750], Loss: 0.0854\n",
      "Epoch [8/8], Step [588/750], Loss: 0.0018\n",
      "Epoch [8/8], Step [589/750], Loss: 0.0512\n",
      "Epoch [8/8], Step [590/750], Loss: 0.0015\n",
      "Epoch [8/8], Step [591/750], Loss: 0.0094\n",
      "Epoch [8/8], Step [592/750], Loss: 0.0082\n",
      "Epoch [8/8], Step [593/750], Loss: 0.0053\n",
      "Epoch [8/8], Step [594/750], Loss: 0.0540\n",
      "Epoch [8/8], Step [595/750], Loss: 0.0361\n",
      "Epoch [8/8], Step [596/750], Loss: 0.0036\n",
      "Epoch [8/8], Step [597/750], Loss: 0.0339\n",
      "Epoch [8/8], Step [598/750], Loss: 0.0304\n",
      "Epoch [8/8], Step [599/750], Loss: 0.0758\n",
      "Epoch [8/8], Step [600/750], Loss: 0.0046\n",
      "Epoch [8/8], Step [601/750], Loss: 0.0019\n",
      "Epoch [8/8], Step [602/750], Loss: 0.0841\n",
      "Epoch [8/8], Step [603/750], Loss: 0.0113\n",
      "Epoch [8/8], Step [604/750], Loss: 0.0071\n",
      "Epoch [8/8], Step [605/750], Loss: 0.0047\n",
      "Epoch [8/8], Step [606/750], Loss: 0.0042\n",
      "Epoch [8/8], Step [607/750], Loss: 0.0923\n",
      "Epoch [8/8], Step [608/750], Loss: 0.0954\n",
      "Epoch [8/8], Step [609/750], Loss: 0.0018\n",
      "Epoch [8/8], Step [610/750], Loss: 0.0141\n",
      "Epoch [8/8], Step [611/750], Loss: 0.0079\n",
      "Epoch [8/8], Step [612/750], Loss: 0.1715\n",
      "Epoch [8/8], Step [613/750], Loss: 0.0461\n",
      "Epoch [8/8], Step [614/750], Loss: 0.0066\n",
      "Epoch [8/8], Step [615/750], Loss: 0.0036\n",
      "Epoch [8/8], Step [616/750], Loss: 0.0514\n",
      "Epoch [8/8], Step [617/750], Loss: 0.0263\n",
      "Epoch [8/8], Step [618/750], Loss: 0.0059\n",
      "Epoch [8/8], Step [619/750], Loss: 0.0401\n",
      "Epoch [8/8], Step [620/750], Loss: 0.1155\n",
      "Epoch [8/8], Step [621/750], Loss: 0.0060\n",
      "Epoch [8/8], Step [622/750], Loss: 0.0134\n",
      "Epoch [8/8], Step [623/750], Loss: 0.0137\n",
      "Epoch [8/8], Step [624/750], Loss: 0.0013\n",
      "Epoch [8/8], Step [625/750], Loss: 0.0050\n",
      "Epoch [8/8], Step [626/750], Loss: 0.1609\n",
      "Epoch [8/8], Step [627/750], Loss: 0.0140\n",
      "Epoch [8/8], Step [628/750], Loss: 0.0177\n",
      "Epoch [8/8], Step [629/750], Loss: 0.0082\n",
      "Epoch [8/8], Step [630/750], Loss: 0.0491\n",
      "Epoch [8/8], Step [631/750], Loss: 0.0756\n",
      "Epoch [8/8], Step [632/750], Loss: 0.0062\n",
      "Epoch [8/8], Step [633/750], Loss: 0.0374\n",
      "Epoch [8/8], Step [634/750], Loss: 0.0114\n",
      "Epoch [8/8], Step [635/750], Loss: 0.0055\n",
      "Epoch [8/8], Step [636/750], Loss: 0.0611\n",
      "Epoch [8/8], Step [637/750], Loss: 0.0383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Step [638/750], Loss: 0.0029\n",
      "Epoch [8/8], Step [639/750], Loss: 0.0126\n",
      "Epoch [8/8], Step [640/750], Loss: 0.0029\n",
      "Epoch [8/8], Step [641/750], Loss: 0.0133\n",
      "Epoch [8/8], Step [642/750], Loss: 0.0029\n",
      "Epoch [8/8], Step [643/750], Loss: 0.0461\n",
      "Epoch [8/8], Step [644/750], Loss: 0.0024\n",
      "Epoch [8/8], Step [645/750], Loss: 0.0074\n",
      "Epoch [8/8], Step [646/750], Loss: 0.0135\n",
      "Epoch [8/8], Step [647/750], Loss: 0.0157\n",
      "Epoch [8/8], Step [648/750], Loss: 0.0144\n",
      "Epoch [8/8], Step [649/750], Loss: 0.0538\n",
      "Epoch [8/8], Step [650/750], Loss: 0.0014\n",
      "Epoch [8/8], Step [651/750], Loss: 0.0119\n",
      "Epoch [8/8], Step [652/750], Loss: 0.0650\n",
      "Epoch [8/8], Step [653/750], Loss: 0.0052\n",
      "Epoch [8/8], Step [654/750], Loss: 0.0039\n",
      "Epoch [8/8], Step [655/750], Loss: 0.0025\n",
      "Epoch [8/8], Step [656/750], Loss: 0.0587\n",
      "Epoch [8/8], Step [657/750], Loss: 0.0712\n",
      "Epoch [8/8], Step [658/750], Loss: 0.0278\n",
      "Epoch [8/8], Step [659/750], Loss: 0.1219\n",
      "Epoch [8/8], Step [660/750], Loss: 0.0391\n",
      "Epoch [8/8], Step [661/750], Loss: 0.0733\n",
      "Epoch [8/8], Step [662/750], Loss: 0.0078\n",
      "Epoch [8/8], Step [663/750], Loss: 0.0146\n",
      "Epoch [8/8], Step [664/750], Loss: 0.0116\n",
      "Epoch [8/8], Step [665/750], Loss: 0.0644\n",
      "Epoch [8/8], Step [666/750], Loss: 0.0210\n",
      "Epoch [8/8], Step [667/750], Loss: 0.0429\n",
      "Epoch [8/8], Step [668/750], Loss: 0.0605\n",
      "Epoch [8/8], Step [669/750], Loss: 0.0258\n",
      "Epoch [8/8], Step [670/750], Loss: 0.0014\n",
      "Epoch [8/8], Step [671/750], Loss: 0.0037\n",
      "Epoch [8/8], Step [672/750], Loss: 0.0055\n",
      "Epoch [8/8], Step [673/750], Loss: 0.0309\n",
      "Epoch [8/8], Step [674/750], Loss: 0.0240\n",
      "Epoch [8/8], Step [675/750], Loss: 0.0803\n",
      "Epoch [8/8], Step [676/750], Loss: 0.0086\n",
      "Epoch [8/8], Step [677/750], Loss: 0.0246\n",
      "Epoch [8/8], Step [678/750], Loss: 0.0458\n",
      "Epoch [8/8], Step [679/750], Loss: 0.0628\n",
      "Epoch [8/8], Step [680/750], Loss: 0.0046\n",
      "Epoch [8/8], Step [681/750], Loss: 0.0138\n",
      "Epoch [8/8], Step [682/750], Loss: 0.0219\n",
      "Epoch [8/8], Step [683/750], Loss: 0.0150\n",
      "Epoch [8/8], Step [684/750], Loss: 0.0017\n",
      "Epoch [8/8], Step [685/750], Loss: 0.0155\n",
      "Epoch [8/8], Step [686/750], Loss: 0.0061\n",
      "Epoch [8/8], Step [687/750], Loss: 0.0838\n",
      "Epoch [8/8], Step [688/750], Loss: 0.0754\n",
      "Epoch [8/8], Step [689/750], Loss: 0.0688\n",
      "Epoch [8/8], Step [690/750], Loss: 0.0193\n",
      "Epoch [8/8], Step [691/750], Loss: 0.0443\n",
      "Epoch [8/8], Step [692/750], Loss: 0.0433\n",
      "Epoch [8/8], Step [693/750], Loss: 0.0202\n",
      "Epoch [8/8], Step [694/750], Loss: 0.0264\n",
      "Epoch [8/8], Step [695/750], Loss: 0.0062\n",
      "Epoch [8/8], Step [696/750], Loss: 0.0524\n",
      "Epoch [8/8], Step [697/750], Loss: 0.0106\n",
      "Epoch [8/8], Step [698/750], Loss: 0.0151\n",
      "Epoch [8/8], Step [699/750], Loss: 0.1807\n",
      "Epoch [8/8], Step [700/750], Loss: 0.0127\n",
      "Epoch [8/8], Step [701/750], Loss: 0.0937\n",
      "Epoch [8/8], Step [702/750], Loss: 0.0033\n",
      "Epoch [8/8], Step [703/750], Loss: 0.0214\n",
      "Epoch [8/8], Step [704/750], Loss: 0.0056\n",
      "Epoch [8/8], Step [705/750], Loss: 0.0070\n",
      "Epoch [8/8], Step [706/750], Loss: 0.0220\n",
      "Epoch [8/8], Step [707/750], Loss: 0.0511\n",
      "Epoch [8/8], Step [708/750], Loss: 0.0597\n",
      "Epoch [8/8], Step [709/750], Loss: 0.0023\n",
      "Epoch [8/8], Step [710/750], Loss: 0.0154\n",
      "Epoch [8/8], Step [711/750], Loss: 0.0626\n",
      "Epoch [8/8], Step [712/750], Loss: 0.0082\n",
      "Epoch [8/8], Step [713/750], Loss: 0.1168\n",
      "Epoch [8/8], Step [714/750], Loss: 0.0122\n",
      "Epoch [8/8], Step [715/750], Loss: 0.0202\n",
      "Epoch [8/8], Step [716/750], Loss: 0.0158\n",
      "Epoch [8/8], Step [717/750], Loss: 0.0189\n",
      "Epoch [8/8], Step [718/750], Loss: 0.0081\n",
      "Epoch [8/8], Step [719/750], Loss: 0.0638\n",
      "Epoch [8/8], Step [720/750], Loss: 0.0068\n",
      "Epoch [8/8], Step [721/750], Loss: 0.0725\n",
      "Epoch [8/8], Step [722/750], Loss: 0.0271\n",
      "Epoch [8/8], Step [723/750], Loss: 0.0023\n",
      "Epoch [8/8], Step [724/750], Loss: 0.0084\n",
      "Epoch [8/8], Step [725/750], Loss: 0.0628\n",
      "Epoch [8/8], Step [726/750], Loss: 0.0137\n",
      "Epoch [8/8], Step [727/750], Loss: 0.0277\n",
      "Epoch [8/8], Step [728/750], Loss: 0.0219\n",
      "Epoch [8/8], Step [729/750], Loss: 0.0583\n",
      "Epoch [8/8], Step [730/750], Loss: 0.0033\n",
      "Epoch [8/8], Step [731/750], Loss: 0.0211\n",
      "Epoch [8/8], Step [732/750], Loss: 0.0127\n",
      "Epoch [8/8], Step [733/750], Loss: 0.0035\n",
      "Epoch [8/8], Step [734/750], Loss: 0.0243\n",
      "Epoch [8/8], Step [735/750], Loss: 0.0757\n",
      "Epoch [8/8], Step [736/750], Loss: 0.0328\n",
      "Epoch [8/8], Step [737/750], Loss: 0.0044\n",
      "Epoch [8/8], Step [738/750], Loss: 0.0454\n",
      "Epoch [8/8], Step [739/750], Loss: 0.0053\n",
      "Epoch [8/8], Step [740/750], Loss: 0.0152\n",
      "Epoch [8/8], Step [741/750], Loss: 0.0698\n",
      "Epoch [8/8], Step [742/750], Loss: 0.0009\n",
      "Epoch [8/8], Step [743/750], Loss: 0.0035\n",
      "Epoch [8/8], Step [744/750], Loss: 0.0020\n",
      "Epoch [8/8], Step [745/750], Loss: 0.0051\n",
      "Epoch [8/8], Step [746/750], Loss: 0.0121\n",
      "Epoch [8/8], Step [747/750], Loss: 0.0594\n",
      "Epoch [8/8], Step [748/750], Loss: 0.1712\n",
      "Epoch [8/8], Step [749/750], Loss: 0.0029\n",
      "Epoch [8/8], Step [750/750], Loss: 0.0040\n",
      "Epoch [8/8], Tr. loss: 0.4236. Test loss: 0.3225\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "\n",
    "history = train(num_epochs, hnn, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1ed953a",
   "metadata": {
    "id": "f1ed953a"
   },
   "outputs": [],
   "source": [
    "torch.save(hnn.state_dict(), 'trained_model_24qubits_3_layers_with_strong_entagling.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "263d8d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt', 'w') as outfile:\n",
    "    json.dump(history, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "312a7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "def draw_plot(history):\n",
    "    loss, val_loss = zip(*history)\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    for i in range(len(loss)):\n",
    "        losses.append(loss[i])\n",
    "        val_losses.append(val_loss[i])\n",
    "        for j in range(i):\n",
    "            losses[i] -= losses[j]\n",
    "            val_losses[i] -= val_losses[j]\n",
    "            \n",
    "    plt.figure(figsize=(15, 9))\n",
    "    plt.plot(losses, label=\"train_loss\")\n",
    "    plt.plot(val_losses, label=\"val_loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d100a494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44f71add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        accuracy = 0\n",
    "        for i, (images, labels) in enumerate(loaders['test']):\n",
    "            print(i)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            test_output = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1]\n",
    "            accuracy += (pred_y == labels).sum().item() \n",
    "        accuracy /= len(loaders['test'].dataset)\n",
    "        print('Test Accuracy of the model on the 10000 test images: {:.2f} %'.format(accuracy*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44383839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "Test Accuracy of the model on the 10000 test images: 99.02 %\n"
     ]
    }
   ],
   "source": [
    "test(hnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959277fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fully_connected_with_qnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00cd50cc19624ebf946c288dcf96ca77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a0c3aa1494d454b85ce6083f2c98234": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ca85bfb648a4586ab15f601f45bd7c2",
      "placeholder": "​",
      "style": "IPY_MODEL_979724284c094071b6c3143666a2a8d5",
      "value": " 0/10 [00:22&lt;?, ?it/s]"
     }
    },
    "0aa6d7245fc747b58457668a840cd66b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0de84b8c5dde45b68b6fe495e3978524": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1784bfcf80e54eb1a3b7873cd36494b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fb4ea8439054fbeaab29cbbdd8d8c62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2454c06e04cc44a59af6a54edf771dcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d31e57f44fef4cd99cbda543609a5363",
      "placeholder": "​",
      "style": "IPY_MODEL_f1cb6810b0f0425c925092fd2c564460",
      "value": " 29696/? [00:00&lt;00:00, 779622.51it/s]"
     }
    },
    "2bae88a771624fce8a7195ab2d3790e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3629620f44dc4eec928e8285f4e292c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c55b6cc69fe4e71bfefdfafe172e1f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42c3a0dfb96d4d0a9cbcfbefdb88634a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b7381bd873134277869b9bf86f7f6cde",
       "IPY_MODEL_92a84a7a4c1a49fca9c649540174de1a",
       "IPY_MODEL_973a9057772b4a33bebe1c71b2548322"
      ],
      "layout": "IPY_MODEL_fb3c5d28cc434035936450f03eaab7d4"
     }
    },
    "4472460dcaf349e282e8505097ac56db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b9f9045a6a64724b75bed3bdf4825e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d29ed3d913f4c3dadd8aa28ffdf60ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fb4ea8439054fbeaab29cbbdd8d8c62",
      "placeholder": "​",
      "style": "IPY_MODEL_7598f22b00ca4188abd0e988d87c8717",
      "value": ""
     }
    },
    "4ee572a2f86844b38f1475756d831d7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53264b907c324fd2903f6a33df4024eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3629620f44dc4eec928e8285f4e292c7",
      "placeholder": "​",
      "style": "IPY_MODEL_ca79710ed42844eda53edd54ad1cce9e",
      "value": ""
     }
    },
    "585d33ea2f6c4c6f866ab5eaf500a486": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4472460dcaf349e282e8505097ac56db",
      "placeholder": "​",
      "style": "IPY_MODEL_00cd50cc19624ebf946c288dcf96ca77",
      "value": " 5120/? [00:00&lt;00:00, 149206.45it/s]"
     }
    },
    "5f557bc2ab954fcdbb3145c5ae1cd1fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_831588c036b84ba5919552e1c0ffaf95",
      "placeholder": "​",
      "style": "IPY_MODEL_4b9f9045a6a64724b75bed3bdf4825e5",
      "value": " 9913344/? [00:00&lt;00:00, 18038716.63it/s]"
     }
    },
    "611e4230f7284f56a36e03b6cab128bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "656d789dd50d4d7c95121ee911a0315e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "664e26cb5e91441d8360e652dc6f3f13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbeac195f2cf4776b6c95dedda39513c",
      "placeholder": "​",
      "style": "IPY_MODEL_611e4230f7284f56a36e03b6cab128bb",
      "value": ""
     }
    },
    "70629c93466c490cb4d10c7ba2e41d62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7081d0216eb6473fae87f7331becfb53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7192858941cb47649969779dc0e63c91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_664e26cb5e91441d8360e652dc6f3f13",
       "IPY_MODEL_ba90ae57f0bf4101b5306b32401f5ca6",
       "IPY_MODEL_585d33ea2f6c4c6f866ab5eaf500a486"
      ],
      "layout": "IPY_MODEL_0de84b8c5dde45b68b6fe495e3978524"
     }
    },
    "739c6da6cd64446686e87fd499fc4e8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1784bfcf80e54eb1a3b7873cd36494b1",
      "max": 28881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae7d8308ff8541d082c143732f92bce6",
      "value": 28881
     }
    },
    "7598f22b00ca4188abd0e988d87c8717": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ca85bfb648a4586ab15f601f45bd7c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "831588c036b84ba5919552e1c0ffaf95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b99aefdd9a247678de2cefd312207bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7081d0216eb6473fae87f7331becfb53",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70629c93466c490cb4d10c7ba2e41d62",
      "value": 0
     }
    },
    "9002f4b1af6d4c03b8f53fecdffaa3b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92a84a7a4c1a49fca9c649540174de1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bae88a771624fce8a7195ab2d3790e1",
      "max": 1648877,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a5767d4185754f1f82e22dc55b9b659e",
      "value": 1648877
     }
    },
    "973a9057772b4a33bebe1c71b2548322": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bcaf3aab2df24777bb896394049144d2",
      "placeholder": "​",
      "style": "IPY_MODEL_656d789dd50d4d7c95121ee911a0315e",
      "value": " 1649664/? [00:00&lt;00:00, 5358482.30it/s]"
     }
    },
    "979724284c094071b6c3143666a2a8d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a0d229daebc428d8d0bea6db980845f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2b522fb50d54cecaead8906ecc2c88f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2db74a18ce8412399aba11b3bab930b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa56b1689edb4a75aa4af965407467e6",
       "IPY_MODEL_8b99aefdd9a247678de2cefd312207bb",
       "IPY_MODEL_0a0c3aa1494d454b85ce6083f2c98234"
      ],
      "layout": "IPY_MODEL_9002f4b1af6d4c03b8f53fecdffaa3b0"
     }
    },
    "a5767d4185754f1f82e22dc55b9b659e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aa56b1689edb4a75aa4af965407467e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df312d8125694913844f2856ee295e67",
      "placeholder": "​",
      "style": "IPY_MODEL_0aa6d7245fc747b58457668a840cd66b",
      "value": "  0%"
     }
    },
    "ae7d8308ff8541d082c143732f92bce6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b7381bd873134277869b9bf86f7f6cde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ee572a2f86844b38f1475756d831d7e",
      "placeholder": "​",
      "style": "IPY_MODEL_a2b522fb50d54cecaead8906ecc2c88f",
      "value": ""
     }
    },
    "ba90ae57f0bf4101b5306b32401f5ca6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a0d229daebc428d8d0bea6db980845f",
      "max": 4542,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dc04c3799392479f84746241eac0c9bc",
      "value": 4542
     }
    },
    "bcaf3aab2df24777bb896394049144d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7dfb28ce1ec490bbbc71bbf8f8bc7bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d29ed3d913f4c3dadd8aa28ffdf60ab",
       "IPY_MODEL_739c6da6cd64446686e87fd499fc4e8c",
       "IPY_MODEL_2454c06e04cc44a59af6a54edf771dcf"
      ],
      "layout": "IPY_MODEL_fc29fb7181a64867aa764553a94327cf"
     }
    },
    "ca79710ed42844eda53edd54ad1cce9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d31e57f44fef4cd99cbda543609a5363": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8d21603da7146628c7025db753adeb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dbeac195f2cf4776b6c95dedda39513c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc04c3799392479f84746241eac0c9bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df312d8125694913844f2856ee295e67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df69f8986a7b46df869d78996f9a92bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_53264b907c324fd2903f6a33df4024eb",
       "IPY_MODEL_fe3e249b5f1947ec9fc496ab71992860",
       "IPY_MODEL_5f557bc2ab954fcdbb3145c5ae1cd1fe"
      ],
      "layout": "IPY_MODEL_3c55b6cc69fe4e71bfefdfafe172e1f4"
     }
    },
    "f1cb6810b0f0425c925092fd2c564460": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4bb69ed5dcf45cabca0294b04062d68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb3c5d28cc434035936450f03eaab7d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc29fb7181a64867aa764553a94327cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe3e249b5f1947ec9fc496ab71992860": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4bb69ed5dcf45cabca0294b04062d68",
      "max": 9912422,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d8d21603da7146628c7025db753adeb3",
      "value": 9912422
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
