{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd6e9a6",
   "metadata": {
    "id": "4fd6e9a6"
   },
   "source": [
    "# Satifsying requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25b8a9e",
   "metadata": {
    "id": "a25b8a9e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc1b4e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dc1b4e5",
    "outputId": "4ba9778d-35b6-457e-d8b4-e9c59969fc68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653bc14",
   "metadata": {
    "id": "e653bc14"
   },
   "source": [
    "# Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a22a6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467,
     "referenced_widgets": [
      "df69f8986a7b46df869d78996f9a92bf",
      "3c55b6cc69fe4e71bfefdfafe172e1f4",
      "53264b907c324fd2903f6a33df4024eb",
      "fe3e249b5f1947ec9fc496ab71992860",
      "5f557bc2ab954fcdbb3145c5ae1cd1fe",
      "ca79710ed42844eda53edd54ad1cce9e",
      "3629620f44dc4eec928e8285f4e292c7",
      "d8d21603da7146628c7025db753adeb3",
      "f4bb69ed5dcf45cabca0294b04062d68",
      "4b9f9045a6a64724b75bed3bdf4825e5",
      "831588c036b84ba5919552e1c0ffaf95",
      "c7dfb28ce1ec490bbbc71bbf8f8bc7bb",
      "fc29fb7181a64867aa764553a94327cf",
      "4d29ed3d913f4c3dadd8aa28ffdf60ab",
      "739c6da6cd64446686e87fd499fc4e8c",
      "2454c06e04cc44a59af6a54edf771dcf",
      "7598f22b00ca4188abd0e988d87c8717",
      "1fb4ea8439054fbeaab29cbbdd8d8c62",
      "ae7d8308ff8541d082c143732f92bce6",
      "1784bfcf80e54eb1a3b7873cd36494b1",
      "f1cb6810b0f0425c925092fd2c564460",
      "d31e57f44fef4cd99cbda543609a5363",
      "42c3a0dfb96d4d0a9cbcfbefdb88634a",
      "fb3c5d28cc434035936450f03eaab7d4",
      "b7381bd873134277869b9bf86f7f6cde",
      "92a84a7a4c1a49fca9c649540174de1a",
      "973a9057772b4a33bebe1c71b2548322",
      "a2b522fb50d54cecaead8906ecc2c88f",
      "4ee572a2f86844b38f1475756d831d7e",
      "a5767d4185754f1f82e22dc55b9b659e",
      "2bae88a771624fce8a7195ab2d3790e1",
      "656d789dd50d4d7c95121ee911a0315e",
      "bcaf3aab2df24777bb896394049144d2",
      "7192858941cb47649969779dc0e63c91",
      "0de84b8c5dde45b68b6fe495e3978524",
      "664e26cb5e91441d8360e652dc6f3f13",
      "ba90ae57f0bf4101b5306b32401f5ca6",
      "585d33ea2f6c4c6f866ab5eaf500a486",
      "611e4230f7284f56a36e03b6cab128bb",
      "dbeac195f2cf4776b6c95dedda39513c",
      "dc04c3799392479f84746241eac0c9bc",
      "9a0d229daebc428d8d0bea6db980845f",
      "00cd50cc19624ebf946c288dcf96ca77",
      "4472460dcaf349e282e8505097ac56db"
     ]
    },
    "id": "33a22a6f",
    "outputId": "d99650b6-68ff-4c1e-bf7c-3ac0dc41c7c3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OLEG\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True,            \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce5a95",
   "metadata": {
    "id": "09ce5a95"
   },
   "source": [
    "# Preparing data with DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd1531f",
   "metadata": {
    "id": "0cd1531f"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_data, \n",
    "                                          batch_size=80, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1,\n",
    "                                          pin_memory=True),\n",
    "    \n",
    "    'test'  : torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1,\n",
    "                                          pin_memory=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dada402",
   "metadata": {
    "id": "3dada402"
   },
   "source": [
    "# Defining a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "121f2af3",
   "metadata": {
    "id": "121f2af3"
   },
   "outputs": [],
   "source": [
    "n_qubits = 5\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def qnode(inputs, weights):\n",
    "    qml.templates.AngleEmbedding(features=inputs, wires=range(n_qubits))\n",
    "    \n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    \n",
    "    return [qml.expval(qml.PauliY(wires=i)) for i in range(n_qubits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e8604a",
   "metadata": {
    "id": "f8e8604a"
   },
   "outputs": [],
   "source": [
    "n_layers = 3\n",
    "weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67b30b7",
   "metadata": {
    "id": "a67b30b7"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class HybridNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=16,              \n",
    "                out_channels=32,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,    \n",
    "            ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),                \n",
    "        )\n",
    "        self.fc_1 = nn.Sequential(\n",
    "            nn.Linear(32 * 7 * 7, 20),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # LIST USAGE?\n",
    "        self.qlayer_1 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        self.qlayer_2 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        self.qlayer_3 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        self.qlayer_4 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        \n",
    "        self.qlayer_1.to(device)\n",
    "        self.qlayer_2.to(device)\n",
    "        self.qlayer_3.to(device)\n",
    "        self.qlayer_4.to(device)\n",
    "        \n",
    "        self.after_q = nn.Sequential(\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc_2 = nn.Linear(20, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1) \n",
    "\n",
    "        x = self.fc_1(x)\n",
    "        #print('Before split')\n",
    "        x_1, x_2, x_3, x_4 = torch.split(x, 5, dim=1) # second argument is number of elements in one new tensor\n",
    "        #print('After split')\n",
    "        #x = torch.Tensor(0)\n",
    "        \n",
    "        x_1 = self.qlayer_1(x_1)\n",
    "        x_2 = self.qlayer_2(x_2)\n",
    "        x_3 = self.qlayer_3(x_3)\n",
    "        x_4 = self.qlayer_4(x_4)\n",
    "        \n",
    "        #print(x.device)\n",
    "        \n",
    "        x = torch.cat([x_1, x_2, x_3, x_4], axis=1)\n",
    "        x = x.to(device)\n",
    "        x = self.after_q(x)\n",
    "        logits = self.fc_2(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db3df4eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db3df4eb",
    "outputId": "9eddce9b-8bf3-4fb1-8d8f-12fbb55c6b1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_1): Sequential(\n",
      "    (0): Linear(in_features=1568, out_features=20, bias=True)\n",
      "    (1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (qlayer_1): <Quantum Torch Layer: func=qnode>\n",
      "  (qlayer_2): <Quantum Torch Layer: func=qnode>\n",
      "  (qlayer_3): <Quantum Torch Layer: func=qnode>\n",
      "  (qlayer_4): <Quantum Torch Layer: func=qnode>\n",
      "  (after_q): Sequential(\n",
      "    (0): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc_2): Linear(in_features=20, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hnn = HybridNN()\n",
    "hnn = hnn.to(device)\n",
    "print(hnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809cbe6",
   "metadata": {
    "id": "6809cbe6"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a415a96",
   "metadata": {
    "id": "1a415a96"
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df9aafa9",
   "metadata": {
    "id": "df9aafa9"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.AdamW(hnn.parameters(), lr = 0.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cc5c6d9",
   "metadata": {
    "id": "7cc5c6d9"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def train(num_epochs, model, loaders):\n",
    "    \n",
    "    model.train()\n",
    "    history = []\n",
    "    \n",
    "    avg_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "        \n",
    "    for epoch in trange(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            b_x, b_y = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()           \n",
    "            \n",
    "            output = model(b_x)             \n",
    "            loss = loss_func(output, b_y)\n",
    "            \n",
    "            loss.backward()               \n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(loaders['train'])\n",
    "            \n",
    "            if (i+1) % 10 >= 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "        \n",
    "        model.eval()        \n",
    "        for X_batch, Y_batch in loaders['test']:\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(X_batch)\n",
    "                loss = loss_func(outputs, Y_batch)\n",
    "                val_loss += loss.item() / len(loaders['test'])\n",
    "            \n",
    "        history.append((avg_loss, val_loss))\n",
    "        print('Epoch [{}/{}], Tr. loss: {:.4f}. Test loss: {:.4f}' \n",
    "                       .format(epoch + 1, num_epochs, avg_loss, val_loss))\n",
    "        print('\\n')\n",
    "        torch.save(hnn.state_dict(), 'trained_model_24qubits_3_layers_with_strong_entagling_' + str(i+1) + 'ep5qu.pt')\n",
    "        with open('data_' + str(i+1) + 'ep5qu.txt', 'w') as outfile:\n",
    "            json.dump(history, outfile)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20f9ed6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "a2db74a18ce8412399aba11b3bab930b",
      "9002f4b1af6d4c03b8f53fecdffaa3b0",
      "aa56b1689edb4a75aa4af965407467e6",
      "8b99aefdd9a247678de2cefd312207bb",
      "0a0c3aa1494d454b85ce6083f2c98234",
      "0aa6d7245fc747b58457668a840cd66b",
      "df312d8125694913844f2856ee295e67",
      "70629c93466c490cb4d10c7ba2e41d62",
      "7081d0216eb6473fae87f7331becfb53",
      "979724284c094071b6c3143666a2a8d5",
      "7ca85bfb648a4586ab15f601f45bd7c2"
     ]
    },
    "id": "20f9ed6c",
    "outputId": "3b1078fe-8904-4227-b481-0d9b43a61f33",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422ed5f9646d41d7b95072f58bb7e2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OLEG\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:147: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  ..\\aten\\src\\ATen\\native\\Copy.cpp:240.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Step [1/750], Loss: 2.4006\n",
      "Epoch [1/8], Step [2/750], Loss: 2.2367\n",
      "Epoch [1/8], Step [3/750], Loss: 2.1521\n",
      "Epoch [1/8], Step [4/750], Loss: 1.9708\n",
      "Epoch [1/8], Step [5/750], Loss: 1.9448\n",
      "Epoch [1/8], Step [6/750], Loss: 1.9341\n",
      "Epoch [1/8], Step [7/750], Loss: 1.8421\n",
      "Epoch [1/8], Step [8/750], Loss: 1.7785\n",
      "Epoch [1/8], Step [9/750], Loss: 1.7914\n",
      "Epoch [1/8], Step [10/750], Loss: 1.6977\n",
      "Epoch [1/8], Step [11/750], Loss: 1.5762\n",
      "Epoch [1/8], Step [12/750], Loss: 1.6286\n",
      "Epoch [1/8], Step [13/750], Loss: 1.4751\n",
      "Epoch [1/8], Step [14/750], Loss: 1.3993\n",
      "Epoch [1/8], Step [15/750], Loss: 1.5742\n",
      "Epoch [1/8], Step [16/750], Loss: 1.4418\n",
      "Epoch [1/8], Step [17/750], Loss: 1.1578\n",
      "Epoch [1/8], Step [18/750], Loss: 1.3520\n",
      "Epoch [1/8], Step [19/750], Loss: 1.1554\n",
      "Epoch [1/8], Step [20/750], Loss: 1.0575\n",
      "Epoch [1/8], Step [21/750], Loss: 1.0462\n",
      "Epoch [1/8], Step [22/750], Loss: 1.1375\n",
      "Epoch [1/8], Step [23/750], Loss: 0.9748\n",
      "Epoch [1/8], Step [24/750], Loss: 0.9348\n",
      "Epoch [1/8], Step [25/750], Loss: 0.9283\n",
      "Epoch [1/8], Step [26/750], Loss: 0.8668\n",
      "Epoch [1/8], Step [27/750], Loss: 0.8482\n",
      "Epoch [1/8], Step [28/750], Loss: 0.7646\n",
      "Epoch [1/8], Step [29/750], Loss: 0.7619\n",
      "Epoch [1/8], Step [30/750], Loss: 0.6245\n",
      "Epoch [1/8], Step [31/750], Loss: 0.6334\n",
      "Epoch [1/8], Step [32/750], Loss: 0.6697\n",
      "Epoch [1/8], Step [33/750], Loss: 0.5792\n",
      "Epoch [1/8], Step [34/750], Loss: 0.5651\n",
      "Epoch [1/8], Step [35/750], Loss: 0.4940\n",
      "Epoch [1/8], Step [36/750], Loss: 0.5116\n",
      "Epoch [1/8], Step [37/750], Loss: 0.4035\n",
      "Epoch [1/8], Step [38/750], Loss: 0.3915\n",
      "Epoch [1/8], Step [39/750], Loss: 0.3065\n",
      "Epoch [1/8], Step [40/750], Loss: 0.3886\n",
      "Epoch [1/8], Step [41/750], Loss: 0.3780\n",
      "Epoch [1/8], Step [42/750], Loss: 0.3290\n",
      "Epoch [1/8], Step [43/750], Loss: 0.4588\n",
      "Epoch [1/8], Step [44/750], Loss: 0.6396\n",
      "Epoch [1/8], Step [45/750], Loss: 0.3331\n",
      "Epoch [1/8], Step [46/750], Loss: 0.3354\n",
      "Epoch [1/8], Step [47/750], Loss: 0.2536\n",
      "Epoch [1/8], Step [48/750], Loss: 0.2383\n",
      "Epoch [1/8], Step [49/750], Loss: 0.3220\n",
      "Epoch [1/8], Step [50/750], Loss: 0.2009\n",
      "Epoch [1/8], Step [51/750], Loss: 0.5300\n",
      "Epoch [1/8], Step [52/750], Loss: 0.1507\n",
      "Epoch [1/8], Step [53/750], Loss: 0.1909\n",
      "Epoch [1/8], Step [54/750], Loss: 0.4891\n",
      "Epoch [1/8], Step [55/750], Loss: 0.2116\n",
      "Epoch [1/8], Step [56/750], Loss: 0.4089\n",
      "Epoch [1/8], Step [57/750], Loss: 0.2954\n",
      "Epoch [1/8], Step [58/750], Loss: 0.2598\n",
      "Epoch [1/8], Step [59/750], Loss: 0.1600\n",
      "Epoch [1/8], Step [60/750], Loss: 0.2065\n",
      "Epoch [1/8], Step [61/750], Loss: 0.2245\n",
      "Epoch [1/8], Step [62/750], Loss: 0.3678\n",
      "Epoch [1/8], Step [63/750], Loss: 0.3263\n",
      "Epoch [1/8], Step [64/750], Loss: 0.3288\n",
      "Epoch [1/8], Step [65/750], Loss: 0.1184\n",
      "Epoch [1/8], Step [66/750], Loss: 0.3562\n",
      "Epoch [1/8], Step [67/750], Loss: 0.2552\n",
      "Epoch [1/8], Step [68/750], Loss: 0.1675\n",
      "Epoch [1/8], Step [69/750], Loss: 0.1403\n",
      "Epoch [1/8], Step [70/750], Loss: 0.2771\n",
      "Epoch [1/8], Step [71/750], Loss: 0.1835\n",
      "Epoch [1/8], Step [72/750], Loss: 0.2424\n",
      "Epoch [1/8], Step [73/750], Loss: 0.1114\n",
      "Epoch [1/8], Step [74/750], Loss: 0.2107\n",
      "Epoch [1/8], Step [75/750], Loss: 0.2029\n",
      "Epoch [1/8], Step [76/750], Loss: 0.2455\n",
      "Epoch [1/8], Step [77/750], Loss: 0.1026\n",
      "Epoch [1/8], Step [78/750], Loss: 0.0862\n",
      "Epoch [1/8], Step [79/750], Loss: 0.2055\n",
      "Epoch [1/8], Step [80/750], Loss: 0.2195\n",
      "Epoch [1/8], Step [81/750], Loss: 0.2324\n",
      "Epoch [1/8], Step [82/750], Loss: 0.1814\n",
      "Epoch [1/8], Step [83/750], Loss: 0.1226\n",
      "Epoch [1/8], Step [84/750], Loss: 0.1110\n",
      "Epoch [1/8], Step [85/750], Loss: 0.2399\n",
      "Epoch [1/8], Step [86/750], Loss: 0.1368\n",
      "Epoch [1/8], Step [87/750], Loss: 0.1225\n",
      "Epoch [1/8], Step [88/750], Loss: 0.1924\n",
      "Epoch [1/8], Step [89/750], Loss: 0.0863\n",
      "Epoch [1/8], Step [90/750], Loss: 0.1674\n",
      "Epoch [1/8], Step [91/750], Loss: 0.2070\n",
      "Epoch [1/8], Step [92/750], Loss: 0.0807\n",
      "Epoch [1/8], Step [93/750], Loss: 0.1036\n",
      "Epoch [1/8], Step [94/750], Loss: 0.2340\n",
      "Epoch [1/8], Step [95/750], Loss: 0.0636\n",
      "Epoch [1/8], Step [96/750], Loss: 0.1270\n",
      "Epoch [1/8], Step [97/750], Loss: 0.1456\n",
      "Epoch [1/8], Step [98/750], Loss: 0.2477\n",
      "Epoch [1/8], Step [99/750], Loss: 0.2111\n",
      "Epoch [1/8], Step [100/750], Loss: 0.1006\n",
      "Epoch [1/8], Step [101/750], Loss: 0.0799\n",
      "Epoch [1/8], Step [102/750], Loss: 0.1450\n",
      "Epoch [1/8], Step [103/750], Loss: 0.3287\n",
      "Epoch [1/8], Step [104/750], Loss: 0.1940\n",
      "Epoch [1/8], Step [105/750], Loss: 0.2018\n",
      "Epoch [1/8], Step [106/750], Loss: 0.2069\n",
      "Epoch [1/8], Step [107/750], Loss: 0.0589\n",
      "Epoch [1/8], Step [108/750], Loss: 0.1469\n",
      "Epoch [1/8], Step [109/750], Loss: 0.1163\n",
      "Epoch [1/8], Step [110/750], Loss: 0.1136\n",
      "Epoch [1/8], Step [111/750], Loss: 0.2486\n",
      "Epoch [1/8], Step [112/750], Loss: 0.1523\n",
      "Epoch [1/8], Step [113/750], Loss: 0.1248\n",
      "Epoch [1/8], Step [114/750], Loss: 0.1256\n",
      "Epoch [1/8], Step [115/750], Loss: 0.1436\n",
      "Epoch [1/8], Step [116/750], Loss: 0.0590\n",
      "Epoch [1/8], Step [117/750], Loss: 0.3455\n",
      "Epoch [1/8], Step [118/750], Loss: 0.0789\n",
      "Epoch [1/8], Step [119/750], Loss: 0.0813\n",
      "Epoch [1/8], Step [120/750], Loss: 0.1562\n",
      "Epoch [1/8], Step [121/750], Loss: 0.1848\n",
      "Epoch [1/8], Step [122/750], Loss: 0.0879\n",
      "Epoch [1/8], Step [123/750], Loss: 0.1696\n",
      "Epoch [1/8], Step [124/750], Loss: 0.1124\n",
      "Epoch [1/8], Step [125/750], Loss: 0.1512\n",
      "Epoch [1/8], Step [126/750], Loss: 0.1734\n",
      "Epoch [1/8], Step [127/750], Loss: 0.1253\n",
      "Epoch [1/8], Step [128/750], Loss: 0.2612\n",
      "Epoch [1/8], Step [129/750], Loss: 0.0714\n",
      "Epoch [1/8], Step [130/750], Loss: 0.1426\n",
      "Epoch [1/8], Step [131/750], Loss: 0.2175\n",
      "Epoch [1/8], Step [132/750], Loss: 0.0655\n",
      "Epoch [1/8], Step [133/750], Loss: 0.3155\n",
      "Epoch [1/8], Step [134/750], Loss: 0.0797\n",
      "Epoch [1/8], Step [135/750], Loss: 0.1806\n",
      "Epoch [1/8], Step [136/750], Loss: 0.2528\n",
      "Epoch [1/8], Step [137/750], Loss: 0.1287\n",
      "Epoch [1/8], Step [138/750], Loss: 0.1448\n",
      "Epoch [1/8], Step [139/750], Loss: 0.1232\n",
      "Epoch [1/8], Step [140/750], Loss: 0.1534\n",
      "Epoch [1/8], Step [141/750], Loss: 0.1005\n",
      "Epoch [1/8], Step [142/750], Loss: 0.1502\n",
      "Epoch [1/8], Step [143/750], Loss: 0.2536\n",
      "Epoch [1/8], Step [144/750], Loss: 0.2688\n",
      "Epoch [1/8], Step [145/750], Loss: 0.0987\n",
      "Epoch [1/8], Step [146/750], Loss: 0.1547\n",
      "Epoch [1/8], Step [147/750], Loss: 0.3620\n",
      "Epoch [1/8], Step [148/750], Loss: 0.0774\n",
      "Epoch [1/8], Step [149/750], Loss: 0.2409\n",
      "Epoch [1/8], Step [150/750], Loss: 0.0872\n",
      "Epoch [1/8], Step [151/750], Loss: 0.1055\n",
      "Epoch [1/8], Step [152/750], Loss: 0.1969\n",
      "Epoch [1/8], Step [153/750], Loss: 0.1636\n",
      "Epoch [1/8], Step [154/750], Loss: 0.1859\n",
      "Epoch [1/8], Step [155/750], Loss: 0.1810\n",
      "Epoch [1/8], Step [156/750], Loss: 0.1626\n",
      "Epoch [1/8], Step [157/750], Loss: 0.1070\n",
      "Epoch [1/8], Step [158/750], Loss: 0.1157\n",
      "Epoch [1/8], Step [159/750], Loss: 0.1816\n",
      "Epoch [1/8], Step [160/750], Loss: 0.0987\n",
      "Epoch [1/8], Step [161/750], Loss: 0.0285\n",
      "Epoch [1/8], Step [162/750], Loss: 0.1929\n",
      "Epoch [1/8], Step [163/750], Loss: 0.2512\n",
      "Epoch [1/8], Step [164/750], Loss: 0.1249\n",
      "Epoch [1/8], Step [165/750], Loss: 0.2039\n",
      "Epoch [1/8], Step [166/750], Loss: 0.2761\n",
      "Epoch [1/8], Step [167/750], Loss: 0.1735\n",
      "Epoch [1/8], Step [168/750], Loss: 0.1169\n",
      "Epoch [1/8], Step [169/750], Loss: 0.1261\n",
      "Epoch [1/8], Step [170/750], Loss: 0.1915\n",
      "Epoch [1/8], Step [171/750], Loss: 0.2024\n",
      "Epoch [1/8], Step [172/750], Loss: 0.1257\n",
      "Epoch [1/8], Step [173/750], Loss: 0.2976\n",
      "Epoch [1/8], Step [174/750], Loss: 0.1137\n",
      "Epoch [1/8], Step [175/750], Loss: 0.1874\n",
      "Epoch [1/8], Step [176/750], Loss: 0.0393\n",
      "Epoch [1/8], Step [177/750], Loss: 0.1931\n",
      "Epoch [1/8], Step [178/750], Loss: 0.1536\n",
      "Epoch [1/8], Step [179/750], Loss: 0.1912\n",
      "Epoch [1/8], Step [180/750], Loss: 0.0447\n",
      "Epoch [1/8], Step [181/750], Loss: 0.1929\n",
      "Epoch [1/8], Step [182/750], Loss: 0.1613\n",
      "Epoch [1/8], Step [183/750], Loss: 0.2006\n",
      "Epoch [1/8], Step [184/750], Loss: 0.1346\n",
      "Epoch [1/8], Step [185/750], Loss: 0.0946\n",
      "Epoch [1/8], Step [186/750], Loss: 0.1195\n",
      "Epoch [1/8], Step [187/750], Loss: 0.1284\n",
      "Epoch [1/8], Step [188/750], Loss: 0.1594\n",
      "Epoch [1/8], Step [189/750], Loss: 0.1758\n",
      "Epoch [1/8], Step [190/750], Loss: 0.1249\n",
      "Epoch [1/8], Step [191/750], Loss: 0.1043\n",
      "Epoch [1/8], Step [192/750], Loss: 0.1602\n",
      "Epoch [1/8], Step [193/750], Loss: 0.1251\n",
      "Epoch [1/8], Step [194/750], Loss: 0.1989\n",
      "Epoch [1/8], Step [195/750], Loss: 0.1549\n",
      "Epoch [1/8], Step [196/750], Loss: 0.1595\n",
      "Epoch [1/8], Step [197/750], Loss: 0.1866\n",
      "Epoch [1/8], Step [198/750], Loss: 0.0459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Step [199/750], Loss: 0.1943\n",
      "Epoch [1/8], Step [200/750], Loss: 0.0629\n",
      "Epoch [1/8], Step [201/750], Loss: 0.0743\n",
      "Epoch [1/8], Step [202/750], Loss: 0.2051\n",
      "Epoch [1/8], Step [203/750], Loss: 0.1137\n",
      "Epoch [1/8], Step [204/750], Loss: 0.0783\n",
      "Epoch [1/8], Step [205/750], Loss: 0.1751\n",
      "Epoch [1/8], Step [206/750], Loss: 0.1624\n",
      "Epoch [1/8], Step [207/750], Loss: 0.1419\n",
      "Epoch [1/8], Step [208/750], Loss: 0.0509\n",
      "Epoch [1/8], Step [209/750], Loss: 0.0444\n",
      "Epoch [1/8], Step [210/750], Loss: 0.0818\n",
      "Epoch [1/8], Step [211/750], Loss: 0.0878\n",
      "Epoch [1/8], Step [212/750], Loss: 0.1790\n",
      "Epoch [1/8], Step [213/750], Loss: 0.1059\n",
      "Epoch [1/8], Step [214/750], Loss: 0.1426\n",
      "Epoch [1/8], Step [215/750], Loss: 0.0893\n",
      "Epoch [1/8], Step [216/750], Loss: 0.1344\n",
      "Epoch [1/8], Step [217/750], Loss: 0.0944\n",
      "Epoch [1/8], Step [218/750], Loss: 0.0617\n",
      "Epoch [1/8], Step [219/750], Loss: 0.1382\n",
      "Epoch [1/8], Step [220/750], Loss: 0.1395\n",
      "Epoch [1/8], Step [221/750], Loss: 0.0370\n",
      "Epoch [1/8], Step [222/750], Loss: 0.0900\n",
      "Epoch [1/8], Step [223/750], Loss: 0.1328\n",
      "Epoch [1/8], Step [224/750], Loss: 0.1063\n",
      "Epoch [1/8], Step [225/750], Loss: 0.2240\n",
      "Epoch [1/8], Step [226/750], Loss: 0.1298\n",
      "Epoch [1/8], Step [227/750], Loss: 0.1231\n",
      "Epoch [1/8], Step [228/750], Loss: 0.1196\n",
      "Epoch [1/8], Step [229/750], Loss: 0.0938\n",
      "Epoch [1/8], Step [230/750], Loss: 0.1472\n",
      "Epoch [1/8], Step [231/750], Loss: 0.1298\n",
      "Epoch [1/8], Step [232/750], Loss: 0.0657\n",
      "Epoch [1/8], Step [233/750], Loss: 0.0962\n",
      "Epoch [1/8], Step [234/750], Loss: 0.1929\n",
      "Epoch [1/8], Step [235/750], Loss: 0.0285\n",
      "Epoch [1/8], Step [236/750], Loss: 0.0304\n",
      "Epoch [1/8], Step [237/750], Loss: 0.2193\n",
      "Epoch [1/8], Step [238/750], Loss: 0.1672\n",
      "Epoch [1/8], Step [239/750], Loss: 0.0475\n",
      "Epoch [1/8], Step [240/750], Loss: 0.0788\n",
      "Epoch [1/8], Step [241/750], Loss: 0.0477\n",
      "Epoch [1/8], Step [242/750], Loss: 0.0580\n",
      "Epoch [1/8], Step [243/750], Loss: 0.0957\n",
      "Epoch [1/8], Step [244/750], Loss: 0.1022\n",
      "Epoch [1/8], Step [245/750], Loss: 0.1264\n",
      "Epoch [1/8], Step [246/750], Loss: 0.1079\n",
      "Epoch [1/8], Step [247/750], Loss: 0.1266\n",
      "Epoch [1/8], Step [248/750], Loss: 0.3187\n",
      "Epoch [1/8], Step [249/750], Loss: 0.0337\n",
      "Epoch [1/8], Step [250/750], Loss: 0.1246\n",
      "Epoch [1/8], Step [251/750], Loss: 0.0563\n",
      "Epoch [1/8], Step [252/750], Loss: 0.0981\n",
      "Epoch [1/8], Step [253/750], Loss: 0.2036\n",
      "Epoch [1/8], Step [254/750], Loss: 0.1336\n",
      "Epoch [1/8], Step [255/750], Loss: 0.0267\n",
      "Epoch [1/8], Step [256/750], Loss: 0.1420\n",
      "Epoch [1/8], Step [257/750], Loss: 0.0749\n",
      "Epoch [1/8], Step [258/750], Loss: 0.1057\n",
      "Epoch [1/8], Step [259/750], Loss: 0.1634\n",
      "Epoch [1/8], Step [260/750], Loss: 0.1914\n",
      "Epoch [1/8], Step [261/750], Loss: 0.1460\n",
      "Epoch [1/8], Step [262/750], Loss: 0.0589\n",
      "Epoch [1/8], Step [263/750], Loss: 0.1275\n",
      "Epoch [1/8], Step [264/750], Loss: 0.1242\n",
      "Epoch [1/8], Step [265/750], Loss: 0.0461\n",
      "Epoch [1/8], Step [266/750], Loss: 0.0391\n",
      "Epoch [1/8], Step [267/750], Loss: 0.1704\n",
      "Epoch [1/8], Step [268/750], Loss: 0.1008\n",
      "Epoch [1/8], Step [269/750], Loss: 0.0392\n",
      "Epoch [1/8], Step [270/750], Loss: 0.0146\n",
      "Epoch [1/8], Step [271/750], Loss: 0.1560\n",
      "Epoch [1/8], Step [272/750], Loss: 0.0725\n",
      "Epoch [1/8], Step [273/750], Loss: 0.1174\n",
      "Epoch [1/8], Step [274/750], Loss: 0.1154\n",
      "Epoch [1/8], Step [275/750], Loss: 0.0824\n",
      "Epoch [1/8], Step [276/750], Loss: 0.0649\n",
      "Epoch [1/8], Step [277/750], Loss: 0.0987\n",
      "Epoch [1/8], Step [278/750], Loss: 0.0393\n",
      "Epoch [1/8], Step [279/750], Loss: 0.0196\n",
      "Epoch [1/8], Step [280/750], Loss: 0.1151\n",
      "Epoch [1/8], Step [281/750], Loss: 0.0708\n",
      "Epoch [1/8], Step [282/750], Loss: 0.0856\n",
      "Epoch [1/8], Step [283/750], Loss: 0.1050\n",
      "Epoch [1/8], Step [284/750], Loss: 0.1305\n",
      "Epoch [1/8], Step [285/750], Loss: 0.1102\n",
      "Epoch [1/8], Step [286/750], Loss: 0.0753\n",
      "Epoch [1/8], Step [287/750], Loss: 0.0690\n",
      "Epoch [1/8], Step [288/750], Loss: 0.0791\n",
      "Epoch [1/8], Step [289/750], Loss: 0.0620\n",
      "Epoch [1/8], Step [290/750], Loss: 0.0414\n",
      "Epoch [1/8], Step [291/750], Loss: 0.0974\n",
      "Epoch [1/8], Step [292/750], Loss: 0.0448\n",
      "Epoch [1/8], Step [293/750], Loss: 0.1213\n",
      "Epoch [1/8], Step [294/750], Loss: 0.1186\n",
      "Epoch [1/8], Step [295/750], Loss: 0.0788\n",
      "Epoch [1/8], Step [296/750], Loss: 0.0902\n",
      "Epoch [1/8], Step [297/750], Loss: 0.0405\n",
      "Epoch [1/8], Step [298/750], Loss: 0.1064\n",
      "Epoch [1/8], Step [299/750], Loss: 0.1068\n",
      "Epoch [1/8], Step [300/750], Loss: 0.1117\n",
      "Epoch [1/8], Step [301/750], Loss: 0.2417\n",
      "Epoch [1/8], Step [302/750], Loss: 0.1067\n",
      "Epoch [1/8], Step [303/750], Loss: 0.0285\n",
      "Epoch [1/8], Step [304/750], Loss: 0.1835\n",
      "Epoch [1/8], Step [305/750], Loss: 0.0533\n",
      "Epoch [1/8], Step [306/750], Loss: 0.1140\n",
      "Epoch [1/8], Step [307/750], Loss: 0.1107\n",
      "Epoch [1/8], Step [308/750], Loss: 0.0548\n",
      "Epoch [1/8], Step [309/750], Loss: 0.2129\n",
      "Epoch [1/8], Step [310/750], Loss: 0.1430\n",
      "Epoch [1/8], Step [311/750], Loss: 0.0261\n",
      "Epoch [1/8], Step [312/750], Loss: 0.0701\n",
      "Epoch [1/8], Step [313/750], Loss: 0.0374\n",
      "Epoch [1/8], Step [314/750], Loss: 0.0654\n",
      "Epoch [1/8], Step [315/750], Loss: 0.1617\n",
      "Epoch [1/8], Step [316/750], Loss: 0.0397\n",
      "Epoch [1/8], Step [317/750], Loss: 0.0933\n",
      "Epoch [1/8], Step [318/750], Loss: 0.1515\n",
      "Epoch [1/8], Step [319/750], Loss: 0.0902\n",
      "Epoch [1/8], Step [320/750], Loss: 0.0575\n",
      "Epoch [1/8], Step [321/750], Loss: 0.1024\n",
      "Epoch [1/8], Step [322/750], Loss: 0.2890\n",
      "Epoch [1/8], Step [323/750], Loss: 0.1224\n",
      "Epoch [1/8], Step [324/750], Loss: 0.0382\n",
      "Epoch [1/8], Step [325/750], Loss: 0.0724\n",
      "Epoch [1/8], Step [326/750], Loss: 0.0729\n",
      "Epoch [1/8], Step [327/750], Loss: 0.0916\n",
      "Epoch [1/8], Step [328/750], Loss: 0.2204\n",
      "Epoch [1/8], Step [329/750], Loss: 0.0653\n",
      "Epoch [1/8], Step [330/750], Loss: 0.0231\n",
      "Epoch [1/8], Step [331/750], Loss: 0.0951\n",
      "Epoch [1/8], Step [332/750], Loss: 0.0213\n",
      "Epoch [1/8], Step [333/750], Loss: 0.0537\n",
      "Epoch [1/8], Step [334/750], Loss: 0.3317\n",
      "Epoch [1/8], Step [335/750], Loss: 0.1277\n",
      "Epoch [1/8], Step [336/750], Loss: 0.1335\n",
      "Epoch [1/8], Step [337/750], Loss: 0.0598\n",
      "Epoch [1/8], Step [338/750], Loss: 0.0661\n",
      "Epoch [1/8], Step [339/750], Loss: 0.0742\n",
      "Epoch [1/8], Step [340/750], Loss: 0.1524\n",
      "Epoch [1/8], Step [341/750], Loss: 0.0787\n",
      "Epoch [1/8], Step [342/750], Loss: 0.0877\n",
      "Epoch [1/8], Step [343/750], Loss: 0.0701\n",
      "Epoch [1/8], Step [344/750], Loss: 0.0922\n",
      "Epoch [1/8], Step [345/750], Loss: 0.0710\n",
      "Epoch [1/8], Step [346/750], Loss: 0.0262\n",
      "Epoch [1/8], Step [347/750], Loss: 0.0486\n",
      "Epoch [1/8], Step [348/750], Loss: 0.0966\n",
      "Epoch [1/8], Step [349/750], Loss: 0.0501\n",
      "Epoch [1/8], Step [350/750], Loss: 0.1114\n",
      "Epoch [1/8], Step [351/750], Loss: 0.2509\n",
      "Epoch [1/8], Step [352/750], Loss: 0.1862\n",
      "Epoch [1/8], Step [353/750], Loss: 0.0859\n",
      "Epoch [1/8], Step [354/750], Loss: 0.0105\n",
      "Epoch [1/8], Step [355/750], Loss: 0.0881\n",
      "Epoch [1/8], Step [356/750], Loss: 0.0994\n",
      "Epoch [1/8], Step [357/750], Loss: 0.0842\n",
      "Epoch [1/8], Step [358/750], Loss: 0.2364\n",
      "Epoch [1/8], Step [359/750], Loss: 0.0731\n",
      "Epoch [1/8], Step [360/750], Loss: 0.0750\n",
      "Epoch [1/8], Step [361/750], Loss: 0.0343\n",
      "Epoch [1/8], Step [362/750], Loss: 0.0827\n",
      "Epoch [1/8], Step [363/750], Loss: 0.1424\n",
      "Epoch [1/8], Step [364/750], Loss: 0.0980\n",
      "Epoch [1/8], Step [365/750], Loss: 0.1869\n",
      "Epoch [1/8], Step [366/750], Loss: 0.0618\n",
      "Epoch [1/8], Step [367/750], Loss: 0.0744\n",
      "Epoch [1/8], Step [368/750], Loss: 0.0817\n",
      "Epoch [1/8], Step [369/750], Loss: 0.0718\n",
      "Epoch [1/8], Step [370/750], Loss: 0.2076\n",
      "Epoch [1/8], Step [371/750], Loss: 0.0166\n",
      "Epoch [1/8], Step [372/750], Loss: 0.0333\n",
      "Epoch [1/8], Step [373/750], Loss: 0.0390\n",
      "Epoch [1/8], Step [374/750], Loss: 0.0613\n",
      "Epoch [1/8], Step [375/750], Loss: 0.0748\n",
      "Epoch [1/8], Step [376/750], Loss: 0.0529\n",
      "Epoch [1/8], Step [377/750], Loss: 0.0712\n",
      "Epoch [1/8], Step [378/750], Loss: 0.0843\n",
      "Epoch [1/8], Step [379/750], Loss: 0.1087\n",
      "Epoch [1/8], Step [380/750], Loss: 0.0657\n",
      "Epoch [1/8], Step [381/750], Loss: 0.1459\n",
      "Epoch [1/8], Step [382/750], Loss: 0.0446\n",
      "Epoch [1/8], Step [383/750], Loss: 0.1136\n",
      "Epoch [1/8], Step [384/750], Loss: 0.0723\n",
      "Epoch [1/8], Step [385/750], Loss: 0.1119\n",
      "Epoch [1/8], Step [386/750], Loss: 0.0323\n",
      "Epoch [1/8], Step [387/750], Loss: 0.1448\n",
      "Epoch [1/8], Step [388/750], Loss: 0.0189\n",
      "Epoch [1/8], Step [389/750], Loss: 0.1485\n",
      "Epoch [1/8], Step [390/750], Loss: 0.1663\n",
      "Epoch [1/8], Step [391/750], Loss: 0.1295\n",
      "Epoch [1/8], Step [392/750], Loss: 0.1032\n",
      "Epoch [1/8], Step [393/750], Loss: 0.1042\n",
      "Epoch [1/8], Step [394/750], Loss: 0.0537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Step [395/750], Loss: 0.0233\n",
      "Epoch [1/8], Step [396/750], Loss: 0.0117\n",
      "Epoch [1/8], Step [397/750], Loss: 0.0687\n",
      "Epoch [1/8], Step [398/750], Loss: 0.2083\n",
      "Epoch [1/8], Step [399/750], Loss: 0.0267\n",
      "Epoch [1/8], Step [400/750], Loss: 0.0419\n",
      "Epoch [1/8], Step [401/750], Loss: 0.1274\n",
      "Epoch [1/8], Step [402/750], Loss: 0.1187\n",
      "Epoch [1/8], Step [403/750], Loss: 0.1631\n",
      "Epoch [1/8], Step [404/750], Loss: 0.0749\n",
      "Epoch [1/8], Step [405/750], Loss: 0.1018\n",
      "Epoch [1/8], Step [406/750], Loss: 0.1024\n",
      "Epoch [1/8], Step [407/750], Loss: 0.0324\n",
      "Epoch [1/8], Step [408/750], Loss: 0.1282\n",
      "Epoch [1/8], Step [409/750], Loss: 0.0486\n",
      "Epoch [1/8], Step [410/750], Loss: 0.0444\n",
      "Epoch [1/8], Step [411/750], Loss: 0.1196\n",
      "Epoch [1/8], Step [412/750], Loss: 0.1716\n",
      "Epoch [1/8], Step [413/750], Loss: 0.1099\n",
      "Epoch [1/8], Step [414/750], Loss: 0.0602\n",
      "Epoch [1/8], Step [415/750], Loss: 0.1207\n",
      "Epoch [1/8], Step [416/750], Loss: 0.0975\n",
      "Epoch [1/8], Step [417/750], Loss: 0.0424\n",
      "Epoch [1/8], Step [418/750], Loss: 0.0927\n",
      "Epoch [1/8], Step [419/750], Loss: 0.1028\n",
      "Epoch [1/8], Step [420/750], Loss: 0.0543\n",
      "Epoch [1/8], Step [421/750], Loss: 0.0893\n",
      "Epoch [1/8], Step [422/750], Loss: 0.0696\n",
      "Epoch [1/8], Step [423/750], Loss: 0.0167\n",
      "Epoch [1/8], Step [424/750], Loss: 0.1271\n",
      "Epoch [1/8], Step [425/750], Loss: 0.0772\n",
      "Epoch [1/8], Step [426/750], Loss: 0.0084\n",
      "Epoch [1/8], Step [427/750], Loss: 0.1035\n",
      "Epoch [1/8], Step [428/750], Loss: 0.2743\n",
      "Epoch [1/8], Step [429/750], Loss: 0.0962\n",
      "Epoch [1/8], Step [430/750], Loss: 0.0092\n",
      "Epoch [1/8], Step [431/750], Loss: 0.0979\n",
      "Epoch [1/8], Step [432/750], Loss: 0.0614\n",
      "Epoch [1/8], Step [433/750], Loss: 0.0210\n",
      "Epoch [1/8], Step [434/750], Loss: 0.1233\n",
      "Epoch [1/8], Step [435/750], Loss: 0.0238\n",
      "Epoch [1/8], Step [436/750], Loss: 0.0308\n",
      "Epoch [1/8], Step [437/750], Loss: 0.0793\n",
      "Epoch [1/8], Step [438/750], Loss: 0.0765\n",
      "Epoch [1/8], Step [439/750], Loss: 0.1053\n",
      "Epoch [1/8], Step [440/750], Loss: 0.0491\n",
      "Epoch [1/8], Step [441/750], Loss: 0.0957\n",
      "Epoch [1/8], Step [442/750], Loss: 0.0437\n",
      "Epoch [1/8], Step [443/750], Loss: 0.0279\n",
      "Epoch [1/8], Step [444/750], Loss: 0.0617\n",
      "Epoch [1/8], Step [445/750], Loss: 0.0551\n",
      "Epoch [1/8], Step [446/750], Loss: 0.1280\n",
      "Epoch [1/8], Step [447/750], Loss: 0.0425\n",
      "Epoch [1/8], Step [448/750], Loss: 0.0536\n",
      "Epoch [1/8], Step [449/750], Loss: 0.1648\n",
      "Epoch [1/8], Step [450/750], Loss: 0.0256\n",
      "Epoch [1/8], Step [451/750], Loss: 0.0591\n",
      "Epoch [1/8], Step [452/750], Loss: 0.1068\n",
      "Epoch [1/8], Step [453/750], Loss: 0.0479\n",
      "Epoch [1/8], Step [454/750], Loss: 0.1475\n",
      "Epoch [1/8], Step [455/750], Loss: 0.0263\n",
      "Epoch [1/8], Step [456/750], Loss: 0.0161\n",
      "Epoch [1/8], Step [457/750], Loss: 0.1039\n",
      "Epoch [1/8], Step [458/750], Loss: 0.1262\n",
      "Epoch [1/8], Step [459/750], Loss: 0.0908\n",
      "Epoch [1/8], Step [460/750], Loss: 0.1430\n",
      "Epoch [1/8], Step [461/750], Loss: 0.0537\n",
      "Epoch [1/8], Step [462/750], Loss: 0.0243\n",
      "Epoch [1/8], Step [463/750], Loss: 0.0577\n",
      "Epoch [1/8], Step [464/750], Loss: 0.0144\n",
      "Epoch [1/8], Step [465/750], Loss: 0.1081\n",
      "Epoch [1/8], Step [466/750], Loss: 0.0562\n",
      "Epoch [1/8], Step [467/750], Loss: 0.1325\n",
      "Epoch [1/8], Step [468/750], Loss: 0.0485\n",
      "Epoch [1/8], Step [469/750], Loss: 0.0976\n",
      "Epoch [1/8], Step [470/750], Loss: 0.0458\n",
      "Epoch [1/8], Step [471/750], Loss: 0.0987\n",
      "Epoch [1/8], Step [472/750], Loss: 0.0715\n",
      "Epoch [1/8], Step [473/750], Loss: 0.0589\n",
      "Epoch [1/8], Step [474/750], Loss: 0.1399\n",
      "Epoch [1/8], Step [475/750], Loss: 0.0483\n",
      "Epoch [1/8], Step [476/750], Loss: 0.0709\n",
      "Epoch [1/8], Step [477/750], Loss: 0.0858\n",
      "Epoch [1/8], Step [478/750], Loss: 0.0539\n",
      "Epoch [1/8], Step [479/750], Loss: 0.0358\n",
      "Epoch [1/8], Step [480/750], Loss: 0.1197\n",
      "Epoch [1/8], Step [481/750], Loss: 0.0386\n",
      "Epoch [1/8], Step [482/750], Loss: 0.0481\n",
      "Epoch [1/8], Step [483/750], Loss: 0.0962\n",
      "Epoch [1/8], Step [484/750], Loss: 0.1001\n",
      "Epoch [1/8], Step [485/750], Loss: 0.0932\n",
      "Epoch [1/8], Step [486/750], Loss: 0.0128\n",
      "Epoch [1/8], Step [487/750], Loss: 0.1115\n",
      "Epoch [1/8], Step [488/750], Loss: 0.0294\n",
      "Epoch [1/8], Step [489/750], Loss: 0.0147\n",
      "Epoch [1/8], Step [490/750], Loss: 0.0331\n",
      "Epoch [1/8], Step [491/750], Loss: 0.2438\n",
      "Epoch [1/8], Step [492/750], Loss: 0.0571\n",
      "Epoch [1/8], Step [493/750], Loss: 0.0375\n",
      "Epoch [1/8], Step [494/750], Loss: 0.0203\n",
      "Epoch [1/8], Step [495/750], Loss: 0.0668\n",
      "Epoch [1/8], Step [496/750], Loss: 0.0801\n",
      "Epoch [1/8], Step [497/750], Loss: 0.1079\n",
      "Epoch [1/8], Step [498/750], Loss: 0.2472\n",
      "Epoch [1/8], Step [499/750], Loss: 0.1140\n",
      "Epoch [1/8], Step [500/750], Loss: 0.1352\n",
      "Epoch [1/8], Step [501/750], Loss: 0.0203\n",
      "Epoch [1/8], Step [502/750], Loss: 0.0974\n",
      "Epoch [1/8], Step [503/750], Loss: 0.0272\n",
      "Epoch [1/8], Step [504/750], Loss: 0.0489\n",
      "Epoch [1/8], Step [505/750], Loss: 0.0161\n",
      "Epoch [1/8], Step [506/750], Loss: 0.0811\n",
      "Epoch [1/8], Step [507/750], Loss: 0.0290\n",
      "Epoch [1/8], Step [508/750], Loss: 0.0659\n",
      "Epoch [1/8], Step [509/750], Loss: 0.0271\n",
      "Epoch [1/8], Step [510/750], Loss: 0.0550\n",
      "Epoch [1/8], Step [511/750], Loss: 0.1559\n",
      "Epoch [1/8], Step [512/750], Loss: 0.1209\n",
      "Epoch [1/8], Step [513/750], Loss: 0.0241\n",
      "Epoch [1/8], Step [514/750], Loss: 0.1917\n",
      "Epoch [1/8], Step [515/750], Loss: 0.0331\n",
      "Epoch [1/8], Step [516/750], Loss: 0.0793\n",
      "Epoch [1/8], Step [517/750], Loss: 0.0407\n",
      "Epoch [1/8], Step [518/750], Loss: 0.0653\n",
      "Epoch [1/8], Step [519/750], Loss: 0.1167\n",
      "Epoch [1/8], Step [520/750], Loss: 0.0435\n",
      "Epoch [1/8], Step [521/750], Loss: 0.0522\n",
      "Epoch [1/8], Step [522/750], Loss: 0.0678\n",
      "Epoch [1/8], Step [523/750], Loss: 0.0580\n",
      "Epoch [1/8], Step [524/750], Loss: 0.0805\n",
      "Epoch [1/8], Step [525/750], Loss: 0.0179\n",
      "Epoch [1/8], Step [526/750], Loss: 0.2303\n",
      "Epoch [1/8], Step [527/750], Loss: 0.0962\n",
      "Epoch [1/8], Step [528/750], Loss: 0.0680\n",
      "Epoch [1/8], Step [529/750], Loss: 0.1023\n",
      "Epoch [1/8], Step [530/750], Loss: 0.0873\n",
      "Epoch [1/8], Step [531/750], Loss: 0.1799\n",
      "Epoch [1/8], Step [532/750], Loss: 0.0484\n",
      "Epoch [1/8], Step [533/750], Loss: 0.0293\n",
      "Epoch [1/8], Step [534/750], Loss: 0.0330\n",
      "Epoch [1/8], Step [535/750], Loss: 0.1432\n",
      "Epoch [1/8], Step [536/750], Loss: 0.0431\n",
      "Epoch [1/8], Step [537/750], Loss: 0.0367\n",
      "Epoch [1/8], Step [538/750], Loss: 0.1944\n",
      "Epoch [1/8], Step [539/750], Loss: 0.1107\n",
      "Epoch [1/8], Step [540/750], Loss: 0.0371\n",
      "Epoch [1/8], Step [541/750], Loss: 0.0500\n",
      "Epoch [1/8], Step [542/750], Loss: 0.0238\n",
      "Epoch [1/8], Step [543/750], Loss: 0.0125\n",
      "Epoch [1/8], Step [544/750], Loss: 0.1212\n",
      "Epoch [1/8], Step [545/750], Loss: 0.2091\n",
      "Epoch [1/8], Step [546/750], Loss: 0.0468\n",
      "Epoch [1/8], Step [547/750], Loss: 0.0693\n",
      "Epoch [1/8], Step [548/750], Loss: 0.0537\n",
      "Epoch [1/8], Step [549/750], Loss: 0.0142\n",
      "Epoch [1/8], Step [550/750], Loss: 0.0702\n",
      "Epoch [1/8], Step [551/750], Loss: 0.1301\n",
      "Epoch [1/8], Step [552/750], Loss: 0.0605\n",
      "Epoch [1/8], Step [553/750], Loss: 0.0358\n",
      "Epoch [1/8], Step [554/750], Loss: 0.0540\n",
      "Epoch [1/8], Step [555/750], Loss: 0.0691\n",
      "Epoch [1/8], Step [556/750], Loss: 0.1746\n",
      "Epoch [1/8], Step [557/750], Loss: 0.0317\n",
      "Epoch [1/8], Step [558/750], Loss: 0.0621\n",
      "Epoch [1/8], Step [559/750], Loss: 0.0641\n",
      "Epoch [1/8], Step [560/750], Loss: 0.1575\n",
      "Epoch [1/8], Step [561/750], Loss: 0.0772\n",
      "Epoch [1/8], Step [562/750], Loss: 0.0577\n",
      "Epoch [1/8], Step [563/750], Loss: 0.0882\n",
      "Epoch [1/8], Step [564/750], Loss: 0.0861\n",
      "Epoch [1/8], Step [565/750], Loss: 0.0535\n",
      "Epoch [1/8], Step [566/750], Loss: 0.0677\n",
      "Epoch [1/8], Step [567/750], Loss: 0.0866\n",
      "Epoch [1/8], Step [568/750], Loss: 0.0627\n",
      "Epoch [1/8], Step [569/750], Loss: 0.0442\n",
      "Epoch [1/8], Step [570/750], Loss: 0.0204\n",
      "Epoch [1/8], Step [571/750], Loss: 0.1355\n",
      "Epoch [1/8], Step [572/750], Loss: 0.0543\n",
      "Epoch [1/8], Step [573/750], Loss: 0.1114\n",
      "Epoch [1/8], Step [574/750], Loss: 0.1710\n",
      "Epoch [1/8], Step [575/750], Loss: 0.0382\n",
      "Epoch [1/8], Step [576/750], Loss: 0.0317\n",
      "Epoch [1/8], Step [577/750], Loss: 0.0287\n",
      "Epoch [1/8], Step [578/750], Loss: 0.0627\n",
      "Epoch [1/8], Step [579/750], Loss: 0.0723\n",
      "Epoch [1/8], Step [580/750], Loss: 0.0450\n",
      "Epoch [1/8], Step [581/750], Loss: 0.0380\n",
      "Epoch [1/8], Step [582/750], Loss: 0.0146\n",
      "Epoch [1/8], Step [583/750], Loss: 0.0499\n",
      "Epoch [1/8], Step [584/750], Loss: 0.0251\n",
      "Epoch [1/8], Step [585/750], Loss: 0.0598\n",
      "Epoch [1/8], Step [586/750], Loss: 0.0984\n",
      "Epoch [1/8], Step [587/750], Loss: 0.0182\n",
      "Epoch [1/8], Step [588/750], Loss: 0.1148\n",
      "Epoch [1/8], Step [589/750], Loss: 0.1028\n",
      "Epoch [1/8], Step [590/750], Loss: 0.0833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Step [591/750], Loss: 0.1133\n",
      "Epoch [1/8], Step [592/750], Loss: 0.0812\n",
      "Epoch [1/8], Step [593/750], Loss: 0.0749\n",
      "Epoch [1/8], Step [594/750], Loss: 0.1037\n",
      "Epoch [1/8], Step [595/750], Loss: 0.0126\n",
      "Epoch [1/8], Step [596/750], Loss: 0.1119\n",
      "Epoch [1/8], Step [597/750], Loss: 0.0940\n",
      "Epoch [1/8], Step [598/750], Loss: 0.0805\n",
      "Epoch [1/8], Step [599/750], Loss: 0.0332\n",
      "Epoch [1/8], Step [600/750], Loss: 0.0416\n",
      "Epoch [1/8], Step [601/750], Loss: 0.0445\n",
      "Epoch [1/8], Step [602/750], Loss: 0.0144\n",
      "Epoch [1/8], Step [603/750], Loss: 0.0225\n",
      "Epoch [1/8], Step [604/750], Loss: 0.0592\n",
      "Epoch [1/8], Step [605/750], Loss: 0.0903\n",
      "Epoch [1/8], Step [606/750], Loss: 0.1255\n",
      "Epoch [1/8], Step [607/750], Loss: 0.0786\n",
      "Epoch [1/8], Step [608/750], Loss: 0.0385\n",
      "Epoch [1/8], Step [609/750], Loss: 0.0694\n",
      "Epoch [1/8], Step [610/750], Loss: 0.2410\n",
      "Epoch [1/8], Step [611/750], Loss: 0.0693\n",
      "Epoch [1/8], Step [612/750], Loss: 0.0782\n",
      "Epoch [1/8], Step [613/750], Loss: 0.0168\n",
      "Epoch [1/8], Step [614/750], Loss: 0.0458\n",
      "Epoch [1/8], Step [615/750], Loss: 0.0944\n",
      "Epoch [1/8], Step [616/750], Loss: 0.0267\n",
      "Epoch [1/8], Step [617/750], Loss: 0.0667\n",
      "Epoch [1/8], Step [618/750], Loss: 0.0217\n",
      "Epoch [1/8], Step [619/750], Loss: 0.0261\n",
      "Epoch [1/8], Step [620/750], Loss: 0.0666\n",
      "Epoch [1/8], Step [621/750], Loss: 0.0381\n",
      "Epoch [1/8], Step [622/750], Loss: 0.0968\n",
      "Epoch [1/8], Step [623/750], Loss: 0.1427\n",
      "Epoch [1/8], Step [624/750], Loss: 0.0571\n",
      "Epoch [1/8], Step [625/750], Loss: 0.1168\n",
      "Epoch [1/8], Step [626/750], Loss: 0.0292\n",
      "Epoch [1/8], Step [627/750], Loss: 0.1563\n",
      "Epoch [1/8], Step [628/750], Loss: 0.0211\n",
      "Epoch [1/8], Step [629/750], Loss: 0.0412\n",
      "Epoch [1/8], Step [630/750], Loss: 0.1207\n",
      "Epoch [1/8], Step [631/750], Loss: 0.0314\n",
      "Epoch [1/8], Step [632/750], Loss: 0.0038\n",
      "Epoch [1/8], Step [633/750], Loss: 0.0723\n",
      "Epoch [1/8], Step [634/750], Loss: 0.1235\n",
      "Epoch [1/8], Step [635/750], Loss: 0.0484\n",
      "Epoch [1/8], Step [636/750], Loss: 0.0659\n",
      "Epoch [1/8], Step [637/750], Loss: 0.0439\n",
      "Epoch [1/8], Step [638/750], Loss: 0.0656\n",
      "Epoch [1/8], Step [639/750], Loss: 0.1667\n",
      "Epoch [1/8], Step [640/750], Loss: 0.1051\n",
      "Epoch [1/8], Step [641/750], Loss: 0.0267\n",
      "Epoch [1/8], Step [642/750], Loss: 0.1085\n",
      "Epoch [1/8], Step [643/750], Loss: 0.0508\n",
      "Epoch [1/8], Step [644/750], Loss: 0.0297\n",
      "Epoch [1/8], Step [645/750], Loss: 0.0232\n",
      "Epoch [1/8], Step [646/750], Loss: 0.0861\n",
      "Epoch [1/8], Step [647/750], Loss: 0.0749\n",
      "Epoch [1/8], Step [648/750], Loss: 0.0088\n",
      "Epoch [1/8], Step [649/750], Loss: 0.0280\n",
      "Epoch [1/8], Step [650/750], Loss: 0.1408\n",
      "Epoch [1/8], Step [651/750], Loss: 0.0827\n",
      "Epoch [1/8], Step [652/750], Loss: 0.1442\n",
      "Epoch [1/8], Step [653/750], Loss: 0.0123\n",
      "Epoch [1/8], Step [654/750], Loss: 0.1825\n",
      "Epoch [1/8], Step [655/750], Loss: 0.1268\n",
      "Epoch [1/8], Step [656/750], Loss: 0.0611\n",
      "Epoch [1/8], Step [657/750], Loss: 0.0699\n",
      "Epoch [1/8], Step [658/750], Loss: 0.0547\n",
      "Epoch [1/8], Step [659/750], Loss: 0.1587\n",
      "Epoch [1/8], Step [660/750], Loss: 0.1159\n",
      "Epoch [1/8], Step [661/750], Loss: 0.0600\n",
      "Epoch [1/8], Step [662/750], Loss: 0.0989\n",
      "Epoch [1/8], Step [663/750], Loss: 0.1033\n",
      "Epoch [1/8], Step [664/750], Loss: 0.0711\n",
      "Epoch [1/8], Step [665/750], Loss: 0.1688\n",
      "Epoch [1/8], Step [666/750], Loss: 0.0332\n",
      "Epoch [1/8], Step [667/750], Loss: 0.0124\n",
      "Epoch [1/8], Step [668/750], Loss: 0.0667\n",
      "Epoch [1/8], Step [669/750], Loss: 0.1731\n",
      "Epoch [1/8], Step [670/750], Loss: 0.1843\n",
      "Epoch [1/8], Step [671/750], Loss: 0.0341\n",
      "Epoch [1/8], Step [672/750], Loss: 0.0322\n",
      "Epoch [1/8], Step [673/750], Loss: 0.1158\n",
      "Epoch [1/8], Step [674/750], Loss: 0.0304\n",
      "Epoch [1/8], Step [675/750], Loss: 0.0360\n",
      "Epoch [1/8], Step [676/750], Loss: 0.0986\n",
      "Epoch [1/8], Step [677/750], Loss: 0.0467\n",
      "Epoch [1/8], Step [678/750], Loss: 0.0442\n",
      "Epoch [1/8], Step [679/750], Loss: 0.0619\n",
      "Epoch [1/8], Step [680/750], Loss: 0.0466\n",
      "Epoch [1/8], Step [681/750], Loss: 0.0645\n",
      "Epoch [1/8], Step [682/750], Loss: 0.0833\n",
      "Epoch [1/8], Step [683/750], Loss: 0.1044\n",
      "Epoch [1/8], Step [684/750], Loss: 0.2442\n",
      "Epoch [1/8], Step [685/750], Loss: 0.0678\n",
      "Epoch [1/8], Step [686/750], Loss: 0.0223\n",
      "Epoch [1/8], Step [687/750], Loss: 0.0092\n",
      "Epoch [1/8], Step [688/750], Loss: 0.0637\n",
      "Epoch [1/8], Step [689/750], Loss: 0.0326\n",
      "Epoch [1/8], Step [690/750], Loss: 0.0153\n",
      "Epoch [1/8], Step [691/750], Loss: 0.0856\n",
      "Epoch [1/8], Step [692/750], Loss: 0.0128\n",
      "Epoch [1/8], Step [693/750], Loss: 0.0701\n",
      "Epoch [1/8], Step [694/750], Loss: 0.1860\n",
      "Epoch [1/8], Step [695/750], Loss: 0.0295\n",
      "Epoch [1/8], Step [696/750], Loss: 0.0419\n",
      "Epoch [1/8], Step [697/750], Loss: 0.0088\n",
      "Epoch [1/8], Step [698/750], Loss: 0.0970\n",
      "Epoch [1/8], Step [699/750], Loss: 0.0470\n",
      "Epoch [1/8], Step [700/750], Loss: 0.0804\n",
      "Epoch [1/8], Step [701/750], Loss: 0.0325\n",
      "Epoch [1/8], Step [702/750], Loss: 0.0481\n",
      "Epoch [1/8], Step [703/750], Loss: 0.0431\n",
      "Epoch [1/8], Step [704/750], Loss: 0.0297\n",
      "Epoch [1/8], Step [705/750], Loss: 0.0278\n",
      "Epoch [1/8], Step [706/750], Loss: 0.1533\n",
      "Epoch [1/8], Step [707/750], Loss: 0.0144\n",
      "Epoch [1/8], Step [708/750], Loss: 0.1329\n",
      "Epoch [1/8], Step [709/750], Loss: 0.1044\n",
      "Epoch [1/8], Step [710/750], Loss: 0.1045\n",
      "Epoch [1/8], Step [711/750], Loss: 0.1074\n",
      "Epoch [1/8], Step [712/750], Loss: 0.2159\n",
      "Epoch [1/8], Step [713/750], Loss: 0.0068\n",
      "Epoch [1/8], Step [714/750], Loss: 0.0163\n",
      "Epoch [1/8], Step [715/750], Loss: 0.0986\n",
      "Epoch [1/8], Step [716/750], Loss: 0.0278\n",
      "Epoch [1/8], Step [717/750], Loss: 0.2158\n",
      "Epoch [1/8], Step [718/750], Loss: 0.0673\n",
      "Epoch [1/8], Step [719/750], Loss: 0.1125\n",
      "Epoch [1/8], Step [720/750], Loss: 0.0849\n",
      "Epoch [1/8], Step [721/750], Loss: 0.1757\n",
      "Epoch [1/8], Step [722/750], Loss: 0.0398\n",
      "Epoch [1/8], Step [723/750], Loss: 0.0220\n",
      "Epoch [1/8], Step [724/750], Loss: 0.1144\n",
      "Epoch [1/8], Step [725/750], Loss: 0.0335\n",
      "Epoch [1/8], Step [726/750], Loss: 0.0625\n",
      "Epoch [1/8], Step [727/750], Loss: 0.0713\n",
      "Epoch [1/8], Step [728/750], Loss: 0.1469\n",
      "Epoch [1/8], Step [729/750], Loss: 0.0769\n",
      "Epoch [1/8], Step [730/750], Loss: 0.0124\n",
      "Epoch [1/8], Step [731/750], Loss: 0.1956\n",
      "Epoch [1/8], Step [732/750], Loss: 0.0580\n",
      "Epoch [1/8], Step [733/750], Loss: 0.0502\n",
      "Epoch [1/8], Step [734/750], Loss: 0.0279\n",
      "Epoch [1/8], Step [735/750], Loss: 0.0154\n",
      "Epoch [1/8], Step [736/750], Loss: 0.0242\n",
      "Epoch [1/8], Step [737/750], Loss: 0.0268\n",
      "Epoch [1/8], Step [738/750], Loss: 0.0643\n",
      "Epoch [1/8], Step [739/750], Loss: 0.0763\n",
      "Epoch [1/8], Step [740/750], Loss: 0.0377\n",
      "Epoch [1/8], Step [741/750], Loss: 0.0465\n",
      "Epoch [1/8], Step [742/750], Loss: 0.1406\n",
      "Epoch [1/8], Step [743/750], Loss: 0.1623\n",
      "Epoch [1/8], Step [744/750], Loss: 0.0273\n",
      "Epoch [1/8], Step [745/750], Loss: 0.0373\n",
      "Epoch [1/8], Step [746/750], Loss: 0.0721\n",
      "Epoch [1/8], Step [747/750], Loss: 0.0387\n",
      "Epoch [1/8], Step [748/750], Loss: 0.1954\n",
      "Epoch [1/8], Step [749/750], Loss: 0.0429\n",
      "Epoch [1/8], Step [750/750], Loss: 0.0293\n",
      "Epoch [1/8], Tr. loss: 0.1646. Test loss: 0.0560\n",
      "\n",
      "\n",
      "Epoch [2/8], Step [1/750], Loss: 0.0265\n",
      "Epoch [2/8], Step [2/750], Loss: 0.1331\n",
      "Epoch [2/8], Step [3/750], Loss: 0.0600\n",
      "Epoch [2/8], Step [4/750], Loss: 0.1437\n",
      "Epoch [2/8], Step [5/750], Loss: 0.1214\n",
      "Epoch [2/8], Step [6/750], Loss: 0.0379\n",
      "Epoch [2/8], Step [7/750], Loss: 0.0257\n",
      "Epoch [2/8], Step [8/750], Loss: 0.2233\n",
      "Epoch [2/8], Step [9/750], Loss: 0.0947\n",
      "Epoch [2/8], Step [10/750], Loss: 0.2321\n",
      "Epoch [2/8], Step [11/750], Loss: 0.0591\n",
      "Epoch [2/8], Step [12/750], Loss: 0.0768\n",
      "Epoch [2/8], Step [13/750], Loss: 0.0584\n",
      "Epoch [2/8], Step [14/750], Loss: 0.0850\n",
      "Epoch [2/8], Step [15/750], Loss: 0.0798\n",
      "Epoch [2/8], Step [16/750], Loss: 0.0893\n",
      "Epoch [2/8], Step [17/750], Loss: 0.0815\n",
      "Epoch [2/8], Step [18/750], Loss: 0.1289\n",
      "Epoch [2/8], Step [19/750], Loss: 0.2159\n",
      "Epoch [2/8], Step [20/750], Loss: 0.2279\n",
      "Epoch [2/8], Step [21/750], Loss: 0.1425\n",
      "Epoch [2/8], Step [22/750], Loss: 0.3005\n",
      "Epoch [2/8], Step [23/750], Loss: 0.0130\n",
      "Epoch [2/8], Step [24/750], Loss: 0.1238\n",
      "Epoch [2/8], Step [25/750], Loss: 0.1625\n",
      "Epoch [2/8], Step [26/750], Loss: 0.0794\n",
      "Epoch [2/8], Step [27/750], Loss: 0.0662\n",
      "Epoch [2/8], Step [28/750], Loss: 0.0991\n",
      "Epoch [2/8], Step [29/750], Loss: 0.1950\n",
      "Epoch [2/8], Step [30/750], Loss: 0.0339\n",
      "Epoch [2/8], Step [31/750], Loss: 0.1643\n",
      "Epoch [2/8], Step [32/750], Loss: 0.0562\n",
      "Epoch [2/8], Step [33/750], Loss: 0.1703\n",
      "Epoch [2/8], Step [34/750], Loss: 0.0829\n",
      "Epoch [2/8], Step [35/750], Loss: 0.0653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Step [36/750], Loss: 0.1470\n",
      "Epoch [2/8], Step [37/750], Loss: 0.0684\n",
      "Epoch [2/8], Step [38/750], Loss: 0.0539\n",
      "Epoch [2/8], Step [39/750], Loss: 0.1471\n",
      "Epoch [2/8], Step [40/750], Loss: 0.0276\n",
      "Epoch [2/8], Step [41/750], Loss: 0.0581\n",
      "Epoch [2/8], Step [42/750], Loss: 0.1342\n",
      "Epoch [2/8], Step [43/750], Loss: 0.0804\n",
      "Epoch [2/8], Step [44/750], Loss: 0.0580\n",
      "Epoch [2/8], Step [45/750], Loss: 0.1992\n",
      "Epoch [2/8], Step [46/750], Loss: 0.1404\n",
      "Epoch [2/8], Step [47/750], Loss: 0.0693\n",
      "Epoch [2/8], Step [48/750], Loss: 0.1505\n",
      "Epoch [2/8], Step [49/750], Loss: 0.0313\n",
      "Epoch [2/8], Step [50/750], Loss: 0.0447\n",
      "Epoch [2/8], Step [51/750], Loss: 0.0834\n",
      "Epoch [2/8], Step [52/750], Loss: 0.0608\n",
      "Epoch [2/8], Step [53/750], Loss: 0.1206\n",
      "Epoch [2/8], Step [54/750], Loss: 0.0168\n",
      "Epoch [2/8], Step [55/750], Loss: 0.1483\n",
      "Epoch [2/8], Step [56/750], Loss: 0.0964\n",
      "Epoch [2/8], Step [57/750], Loss: 0.0704\n",
      "Epoch [2/8], Step [58/750], Loss: 0.0089\n",
      "Epoch [2/8], Step [59/750], Loss: 0.0358\n",
      "Epoch [2/8], Step [60/750], Loss: 0.0346\n",
      "Epoch [2/8], Step [61/750], Loss: 0.0460\n",
      "Epoch [2/8], Step [62/750], Loss: 0.1023\n",
      "Epoch [2/8], Step [63/750], Loss: 0.0528\n",
      "Epoch [2/8], Step [64/750], Loss: 0.0690\n",
      "Epoch [2/8], Step [65/750], Loss: 0.0066\n",
      "Epoch [2/8], Step [66/750], Loss: 0.1461\n",
      "Epoch [2/8], Step [67/750], Loss: 0.0316\n",
      "Epoch [2/8], Step [68/750], Loss: 0.0353\n",
      "Epoch [2/8], Step [69/750], Loss: 0.1072\n",
      "Epoch [2/8], Step [70/750], Loss: 0.0860\n",
      "Epoch [2/8], Step [71/750], Loss: 0.0576\n",
      "Epoch [2/8], Step [72/750], Loss: 0.0242\n",
      "Epoch [2/8], Step [73/750], Loss: 0.0474\n",
      "Epoch [2/8], Step [74/750], Loss: 0.0150\n",
      "Epoch [2/8], Step [75/750], Loss: 0.0260\n",
      "Epoch [2/8], Step [76/750], Loss: 0.1587\n",
      "Epoch [2/8], Step [77/750], Loss: 0.0833\n",
      "Epoch [2/8], Step [78/750], Loss: 0.0425\n",
      "Epoch [2/8], Step [79/750], Loss: 0.0873\n",
      "Epoch [2/8], Step [80/750], Loss: 0.0945\n",
      "Epoch [2/8], Step [81/750], Loss: 0.0999\n",
      "Epoch [2/8], Step [82/750], Loss: 0.1082\n",
      "Epoch [2/8], Step [83/750], Loss: 0.0559\n",
      "Epoch [2/8], Step [84/750], Loss: 0.1314\n",
      "Epoch [2/8], Step [85/750], Loss: 0.0197\n",
      "Epoch [2/8], Step [86/750], Loss: 0.0175\n",
      "Epoch [2/8], Step [87/750], Loss: 0.0782\n",
      "Epoch [2/8], Step [88/750], Loss: 0.0477\n",
      "Epoch [2/8], Step [89/750], Loss: 0.0988\n",
      "Epoch [2/8], Step [90/750], Loss: 0.0257\n",
      "Epoch [2/8], Step [91/750], Loss: 0.0704\n",
      "Epoch [2/8], Step [92/750], Loss: 0.1712\n",
      "Epoch [2/8], Step [93/750], Loss: 0.1503\n",
      "Epoch [2/8], Step [94/750], Loss: 0.2233\n",
      "Epoch [2/8], Step [95/750], Loss: 0.0424\n",
      "Epoch [2/8], Step [96/750], Loss: 0.0868\n",
      "Epoch [2/8], Step [97/750], Loss: 0.0960\n",
      "Epoch [2/8], Step [98/750], Loss: 0.0583\n",
      "Epoch [2/8], Step [99/750], Loss: 0.0142\n",
      "Epoch [2/8], Step [100/750], Loss: 0.0795\n",
      "Epoch [2/8], Step [101/750], Loss: 0.0680\n",
      "Epoch [2/8], Step [102/750], Loss: 0.0787\n",
      "Epoch [2/8], Step [103/750], Loss: 0.0319\n",
      "Epoch [2/8], Step [104/750], Loss: 0.0578\n",
      "Epoch [2/8], Step [105/750], Loss: 0.1072\n",
      "Epoch [2/8], Step [106/750], Loss: 0.0739\n",
      "Epoch [2/8], Step [107/750], Loss: 0.1434\n",
      "Epoch [2/8], Step [108/750], Loss: 0.1320\n",
      "Epoch [2/8], Step [109/750], Loss: 0.0421\n",
      "Epoch [2/8], Step [110/750], Loss: 0.0609\n",
      "Epoch [2/8], Step [111/750], Loss: 0.0235\n",
      "Epoch [2/8], Step [112/750], Loss: 0.0349\n",
      "Epoch [2/8], Step [113/750], Loss: 0.0342\n",
      "Epoch [2/8], Step [114/750], Loss: 0.0884\n",
      "Epoch [2/8], Step [115/750], Loss: 0.0142\n",
      "Epoch [2/8], Step [116/750], Loss: 0.1092\n",
      "Epoch [2/8], Step [117/750], Loss: 0.1158\n",
      "Epoch [2/8], Step [118/750], Loss: 0.0294\n",
      "Epoch [2/8], Step [119/750], Loss: 0.0911\n",
      "Epoch [2/8], Step [120/750], Loss: 0.0446\n",
      "Epoch [2/8], Step [121/750], Loss: 0.0910\n",
      "Epoch [2/8], Step [122/750], Loss: 0.0079\n",
      "Epoch [2/8], Step [123/750], Loss: 0.1226\n",
      "Epoch [2/8], Step [124/750], Loss: 0.0313\n",
      "Epoch [2/8], Step [125/750], Loss: 0.0688\n",
      "Epoch [2/8], Step [126/750], Loss: 0.0988\n",
      "Epoch [2/8], Step [127/750], Loss: 0.0938\n",
      "Epoch [2/8], Step [128/750], Loss: 0.0439\n",
      "Epoch [2/8], Step [129/750], Loss: 0.0292\n",
      "Epoch [2/8], Step [130/750], Loss: 0.0632\n",
      "Epoch [2/8], Step [131/750], Loss: 0.0684\n",
      "Epoch [2/8], Step [132/750], Loss: 0.0196\n",
      "Epoch [2/8], Step [133/750], Loss: 0.0695\n",
      "Epoch [2/8], Step [134/750], Loss: 0.0430\n",
      "Epoch [2/8], Step [135/750], Loss: 0.0341\n",
      "Epoch [2/8], Step [136/750], Loss: 0.0131\n",
      "Epoch [2/8], Step [137/750], Loss: 0.0707\n",
      "Epoch [2/8], Step [138/750], Loss: 0.0084\n",
      "Epoch [2/8], Step [139/750], Loss: 0.0078\n",
      "Epoch [2/8], Step [140/750], Loss: 0.0951\n",
      "Epoch [2/8], Step [141/750], Loss: 0.0587\n",
      "Epoch [2/8], Step [142/750], Loss: 0.0117\n",
      "Epoch [2/8], Step [143/750], Loss: 0.0196\n",
      "Epoch [2/8], Step [144/750], Loss: 0.0360\n",
      "Epoch [2/8], Step [145/750], Loss: 0.0055\n",
      "Epoch [2/8], Step [146/750], Loss: 0.0262\n",
      "Epoch [2/8], Step [147/750], Loss: 0.1750\n",
      "Epoch [2/8], Step [148/750], Loss: 0.0641\n",
      "Epoch [2/8], Step [149/750], Loss: 0.0671\n",
      "Epoch [2/8], Step [150/750], Loss: 0.0920\n",
      "Epoch [2/8], Step [151/750], Loss: 0.0278\n",
      "Epoch [2/8], Step [152/750], Loss: 0.0856\n",
      "Epoch [2/8], Step [153/750], Loss: 0.0751\n",
      "Epoch [2/8], Step [154/750], Loss: 0.1148\n",
      "Epoch [2/8], Step [155/750], Loss: 0.1518\n",
      "Epoch [2/8], Step [156/750], Loss: 0.0121\n",
      "Epoch [2/8], Step [157/750], Loss: 0.0160\n",
      "Epoch [2/8], Step [158/750], Loss: 0.0223\n",
      "Epoch [2/8], Step [159/750], Loss: 0.0424\n",
      "Epoch [2/8], Step [160/750], Loss: 0.0189\n",
      "Epoch [2/8], Step [161/750], Loss: 0.0082\n",
      "Epoch [2/8], Step [162/750], Loss: 0.1076\n",
      "Epoch [2/8], Step [163/750], Loss: 0.0148\n",
      "Epoch [2/8], Step [164/750], Loss: 0.0530\n",
      "Epoch [2/8], Step [165/750], Loss: 0.0124\n",
      "Epoch [2/8], Step [166/750], Loss: 0.0545\n",
      "Epoch [2/8], Step [167/750], Loss: 0.0229\n",
      "Epoch [2/8], Step [168/750], Loss: 0.0554\n",
      "Epoch [2/8], Step [169/750], Loss: 0.0062\n",
      "Epoch [2/8], Step [170/750], Loss: 0.0539\n",
      "Epoch [2/8], Step [171/750], Loss: 0.1871\n",
      "Epoch [2/8], Step [172/750], Loss: 0.1098\n",
      "Epoch [2/8], Step [173/750], Loss: 0.0040\n",
      "Epoch [2/8], Step [174/750], Loss: 0.1135\n",
      "Epoch [2/8], Step [175/750], Loss: 0.0045\n",
      "Epoch [2/8], Step [176/750], Loss: 0.0435\n",
      "Epoch [2/8], Step [177/750], Loss: 0.0029\n",
      "Epoch [2/8], Step [178/750], Loss: 0.0636\n",
      "Epoch [2/8], Step [179/750], Loss: 0.0435\n",
      "Epoch [2/8], Step [180/750], Loss: 0.1167\n",
      "Epoch [2/8], Step [181/750], Loss: 0.0406\n",
      "Epoch [2/8], Step [182/750], Loss: 0.2340\n",
      "Epoch [2/8], Step [183/750], Loss: 0.0241\n",
      "Epoch [2/8], Step [184/750], Loss: 0.0066\n",
      "Epoch [2/8], Step [185/750], Loss: 0.0604\n",
      "Epoch [2/8], Step [186/750], Loss: 0.0351\n",
      "Epoch [2/8], Step [187/750], Loss: 0.1164\n",
      "Epoch [2/8], Step [188/750], Loss: 0.0480\n",
      "Epoch [2/8], Step [189/750], Loss: 0.0599\n",
      "Epoch [2/8], Step [190/750], Loss: 0.1732\n",
      "Epoch [2/8], Step [191/750], Loss: 0.0126\n",
      "Epoch [2/8], Step [192/750], Loss: 0.0633\n",
      "Epoch [2/8], Step [193/750], Loss: 0.0281\n",
      "Epoch [2/8], Step [194/750], Loss: 0.0871\n",
      "Epoch [2/8], Step [195/750], Loss: 0.0883\n",
      "Epoch [2/8], Step [196/750], Loss: 0.0550\n",
      "Epoch [2/8], Step [197/750], Loss: 0.0249\n",
      "Epoch [2/8], Step [198/750], Loss: 0.0121\n",
      "Epoch [2/8], Step [199/750], Loss: 0.0135\n",
      "Epoch [2/8], Step [200/750], Loss: 0.1163\n",
      "Epoch [2/8], Step [201/750], Loss: 0.1109\n",
      "Epoch [2/8], Step [202/750], Loss: 0.0613\n",
      "Epoch [2/8], Step [203/750], Loss: 0.0407\n",
      "Epoch [2/8], Step [204/750], Loss: 0.1306\n",
      "Epoch [2/8], Step [205/750], Loss: 0.1929\n",
      "Epoch [2/8], Step [206/750], Loss: 0.1657\n",
      "Epoch [2/8], Step [207/750], Loss: 0.0881\n",
      "Epoch [2/8], Step [208/750], Loss: 0.0181\n",
      "Epoch [2/8], Step [209/750], Loss: 0.0571\n",
      "Epoch [2/8], Step [210/750], Loss: 0.1345\n",
      "Epoch [2/8], Step [211/750], Loss: 0.0578\n",
      "Epoch [2/8], Step [212/750], Loss: 0.0759\n",
      "Epoch [2/8], Step [213/750], Loss: 0.0723\n",
      "Epoch [2/8], Step [214/750], Loss: 0.0289\n",
      "Epoch [2/8], Step [215/750], Loss: 0.0967\n",
      "Epoch [2/8], Step [216/750], Loss: 0.0577\n",
      "Epoch [2/8], Step [217/750], Loss: 0.0417\n",
      "Epoch [2/8], Step [218/750], Loss: 0.0409\n",
      "Epoch [2/8], Step [219/750], Loss: 0.0408\n",
      "Epoch [2/8], Step [220/750], Loss: 0.0379\n",
      "Epoch [2/8], Step [221/750], Loss: 0.0682\n",
      "Epoch [2/8], Step [222/750], Loss: 0.0353\n",
      "Epoch [2/8], Step [223/750], Loss: 0.1147\n",
      "Epoch [2/8], Step [224/750], Loss: 0.0128\n",
      "Epoch [2/8], Step [225/750], Loss: 0.0748\n",
      "Epoch [2/8], Step [226/750], Loss: 0.0184\n",
      "Epoch [2/8], Step [227/750], Loss: 0.0423\n",
      "Epoch [2/8], Step [228/750], Loss: 0.0460\n",
      "Epoch [2/8], Step [229/750], Loss: 0.0054\n",
      "Epoch [2/8], Step [230/750], Loss: 0.0461\n",
      "Epoch [2/8], Step [231/750], Loss: 0.0341\n",
      "Epoch [2/8], Step [232/750], Loss: 0.0444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Step [233/750], Loss: 0.0417\n",
      "Epoch [2/8], Step [234/750], Loss: 0.0089\n",
      "Epoch [2/8], Step [235/750], Loss: 0.0930\n",
      "Epoch [2/8], Step [236/750], Loss: 0.0242\n",
      "Epoch [2/8], Step [237/750], Loss: 0.0251\n",
      "Epoch [2/8], Step [238/750], Loss: 0.1216\n",
      "Epoch [2/8], Step [239/750], Loss: 0.0156\n",
      "Epoch [2/8], Step [240/750], Loss: 0.0041\n",
      "Epoch [2/8], Step [241/750], Loss: 0.0065\n",
      "Epoch [2/8], Step [242/750], Loss: 0.0331\n",
      "Epoch [2/8], Step [243/750], Loss: 0.1642\n",
      "Epoch [2/8], Step [244/750], Loss: 0.0748\n",
      "Epoch [2/8], Step [245/750], Loss: 0.1145\n",
      "Epoch [2/8], Step [246/750], Loss: 0.0209\n",
      "Epoch [2/8], Step [247/750], Loss: 0.0493\n",
      "Epoch [2/8], Step [248/750], Loss: 0.2000\n",
      "Epoch [2/8], Step [249/750], Loss: 0.1008\n",
      "Epoch [2/8], Step [250/750], Loss: 0.0989\n",
      "Epoch [2/8], Step [251/750], Loss: 0.1252\n",
      "Epoch [2/8], Step [252/750], Loss: 0.0962\n",
      "Epoch [2/8], Step [253/750], Loss: 0.0561\n",
      "Epoch [2/8], Step [254/750], Loss: 0.0536\n",
      "Epoch [2/8], Step [255/750], Loss: 0.0734\n",
      "Epoch [2/8], Step [256/750], Loss: 0.0622\n",
      "Epoch [2/8], Step [257/750], Loss: 0.0458\n",
      "Epoch [2/8], Step [258/750], Loss: 0.0446\n",
      "Epoch [2/8], Step [259/750], Loss: 0.0586\n",
      "Epoch [2/8], Step [260/750], Loss: 0.3420\n",
      "Epoch [2/8], Step [261/750], Loss: 0.0260\n",
      "Epoch [2/8], Step [262/750], Loss: 0.1227\n",
      "Epoch [2/8], Step [263/750], Loss: 0.0361\n",
      "Epoch [2/8], Step [264/750], Loss: 0.0529\n",
      "Epoch [2/8], Step [265/750], Loss: 0.0364\n",
      "Epoch [2/8], Step [266/750], Loss: 0.0744\n",
      "Epoch [2/8], Step [267/750], Loss: 0.0729\n",
      "Epoch [2/8], Step [268/750], Loss: 0.1186\n",
      "Epoch [2/8], Step [269/750], Loss: 0.1035\n",
      "Epoch [2/8], Step [270/750], Loss: 0.0069\n",
      "Epoch [2/8], Step [271/750], Loss: 0.0340\n",
      "Epoch [2/8], Step [272/750], Loss: 0.0709\n",
      "Epoch [2/8], Step [273/750], Loss: 0.0277\n",
      "Epoch [2/8], Step [274/750], Loss: 0.0170\n",
      "Epoch [2/8], Step [275/750], Loss: 0.0621\n",
      "Epoch [2/8], Step [276/750], Loss: 0.0524\n",
      "Epoch [2/8], Step [277/750], Loss: 0.0432\n",
      "Epoch [2/8], Step [278/750], Loss: 0.0910\n",
      "Epoch [2/8], Step [279/750], Loss: 0.0178\n",
      "Epoch [2/8], Step [280/750], Loss: 0.1502\n",
      "Epoch [2/8], Step [281/750], Loss: 0.0880\n",
      "Epoch [2/8], Step [282/750], Loss: 0.0348\n",
      "Epoch [2/8], Step [283/750], Loss: 0.0186\n",
      "Epoch [2/8], Step [284/750], Loss: 0.0256\n",
      "Epoch [2/8], Step [285/750], Loss: 0.1764\n",
      "Epoch [2/8], Step [286/750], Loss: 0.0436\n",
      "Epoch [2/8], Step [287/750], Loss: 0.0157\n",
      "Epoch [2/8], Step [288/750], Loss: 0.0397\n",
      "Epoch [2/8], Step [289/750], Loss: 0.0367\n",
      "Epoch [2/8], Step [290/750], Loss: 0.0502\n",
      "Epoch [2/8], Step [291/750], Loss: 0.0881\n",
      "Epoch [2/8], Step [292/750], Loss: 0.0046\n",
      "Epoch [2/8], Step [293/750], Loss: 0.0185\n",
      "Epoch [2/8], Step [294/750], Loss: 0.0039\n",
      "Epoch [2/8], Step [295/750], Loss: 0.0308\n",
      "Epoch [2/8], Step [296/750], Loss: 0.0132\n",
      "Epoch [2/8], Step [297/750], Loss: 0.0192\n",
      "Epoch [2/8], Step [298/750], Loss: 0.0089\n",
      "Epoch [2/8], Step [299/750], Loss: 0.0421\n",
      "Epoch [2/8], Step [300/750], Loss: 0.0381\n",
      "Epoch [2/8], Step [301/750], Loss: 0.0117\n",
      "Epoch [2/8], Step [302/750], Loss: 0.0601\n",
      "Epoch [2/8], Step [303/750], Loss: 0.0137\n",
      "Epoch [2/8], Step [304/750], Loss: 0.0099\n",
      "Epoch [2/8], Step [305/750], Loss: 0.0282\n",
      "Epoch [2/8], Step [306/750], Loss: 0.1012\n",
      "Epoch [2/8], Step [307/750], Loss: 0.0126\n",
      "Epoch [2/8], Step [308/750], Loss: 0.0103\n",
      "Epoch [2/8], Step [309/750], Loss: 0.0105\n",
      "Epoch [2/8], Step [310/750], Loss: 0.0426\n",
      "Epoch [2/8], Step [311/750], Loss: 0.1185\n",
      "Epoch [2/8], Step [312/750], Loss: 0.0316\n",
      "Epoch [2/8], Step [313/750], Loss: 0.1756\n",
      "Epoch [2/8], Step [314/750], Loss: 0.0340\n",
      "Epoch [2/8], Step [315/750], Loss: 0.0118\n",
      "Epoch [2/8], Step [316/750], Loss: 0.0093\n",
      "Epoch [2/8], Step [317/750], Loss: 0.0330\n",
      "Epoch [2/8], Step [318/750], Loss: 0.0026\n",
      "Epoch [2/8], Step [319/750], Loss: 0.1526\n",
      "Epoch [2/8], Step [320/750], Loss: 0.0392\n",
      "Epoch [2/8], Step [321/750], Loss: 0.0058\n",
      "Epoch [2/8], Step [322/750], Loss: 0.0062\n",
      "Epoch [2/8], Step [323/750], Loss: 0.0212\n",
      "Epoch [2/8], Step [324/750], Loss: 0.1936\n",
      "Epoch [2/8], Step [325/750], Loss: 0.0228\n",
      "Epoch [2/8], Step [326/750], Loss: 0.0044\n",
      "Epoch [2/8], Step [327/750], Loss: 0.0025\n",
      "Epoch [2/8], Step [328/750], Loss: 0.0041\n",
      "Epoch [2/8], Step [329/750], Loss: 0.0523\n",
      "Epoch [2/8], Step [330/750], Loss: 0.0273\n",
      "Epoch [2/8], Step [331/750], Loss: 0.0192\n",
      "Epoch [2/8], Step [332/750], Loss: 0.0504\n",
      "Epoch [2/8], Step [333/750], Loss: 0.0672\n",
      "Epoch [2/8], Step [334/750], Loss: 0.1242\n",
      "Epoch [2/8], Step [335/750], Loss: 0.0342\n",
      "Epoch [2/8], Step [336/750], Loss: 0.0880\n",
      "Epoch [2/8], Step [337/750], Loss: 0.0419\n",
      "Epoch [2/8], Step [338/750], Loss: 0.0412\n",
      "Epoch [2/8], Step [339/750], Loss: 0.0448\n",
      "Epoch [2/8], Step [340/750], Loss: 0.0578\n",
      "Epoch [2/8], Step [341/750], Loss: 0.0356\n",
      "Epoch [2/8], Step [342/750], Loss: 0.0860\n",
      "Epoch [2/8], Step [343/750], Loss: 0.0070\n",
      "Epoch [2/8], Step [344/750], Loss: 0.1275\n",
      "Epoch [2/8], Step [345/750], Loss: 0.0903\n",
      "Epoch [2/8], Step [346/750], Loss: 0.1252\n",
      "Epoch [2/8], Step [347/750], Loss: 0.0739\n",
      "Epoch [2/8], Step [348/750], Loss: 0.0448\n",
      "Epoch [2/8], Step [349/750], Loss: 0.0892\n",
      "Epoch [2/8], Step [350/750], Loss: 0.1168\n",
      "Epoch [2/8], Step [351/750], Loss: 0.0630\n",
      "Epoch [2/8], Step [352/750], Loss: 0.0358\n",
      "Epoch [2/8], Step [353/750], Loss: 0.1148\n",
      "Epoch [2/8], Step [354/750], Loss: 0.0601\n",
      "Epoch [2/8], Step [355/750], Loss: 0.0566\n",
      "Epoch [2/8], Step [356/750], Loss: 0.1245\n",
      "Epoch [2/8], Step [357/750], Loss: 0.0134\n",
      "Epoch [2/8], Step [358/750], Loss: 0.0110\n",
      "Epoch [2/8], Step [359/750], Loss: 0.0624\n",
      "Epoch [2/8], Step [360/750], Loss: 0.0401\n",
      "Epoch [2/8], Step [361/750], Loss: 0.0318\n",
      "Epoch [2/8], Step [362/750], Loss: 0.0129\n",
      "Epoch [2/8], Step [363/750], Loss: 0.0186\n",
      "Epoch [2/8], Step [364/750], Loss: 0.0310\n",
      "Epoch [2/8], Step [365/750], Loss: 0.0489\n",
      "Epoch [2/8], Step [366/750], Loss: 0.0825\n",
      "Epoch [2/8], Step [367/750], Loss: 0.0856\n",
      "Epoch [2/8], Step [368/750], Loss: 0.0682\n",
      "Epoch [2/8], Step [369/750], Loss: 0.1235\n",
      "Epoch [2/8], Step [370/750], Loss: 0.0611\n",
      "Epoch [2/8], Step [371/750], Loss: 0.0426\n",
      "Epoch [2/8], Step [372/750], Loss: 0.0154\n",
      "Epoch [2/8], Step [373/750], Loss: 0.0527\n",
      "Epoch [2/8], Step [374/750], Loss: 0.0822\n",
      "Epoch [2/8], Step [375/750], Loss: 0.0298\n",
      "Epoch [2/8], Step [376/750], Loss: 0.1808\n",
      "Epoch [2/8], Step [377/750], Loss: 0.0440\n",
      "Epoch [2/8], Step [378/750], Loss: 0.0589\n",
      "Epoch [2/8], Step [379/750], Loss: 0.0356\n",
      "Epoch [2/8], Step [380/750], Loss: 0.0485\n",
      "Epoch [2/8], Step [381/750], Loss: 0.0194\n",
      "Epoch [2/8], Step [382/750], Loss: 0.0657\n",
      "Epoch [2/8], Step [383/750], Loss: 0.0312\n",
      "Epoch [2/8], Step [384/750], Loss: 0.0500\n",
      "Epoch [2/8], Step [385/750], Loss: 0.0848\n",
      "Epoch [2/8], Step [386/750], Loss: 0.1736\n",
      "Epoch [2/8], Step [387/750], Loss: 0.0119\n",
      "Epoch [2/8], Step [388/750], Loss: 0.0345\n",
      "Epoch [2/8], Step [389/750], Loss: 0.0332\n",
      "Epoch [2/8], Step [390/750], Loss: 0.0824\n",
      "Epoch [2/8], Step [391/750], Loss: 0.0128\n",
      "Epoch [2/8], Step [392/750], Loss: 0.0190\n",
      "Epoch [2/8], Step [393/750], Loss: 0.0138\n",
      "Epoch [2/8], Step [394/750], Loss: 0.1102\n",
      "Epoch [2/8], Step [395/750], Loss: 0.0315\n",
      "Epoch [2/8], Step [396/750], Loss: 0.0272\n",
      "Epoch [2/8], Step [397/750], Loss: 0.0577\n",
      "Epoch [2/8], Step [398/750], Loss: 0.0397\n",
      "Epoch [2/8], Step [399/750], Loss: 0.0386\n",
      "Epoch [2/8], Step [400/750], Loss: 0.0890\n",
      "Epoch [2/8], Step [401/750], Loss: 0.1435\n",
      "Epoch [2/8], Step [402/750], Loss: 0.0045\n",
      "Epoch [2/8], Step [403/750], Loss: 0.0220\n",
      "Epoch [2/8], Step [404/750], Loss: 0.1683\n",
      "Epoch [2/8], Step [405/750], Loss: 0.0428\n",
      "Epoch [2/8], Step [406/750], Loss: 0.0859\n",
      "Epoch [2/8], Step [407/750], Loss: 0.0444\n",
      "Epoch [2/8], Step [408/750], Loss: 0.0358\n",
      "Epoch [2/8], Step [409/750], Loss: 0.0059\n",
      "Epoch [2/8], Step [410/750], Loss: 0.0152\n",
      "Epoch [2/8], Step [411/750], Loss: 0.0829\n",
      "Epoch [2/8], Step [412/750], Loss: 0.0481\n",
      "Epoch [2/8], Step [413/750], Loss: 0.0235\n",
      "Epoch [2/8], Step [414/750], Loss: 0.1109\n",
      "Epoch [2/8], Step [415/750], Loss: 0.0163\n",
      "Epoch [2/8], Step [416/750], Loss: 0.1035\n",
      "Epoch [2/8], Step [417/750], Loss: 0.0514\n",
      "Epoch [2/8], Step [418/750], Loss: 0.0437\n",
      "Epoch [2/8], Step [419/750], Loss: 0.0341\n",
      "Epoch [2/8], Step [420/750], Loss: 0.0433\n",
      "Epoch [2/8], Step [421/750], Loss: 0.0881\n",
      "Epoch [2/8], Step [422/750], Loss: 0.0928\n",
      "Epoch [2/8], Step [423/750], Loss: 0.0211\n",
      "Epoch [2/8], Step [424/750], Loss: 0.0525\n",
      "Epoch [2/8], Step [425/750], Loss: 0.0158\n",
      "Epoch [2/8], Step [426/750], Loss: 0.0169\n",
      "Epoch [2/8], Step [427/750], Loss: 0.0194\n",
      "Epoch [2/8], Step [428/750], Loss: 0.0241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Step [429/750], Loss: 0.0136\n",
      "Epoch [2/8], Step [430/750], Loss: 0.0913\n",
      "Epoch [2/8], Step [431/750], Loss: 0.1025\n",
      "Epoch [2/8], Step [432/750], Loss: 0.0150\n",
      "Epoch [2/8], Step [433/750], Loss: 0.0161\n",
      "Epoch [2/8], Step [434/750], Loss: 0.2155\n",
      "Epoch [2/8], Step [435/750], Loss: 0.1092\n",
      "Epoch [2/8], Step [436/750], Loss: 0.0201\n",
      "Epoch [2/8], Step [437/750], Loss: 0.0078\n",
      "Epoch [2/8], Step [438/750], Loss: 0.0817\n",
      "Epoch [2/8], Step [439/750], Loss: 0.0783\n",
      "Epoch [2/8], Step [440/750], Loss: 0.0323\n",
      "Epoch [2/8], Step [441/750], Loss: 0.1991\n",
      "Epoch [2/8], Step [442/750], Loss: 0.0332\n",
      "Epoch [2/8], Step [443/750], Loss: 0.0557\n",
      "Epoch [2/8], Step [444/750], Loss: 0.0109\n",
      "Epoch [2/8], Step [445/750], Loss: 0.0869\n",
      "Epoch [2/8], Step [446/750], Loss: 0.0722\n",
      "Epoch [2/8], Step [447/750], Loss: 0.0955\n",
      "Epoch [2/8], Step [448/750], Loss: 0.0209\n",
      "Epoch [2/8], Step [449/750], Loss: 0.0247\n",
      "Epoch [2/8], Step [450/750], Loss: 0.0079\n",
      "Epoch [2/8], Step [451/750], Loss: 0.1019\n",
      "Epoch [2/8], Step [452/750], Loss: 0.0745\n",
      "Epoch [2/8], Step [453/750], Loss: 0.0562\n",
      "Epoch [2/8], Step [454/750], Loss: 0.0162\n",
      "Epoch [2/8], Step [455/750], Loss: 0.0685\n",
      "Epoch [2/8], Step [456/750], Loss: 0.0718\n",
      "Epoch [2/8], Step [457/750], Loss: 0.1460\n",
      "Epoch [2/8], Step [458/750], Loss: 0.0098\n",
      "Epoch [2/8], Step [459/750], Loss: 0.0208\n",
      "Epoch [2/8], Step [460/750], Loss: 0.0281\n",
      "Epoch [2/8], Step [461/750], Loss: 0.0574\n",
      "Epoch [2/8], Step [462/750], Loss: 0.0397\n",
      "Epoch [2/8], Step [463/750], Loss: 0.0410\n",
      "Epoch [2/8], Step [464/750], Loss: 0.0832\n",
      "Epoch [2/8], Step [465/750], Loss: 0.0525\n",
      "Epoch [2/8], Step [466/750], Loss: 0.0707\n",
      "Epoch [2/8], Step [467/750], Loss: 0.0254\n",
      "Epoch [2/8], Step [468/750], Loss: 0.0702\n",
      "Epoch [2/8], Step [469/750], Loss: 0.1277\n",
      "Epoch [2/8], Step [470/750], Loss: 0.1625\n",
      "Epoch [2/8], Step [471/750], Loss: 0.0060\n",
      "Epoch [2/8], Step [472/750], Loss: 0.0397\n",
      "Epoch [2/8], Step [473/750], Loss: 0.0146\n",
      "Epoch [2/8], Step [474/750], Loss: 0.0758\n",
      "Epoch [2/8], Step [475/750], Loss: 0.0383\n",
      "Epoch [2/8], Step [476/750], Loss: 0.1330\n",
      "Epoch [2/8], Step [477/750], Loss: 0.0647\n",
      "Epoch [2/8], Step [478/750], Loss: 0.1339\n",
      "Epoch [2/8], Step [479/750], Loss: 0.0274\n",
      "Epoch [2/8], Step [480/750], Loss: 0.0334\n",
      "Epoch [2/8], Step [481/750], Loss: 0.2215\n",
      "Epoch [2/8], Step [482/750], Loss: 0.0963\n",
      "Epoch [2/8], Step [483/750], Loss: 0.1099\n",
      "Epoch [2/8], Step [484/750], Loss: 0.0599\n",
      "Epoch [2/8], Step [485/750], Loss: 0.0908\n",
      "Epoch [2/8], Step [486/750], Loss: 0.0097\n",
      "Epoch [2/8], Step [487/750], Loss: 0.0601\n",
      "Epoch [2/8], Step [488/750], Loss: 0.1284\n",
      "Epoch [2/8], Step [489/750], Loss: 0.1321\n",
      "Epoch [2/8], Step [490/750], Loss: 0.0872\n",
      "Epoch [2/8], Step [491/750], Loss: 0.0237\n",
      "Epoch [2/8], Step [492/750], Loss: 0.0200\n",
      "Epoch [2/8], Step [493/750], Loss: 0.1129\n",
      "Epoch [2/8], Step [494/750], Loss: 0.0094\n",
      "Epoch [2/8], Step [495/750], Loss: 0.0075\n",
      "Epoch [2/8], Step [496/750], Loss: 0.0561\n",
      "Epoch [2/8], Step [497/750], Loss: 0.0040\n",
      "Epoch [2/8], Step [498/750], Loss: 0.0690\n",
      "Epoch [2/8], Step [499/750], Loss: 0.0601\n",
      "Epoch [2/8], Step [500/750], Loss: 0.0159\n",
      "Epoch [2/8], Step [501/750], Loss: 0.1165\n",
      "Epoch [2/8], Step [502/750], Loss: 0.2058\n",
      "Epoch [2/8], Step [503/750], Loss: 0.0120\n",
      "Epoch [2/8], Step [504/750], Loss: 0.0352\n",
      "Epoch [2/8], Step [505/750], Loss: 0.0549\n",
      "Epoch [2/8], Step [506/750], Loss: 0.0783\n",
      "Epoch [2/8], Step [507/750], Loss: 0.0792\n",
      "Epoch [2/8], Step [508/750], Loss: 0.0340\n",
      "Epoch [2/8], Step [509/750], Loss: 0.0037\n",
      "Epoch [2/8], Step [510/750], Loss: 0.0642\n",
      "Epoch [2/8], Step [511/750], Loss: 0.1011\n",
      "Epoch [2/8], Step [512/750], Loss: 0.0716\n",
      "Epoch [2/8], Step [513/750], Loss: 0.0472\n",
      "Epoch [2/8], Step [514/750], Loss: 0.0118\n",
      "Epoch [2/8], Step [515/750], Loss: 0.0549\n",
      "Epoch [2/8], Step [516/750], Loss: 0.0146\n",
      "Epoch [2/8], Step [517/750], Loss: 0.0682\n",
      "Epoch [2/8], Step [518/750], Loss: 0.0508\n",
      "Epoch [2/8], Step [519/750], Loss: 0.0217\n",
      "Epoch [2/8], Step [520/750], Loss: 0.0108\n",
      "Epoch [2/8], Step [521/750], Loss: 0.0582\n",
      "Epoch [2/8], Step [522/750], Loss: 0.0059\n",
      "Epoch [2/8], Step [523/750], Loss: 0.0657\n",
      "Epoch [2/8], Step [524/750], Loss: 0.0639\n",
      "Epoch [2/8], Step [525/750], Loss: 0.2394\n",
      "Epoch [2/8], Step [526/750], Loss: 0.0572\n",
      "Epoch [2/8], Step [527/750], Loss: 0.0448\n",
      "Epoch [2/8], Step [528/750], Loss: 0.0698\n",
      "Epoch [2/8], Step [529/750], Loss: 0.0172\n",
      "Epoch [2/8], Step [530/750], Loss: 0.0612\n",
      "Epoch [2/8], Step [531/750], Loss: 0.0793\n",
      "Epoch [2/8], Step [532/750], Loss: 0.0189\n",
      "Epoch [2/8], Step [533/750], Loss: 0.0266\n",
      "Epoch [2/8], Step [534/750], Loss: 0.0287\n",
      "Epoch [2/8], Step [535/750], Loss: 0.0590\n",
      "Epoch [2/8], Step [536/750], Loss: 0.0705\n",
      "Epoch [2/8], Step [537/750], Loss: 0.0151\n",
      "Epoch [2/8], Step [538/750], Loss: 0.0152\n",
      "Epoch [2/8], Step [539/750], Loss: 0.0359\n",
      "Epoch [2/8], Step [540/750], Loss: 0.0893\n",
      "Epoch [2/8], Step [541/750], Loss: 0.0439\n",
      "Epoch [2/8], Step [542/750], Loss: 0.0142\n",
      "Epoch [2/8], Step [543/750], Loss: 0.0550\n",
      "Epoch [2/8], Step [544/750], Loss: 0.0495\n",
      "Epoch [2/8], Step [545/750], Loss: 0.0894\n",
      "Epoch [2/8], Step [546/750], Loss: 0.0539\n",
      "Epoch [2/8], Step [547/750], Loss: 0.1939\n",
      "Epoch [2/8], Step [548/750], Loss: 0.0591\n",
      "Epoch [2/8], Step [549/750], Loss: 0.0276\n",
      "Epoch [2/8], Step [550/750], Loss: 0.0187\n",
      "Epoch [2/8], Step [551/750], Loss: 0.1235\n",
      "Epoch [2/8], Step [552/750], Loss: 0.0347\n",
      "Epoch [2/8], Step [553/750], Loss: 0.0518\n",
      "Epoch [2/8], Step [554/750], Loss: 0.0508\n",
      "Epoch [2/8], Step [555/750], Loss: 0.0592\n",
      "Epoch [2/8], Step [556/750], Loss: 0.0326\n",
      "Epoch [2/8], Step [557/750], Loss: 0.2204\n",
      "Epoch [2/8], Step [558/750], Loss: 0.0097\n",
      "Epoch [2/8], Step [559/750], Loss: 0.0704\n",
      "Epoch [2/8], Step [560/750], Loss: 0.0354\n",
      "Epoch [2/8], Step [561/750], Loss: 0.0206\n",
      "Epoch [2/8], Step [562/750], Loss: 0.0262\n",
      "Epoch [2/8], Step [563/750], Loss: 0.0686\n",
      "Epoch [2/8], Step [564/750], Loss: 0.0956\n",
      "Epoch [2/8], Step [565/750], Loss: 0.0301\n",
      "Epoch [2/8], Step [566/750], Loss: 0.0670\n",
      "Epoch [2/8], Step [567/750], Loss: 0.1508\n",
      "Epoch [2/8], Step [568/750], Loss: 0.0251\n",
      "Epoch [2/8], Step [569/750], Loss: 0.0349\n",
      "Epoch [2/8], Step [570/750], Loss: 0.1299\n",
      "Epoch [2/8], Step [571/750], Loss: 0.0340\n",
      "Epoch [2/8], Step [572/750], Loss: 0.0694\n",
      "Epoch [2/8], Step [573/750], Loss: 0.1048\n",
      "Epoch [2/8], Step [574/750], Loss: 0.0435\n",
      "Epoch [2/8], Step [575/750], Loss: 0.0486\n",
      "Epoch [2/8], Step [576/750], Loss: 0.0184\n",
      "Epoch [2/8], Step [577/750], Loss: 0.0537\n",
      "Epoch [2/8], Step [578/750], Loss: 0.0556\n",
      "Epoch [2/8], Step [579/750], Loss: 0.1844\n",
      "Epoch [2/8], Step [580/750], Loss: 0.0544\n",
      "Epoch [2/8], Step [581/750], Loss: 0.1175\n",
      "Epoch [2/8], Step [582/750], Loss: 0.0074\n",
      "Epoch [2/8], Step [583/750], Loss: 0.1132\n",
      "Epoch [2/8], Step [584/750], Loss: 0.0814\n",
      "Epoch [2/8], Step [585/750], Loss: 0.0090\n",
      "Epoch [2/8], Step [586/750], Loss: 0.0208\n",
      "Epoch [2/8], Step [587/750], Loss: 0.0313\n",
      "Epoch [2/8], Step [588/750], Loss: 0.0182\n",
      "Epoch [2/8], Step [589/750], Loss: 0.0958\n",
      "Epoch [2/8], Step [590/750], Loss: 0.0741\n",
      "Epoch [2/8], Step [591/750], Loss: 0.0658\n",
      "Epoch [2/8], Step [592/750], Loss: 0.0387\n",
      "Epoch [2/8], Step [593/750], Loss: 0.1855\n",
      "Epoch [2/8], Step [594/750], Loss: 0.0057\n",
      "Epoch [2/8], Step [595/750], Loss: 0.0081\n",
      "Epoch [2/8], Step [596/750], Loss: 0.0317\n",
      "Epoch [2/8], Step [597/750], Loss: 0.1284\n",
      "Epoch [2/8], Step [598/750], Loss: 0.0367\n",
      "Epoch [2/8], Step [599/750], Loss: 0.0066\n",
      "Epoch [2/8], Step [600/750], Loss: 0.0738\n",
      "Epoch [2/8], Step [601/750], Loss: 0.0397\n",
      "Epoch [2/8], Step [602/750], Loss: 0.0750\n",
      "Epoch [2/8], Step [603/750], Loss: 0.1053\n",
      "Epoch [2/8], Step [604/750], Loss: 0.0345\n",
      "Epoch [2/8], Step [605/750], Loss: 0.0395\n",
      "Epoch [2/8], Step [606/750], Loss: 0.0367\n",
      "Epoch [2/8], Step [607/750], Loss: 0.0315\n",
      "Epoch [2/8], Step [608/750], Loss: 0.0162\n",
      "Epoch [2/8], Step [609/750], Loss: 0.2400\n",
      "Epoch [2/8], Step [610/750], Loss: 0.0134\n",
      "Epoch [2/8], Step [611/750], Loss: 0.0083\n",
      "Epoch [2/8], Step [612/750], Loss: 0.0212\n",
      "Epoch [2/8], Step [613/750], Loss: 0.0419\n",
      "Epoch [2/8], Step [614/750], Loss: 0.0078\n",
      "Epoch [2/8], Step [615/750], Loss: 0.0176\n",
      "Epoch [2/8], Step [616/750], Loss: 0.0236\n",
      "Epoch [2/8], Step [617/750], Loss: 0.0361\n",
      "Epoch [2/8], Step [618/750], Loss: 0.0383\n",
      "Epoch [2/8], Step [619/750], Loss: 0.0081\n",
      "Epoch [2/8], Step [620/750], Loss: 0.0811\n",
      "Epoch [2/8], Step [621/750], Loss: 0.0378\n",
      "Epoch [2/8], Step [622/750], Loss: 0.0646\n",
      "Epoch [2/8], Step [623/750], Loss: 0.0874\n",
      "Epoch [2/8], Step [624/750], Loss: 0.0691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Step [625/750], Loss: 0.0712\n",
      "Epoch [2/8], Step [626/750], Loss: 0.0288\n",
      "Epoch [2/8], Step [627/750], Loss: 0.0880\n",
      "Epoch [2/8], Step [628/750], Loss: 0.0519\n",
      "Epoch [2/8], Step [629/750], Loss: 0.0221\n",
      "Epoch [2/8], Step [630/750], Loss: 0.0432\n",
      "Epoch [2/8], Step [631/750], Loss: 0.0416\n",
      "Epoch [2/8], Step [632/750], Loss: 0.1085\n",
      "Epoch [2/8], Step [633/750], Loss: 0.0124\n",
      "Epoch [2/8], Step [634/750], Loss: 0.0076\n",
      "Epoch [2/8], Step [635/750], Loss: 0.0707\n",
      "Epoch [2/8], Step [636/750], Loss: 0.0168\n",
      "Epoch [2/8], Step [637/750], Loss: 0.0423\n",
      "Epoch [2/8], Step [638/750], Loss: 0.0290\n",
      "Epoch [2/8], Step [639/750], Loss: 0.0221\n",
      "Epoch [2/8], Step [640/750], Loss: 0.0750\n",
      "Epoch [2/8], Step [641/750], Loss: 0.0495\n",
      "Epoch [2/8], Step [642/750], Loss: 0.0223\n",
      "Epoch [2/8], Step [643/750], Loss: 0.0139\n",
      "Epoch [2/8], Step [644/750], Loss: 0.0715\n",
      "Epoch [2/8], Step [645/750], Loss: 0.0407\n",
      "Epoch [2/8], Step [646/750], Loss: 0.0935\n",
      "Epoch [2/8], Step [647/750], Loss: 0.0733\n",
      "Epoch [2/8], Step [648/750], Loss: 0.0221\n",
      "Epoch [2/8], Step [649/750], Loss: 0.0641\n",
      "Epoch [2/8], Step [650/750], Loss: 0.0163\n",
      "Epoch [2/8], Step [651/750], Loss: 0.0575\n",
      "Epoch [2/8], Step [652/750], Loss: 0.1108\n",
      "Epoch [2/8], Step [653/750], Loss: 0.0505\n",
      "Epoch [2/8], Step [654/750], Loss: 0.1442\n",
      "Epoch [2/8], Step [655/750], Loss: 0.0372\n",
      "Epoch [2/8], Step [656/750], Loss: 0.0492\n",
      "Epoch [2/8], Step [657/750], Loss: 0.0562\n",
      "Epoch [2/8], Step [658/750], Loss: 0.0213\n",
      "Epoch [2/8], Step [659/750], Loss: 0.0703\n",
      "Epoch [2/8], Step [660/750], Loss: 0.0628\n",
      "Epoch [2/8], Step [661/750], Loss: 0.1275\n",
      "Epoch [2/8], Step [662/750], Loss: 0.0621\n",
      "Epoch [2/8], Step [663/750], Loss: 0.0250\n",
      "Epoch [2/8], Step [664/750], Loss: 0.0289\n",
      "Epoch [2/8], Step [665/750], Loss: 0.1418\n",
      "Epoch [2/8], Step [666/750], Loss: 0.2074\n",
      "Epoch [2/8], Step [667/750], Loss: 0.0899\n",
      "Epoch [2/8], Step [668/750], Loss: 0.0324\n",
      "Epoch [2/8], Step [669/750], Loss: 0.0414\n",
      "Epoch [2/8], Step [670/750], Loss: 0.0385\n",
      "Epoch [2/8], Step [671/750], Loss: 0.1010\n",
      "Epoch [2/8], Step [672/750], Loss: 0.2062\n",
      "Epoch [2/8], Step [673/750], Loss: 0.0693\n",
      "Epoch [2/8], Step [674/750], Loss: 0.0987\n",
      "Epoch [2/8], Step [675/750], Loss: 0.0273\n",
      "Epoch [2/8], Step [676/750], Loss: 0.2589\n",
      "Epoch [2/8], Step [677/750], Loss: 0.1259\n",
      "Epoch [2/8], Step [678/750], Loss: 0.0574\n",
      "Epoch [2/8], Step [679/750], Loss: 0.0102\n",
      "Epoch [2/8], Step [680/750], Loss: 0.0429\n",
      "Epoch [2/8], Step [681/750], Loss: 0.0327\n",
      "Epoch [2/8], Step [682/750], Loss: 0.0147\n",
      "Epoch [2/8], Step [683/750], Loss: 0.0746\n",
      "Epoch [2/8], Step [684/750], Loss: 0.0210\n",
      "Epoch [2/8], Step [685/750], Loss: 0.0391\n",
      "Epoch [2/8], Step [686/750], Loss: 0.0781\n",
      "Epoch [2/8], Step [687/750], Loss: 0.0514\n",
      "Epoch [2/8], Step [688/750], Loss: 0.0792\n",
      "Epoch [2/8], Step [689/750], Loss: 0.0508\n",
      "Epoch [2/8], Step [690/750], Loss: 0.0558\n",
      "Epoch [2/8], Step [691/750], Loss: 0.0490\n",
      "Epoch [2/8], Step [692/750], Loss: 0.0296\n",
      "Epoch [2/8], Step [693/750], Loss: 0.0479\n",
      "Epoch [2/8], Step [694/750], Loss: 0.0839\n",
      "Epoch [2/8], Step [695/750], Loss: 0.1675\n",
      "Epoch [2/8], Step [696/750], Loss: 0.0344\n",
      "Epoch [2/8], Step [697/750], Loss: 0.0893\n",
      "Epoch [2/8], Step [698/750], Loss: 0.0264\n",
      "Epoch [2/8], Step [699/750], Loss: 0.0520\n",
      "Epoch [2/8], Step [700/750], Loss: 0.0576\n",
      "Epoch [2/8], Step [701/750], Loss: 0.0568\n",
      "Epoch [2/8], Step [702/750], Loss: 0.0777\n",
      "Epoch [2/8], Step [703/750], Loss: 0.0870\n",
      "Epoch [2/8], Step [704/750], Loss: 0.0106\n",
      "Epoch [2/8], Step [705/750], Loss: 0.0105\n",
      "Epoch [2/8], Step [706/750], Loss: 0.0317\n",
      "Epoch [2/8], Step [707/750], Loss: 0.0387\n",
      "Epoch [2/8], Step [708/750], Loss: 0.0131\n",
      "Epoch [2/8], Step [709/750], Loss: 0.0826\n",
      "Epoch [2/8], Step [710/750], Loss: 0.0460\n",
      "Epoch [2/8], Step [711/750], Loss: 0.1581\n",
      "Epoch [2/8], Step [712/750], Loss: 0.0236\n",
      "Epoch [2/8], Step [713/750], Loss: 0.0198\n",
      "Epoch [2/8], Step [714/750], Loss: 0.1217\n",
      "Epoch [2/8], Step [715/750], Loss: 0.0830\n",
      "Epoch [2/8], Step [716/750], Loss: 0.0117\n",
      "Epoch [2/8], Step [717/750], Loss: 0.0242\n",
      "Epoch [2/8], Step [718/750], Loss: 0.0405\n",
      "Epoch [2/8], Step [719/750], Loss: 0.0392\n",
      "Epoch [2/8], Step [720/750], Loss: 0.0543\n",
      "Epoch [2/8], Step [721/750], Loss: 0.3164\n",
      "Epoch [2/8], Step [722/750], Loss: 0.0545\n",
      "Epoch [2/8], Step [723/750], Loss: 0.0032\n",
      "Epoch [2/8], Step [724/750], Loss: 0.0485\n",
      "Epoch [2/8], Step [725/750], Loss: 0.0084\n",
      "Epoch [2/8], Step [726/750], Loss: 0.0473\n",
      "Epoch [2/8], Step [727/750], Loss: 0.1193\n",
      "Epoch [2/8], Step [728/750], Loss: 0.0223\n",
      "Epoch [2/8], Step [729/750], Loss: 0.1332\n",
      "Epoch [2/8], Step [730/750], Loss: 0.0550\n",
      "Epoch [2/8], Step [731/750], Loss: 0.0651\n",
      "Epoch [2/8], Step [732/750], Loss: 0.1841\n",
      "Epoch [2/8], Step [733/750], Loss: 0.0120\n",
      "Epoch [2/8], Step [734/750], Loss: 0.0412\n",
      "Epoch [2/8], Step [735/750], Loss: 0.1451\n",
      "Epoch [2/8], Step [736/750], Loss: 0.0770\n",
      "Epoch [2/8], Step [737/750], Loss: 0.0056\n",
      "Epoch [2/8], Step [738/750], Loss: 0.0190\n",
      "Epoch [2/8], Step [739/750], Loss: 0.0615\n",
      "Epoch [2/8], Step [740/750], Loss: 0.0174\n",
      "Epoch [2/8], Step [741/750], Loss: 0.0844\n",
      "Epoch [2/8], Step [742/750], Loss: 0.1503\n",
      "Epoch [2/8], Step [743/750], Loss: 0.0510\n",
      "Epoch [2/8], Step [744/750], Loss: 0.0486\n",
      "Epoch [2/8], Step [745/750], Loss: 0.0134\n",
      "Epoch [2/8], Step [746/750], Loss: 0.0108\n",
      "Epoch [2/8], Step [747/750], Loss: 0.1043\n",
      "Epoch [2/8], Step [748/750], Loss: 0.0837\n",
      "Epoch [2/8], Step [749/750], Loss: 0.0591\n",
      "Epoch [2/8], Step [750/750], Loss: 0.0418\n",
      "Epoch [2/8], Tr. loss: 0.2284. Test loss: 0.1003\n",
      "\n",
      "\n",
      "Epoch [3/8], Step [1/750], Loss: 0.0083\n",
      "Epoch [3/8], Step [2/750], Loss: 0.0195\n",
      "Epoch [3/8], Step [3/750], Loss: 0.0728\n",
      "Epoch [3/8], Step [4/750], Loss: 0.0243\n",
      "Epoch [3/8], Step [5/750], Loss: 0.0219\n",
      "Epoch [3/8], Step [6/750], Loss: 0.0590\n",
      "Epoch [3/8], Step [7/750], Loss: 0.0306\n",
      "Epoch [3/8], Step [8/750], Loss: 0.0242\n",
      "Epoch [3/8], Step [9/750], Loss: 0.0535\n",
      "Epoch [3/8], Step [10/750], Loss: 0.0173\n",
      "Epoch [3/8], Step [11/750], Loss: 0.0776\n",
      "Epoch [3/8], Step [12/750], Loss: 0.0110\n",
      "Epoch [3/8], Step [13/750], Loss: 0.0219\n",
      "Epoch [3/8], Step [14/750], Loss: 0.0072\n",
      "Epoch [3/8], Step [15/750], Loss: 0.2332\n",
      "Epoch [3/8], Step [16/750], Loss: 0.0124\n",
      "Epoch [3/8], Step [17/750], Loss: 0.0043\n",
      "Epoch [3/8], Step [18/750], Loss: 0.0442\n",
      "Epoch [3/8], Step [19/750], Loss: 0.0041\n",
      "Epoch [3/8], Step [20/750], Loss: 0.0094\n",
      "Epoch [3/8], Step [21/750], Loss: 0.0076\n",
      "Epoch [3/8], Step [22/750], Loss: 0.0034\n",
      "Epoch [3/8], Step [23/750], Loss: 0.0050\n",
      "Epoch [3/8], Step [24/750], Loss: 0.0149\n",
      "Epoch [3/8], Step [25/750], Loss: 0.0034\n",
      "Epoch [3/8], Step [26/750], Loss: 0.0329\n",
      "Epoch [3/8], Step [27/750], Loss: 0.0788\n",
      "Epoch [3/8], Step [28/750], Loss: 0.0033\n",
      "Epoch [3/8], Step [29/750], Loss: 0.0157\n",
      "Epoch [3/8], Step [30/750], Loss: 0.0757\n",
      "Epoch [3/8], Step [31/750], Loss: 0.0808\n",
      "Epoch [3/8], Step [32/750], Loss: 0.0493\n",
      "Epoch [3/8], Step [33/750], Loss: 0.0520\n",
      "Epoch [3/8], Step [34/750], Loss: 0.1277\n",
      "Epoch [3/8], Step [35/750], Loss: 0.0131\n",
      "Epoch [3/8], Step [36/750], Loss: 0.0136\n",
      "Epoch [3/8], Step [37/750], Loss: 0.0777\n",
      "Epoch [3/8], Step [38/750], Loss: 0.0723\n",
      "Epoch [3/8], Step [39/750], Loss: 0.0034\n",
      "Epoch [3/8], Step [40/750], Loss: 0.0454\n",
      "Epoch [3/8], Step [41/750], Loss: 0.0036\n",
      "Epoch [3/8], Step [42/750], Loss: 0.0496\n",
      "Epoch [3/8], Step [43/750], Loss: 0.0185\n",
      "Epoch [3/8], Step [44/750], Loss: 0.0181\n",
      "Epoch [3/8], Step [45/750], Loss: 0.0159\n",
      "Epoch [3/8], Step [46/750], Loss: 0.0241\n",
      "Epoch [3/8], Step [47/750], Loss: 0.0044\n",
      "Epoch [3/8], Step [48/750], Loss: 0.0105\n",
      "Epoch [3/8], Step [49/750], Loss: 0.0333\n",
      "Epoch [3/8], Step [50/750], Loss: 0.1537\n",
      "Epoch [3/8], Step [51/750], Loss: 0.0203\n",
      "Epoch [3/8], Step [52/750], Loss: 0.0052\n",
      "Epoch [3/8], Step [53/750], Loss: 0.0069\n",
      "Epoch [3/8], Step [54/750], Loss: 0.0481\n",
      "Epoch [3/8], Step [55/750], Loss: 0.0128\n",
      "Epoch [3/8], Step [56/750], Loss: 0.0213\n",
      "Epoch [3/8], Step [57/750], Loss: 0.0071\n",
      "Epoch [3/8], Step [58/750], Loss: 0.1086\n",
      "Epoch [3/8], Step [59/750], Loss: 0.0021\n",
      "Epoch [3/8], Step [60/750], Loss: 0.1215\n",
      "Epoch [3/8], Step [61/750], Loss: 0.0484\n",
      "Epoch [3/8], Step [62/750], Loss: 0.0860\n",
      "Epoch [3/8], Step [63/750], Loss: 0.0057\n",
      "Epoch [3/8], Step [64/750], Loss: 0.1407\n",
      "Epoch [3/8], Step [65/750], Loss: 0.1036\n",
      "Epoch [3/8], Step [66/750], Loss: 0.0401\n",
      "Epoch [3/8], Step [67/750], Loss: 0.0175\n",
      "Epoch [3/8], Step [68/750], Loss: 0.0645\n",
      "Epoch [3/8], Step [69/750], Loss: 0.0344\n",
      "Epoch [3/8], Step [70/750], Loss: 0.1094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], Step [71/750], Loss: 0.0081\n",
      "Epoch [3/8], Step [72/750], Loss: 0.0911\n",
      "Epoch [3/8], Step [73/750], Loss: 0.0369\n",
      "Epoch [3/8], Step [74/750], Loss: 0.0485\n",
      "Epoch [3/8], Step [75/750], Loss: 0.0077\n",
      "Epoch [3/8], Step [76/750], Loss: 0.0119\n",
      "Epoch [3/8], Step [77/750], Loss: 0.0036\n",
      "Epoch [3/8], Step [78/750], Loss: 0.0937\n",
      "Epoch [3/8], Step [79/750], Loss: 0.0682\n",
      "Epoch [3/8], Step [80/750], Loss: 0.0865\n",
      "Epoch [3/8], Step [81/750], Loss: 0.1166\n",
      "Epoch [3/8], Step [82/750], Loss: 0.0299\n",
      "Epoch [3/8], Step [83/750], Loss: 0.0691\n",
      "Epoch [3/8], Step [84/750], Loss: 0.0342\n",
      "Epoch [3/8], Step [85/750], Loss: 0.0367\n",
      "Epoch [3/8], Step [86/750], Loss: 0.0181\n",
      "Epoch [3/8], Step [87/750], Loss: 0.0053\n",
      "Epoch [3/8], Step [88/750], Loss: 0.0291\n",
      "Epoch [3/8], Step [89/750], Loss: 0.0018\n",
      "Epoch [3/8], Step [90/750], Loss: 0.0309\n",
      "Epoch [3/8], Step [91/750], Loss: 0.0780\n",
      "Epoch [3/8], Step [92/750], Loss: 0.1346\n",
      "Epoch [3/8], Step [93/750], Loss: 0.0219\n",
      "Epoch [3/8], Step [94/750], Loss: 0.0102\n",
      "Epoch [3/8], Step [95/750], Loss: 0.0746\n",
      "Epoch [3/8], Step [96/750], Loss: 0.0277\n",
      "Epoch [3/8], Step [97/750], Loss: 0.0313\n",
      "Epoch [3/8], Step [98/750], Loss: 0.0140\n",
      "Epoch [3/8], Step [99/750], Loss: 0.0819\n",
      "Epoch [3/8], Step [100/750], Loss: 0.0792\n",
      "Epoch [3/8], Step [101/750], Loss: 0.0524\n",
      "Epoch [3/8], Step [102/750], Loss: 0.0491\n",
      "Epoch [3/8], Step [103/750], Loss: 0.0678\n",
      "Epoch [3/8], Step [104/750], Loss: 0.0668\n",
      "Epoch [3/8], Step [105/750], Loss: 0.0097\n",
      "Epoch [3/8], Step [106/750], Loss: 0.0065\n",
      "Epoch [3/8], Step [107/750], Loss: 0.0929\n",
      "Epoch [3/8], Step [108/750], Loss: 0.0285\n",
      "Epoch [3/8], Step [109/750], Loss: 0.1435\n",
      "Epoch [3/8], Step [110/750], Loss: 0.0385\n",
      "Epoch [3/8], Step [111/750], Loss: 0.0794\n",
      "Epoch [3/8], Step [112/750], Loss: 0.0048\n",
      "Epoch [3/8], Step [113/750], Loss: 0.0277\n",
      "Epoch [3/8], Step [114/750], Loss: 0.0155\n",
      "Epoch [3/8], Step [115/750], Loss: 0.0179\n",
      "Epoch [3/8], Step [116/750], Loss: 0.0046\n",
      "Epoch [3/8], Step [117/750], Loss: 0.0063\n",
      "Epoch [3/8], Step [118/750], Loss: 0.0736\n",
      "Epoch [3/8], Step [119/750], Loss: 0.0852\n",
      "Epoch [3/8], Step [120/750], Loss: 0.0075\n",
      "Epoch [3/8], Step [121/750], Loss: 0.0182\n",
      "Epoch [3/8], Step [122/750], Loss: 0.0479\n",
      "Epoch [3/8], Step [123/750], Loss: 0.0052\n",
      "Epoch [3/8], Step [124/750], Loss: 0.0639\n",
      "Epoch [3/8], Step [125/750], Loss: 0.0810\n",
      "Epoch [3/8], Step [126/750], Loss: 0.0081\n",
      "Epoch [3/8], Step [127/750], Loss: 0.0184\n",
      "Epoch [3/8], Step [128/750], Loss: 0.0169\n",
      "Epoch [3/8], Step [129/750], Loss: 0.0411\n",
      "Epoch [3/8], Step [130/750], Loss: 0.0774\n",
      "Epoch [3/8], Step [131/750], Loss: 0.0507\n",
      "Epoch [3/8], Step [132/750], Loss: 0.0041\n",
      "Epoch [3/8], Step [133/750], Loss: 0.0132\n",
      "Epoch [3/8], Step [134/750], Loss: 0.0142\n",
      "Epoch [3/8], Step [135/750], Loss: 0.0256\n",
      "Epoch [3/8], Step [136/750], Loss: 0.0177\n",
      "Epoch [3/8], Step [137/750], Loss: 0.0092\n",
      "Epoch [3/8], Step [138/750], Loss: 0.0046\n",
      "Epoch [3/8], Step [139/750], Loss: 0.1604\n",
      "Epoch [3/8], Step [140/750], Loss: 0.0112\n",
      "Epoch [3/8], Step [141/750], Loss: 0.1064\n",
      "Epoch [3/8], Step [142/750], Loss: 0.0511\n",
      "Epoch [3/8], Step [143/750], Loss: 0.0168\n",
      "Epoch [3/8], Step [144/750], Loss: 0.0104\n",
      "Epoch [3/8], Step [145/750], Loss: 0.0025\n",
      "Epoch [3/8], Step [146/750], Loss: 0.0147\n",
      "Epoch [3/8], Step [147/750], Loss: 0.0273\n",
      "Epoch [3/8], Step [148/750], Loss: 0.0148\n",
      "Epoch [3/8], Step [149/750], Loss: 0.0294\n",
      "Epoch [3/8], Step [150/750], Loss: 0.0566\n",
      "Epoch [3/8], Step [151/750], Loss: 0.0102\n",
      "Epoch [3/8], Step [152/750], Loss: 0.0228\n",
      "Epoch [3/8], Step [153/750], Loss: 0.0136\n",
      "Epoch [3/8], Step [154/750], Loss: 0.0068\n",
      "Epoch [3/8], Step [155/750], Loss: 0.0043\n",
      "Epoch [3/8], Step [156/750], Loss: 0.0307\n",
      "Epoch [3/8], Step [157/750], Loss: 0.0480\n",
      "Epoch [3/8], Step [158/750], Loss: 0.1702\n",
      "Epoch [3/8], Step [159/750], Loss: 0.0991\n",
      "Epoch [3/8], Step [160/750], Loss: 0.0225\n",
      "Epoch [3/8], Step [161/750], Loss: 0.0106\n",
      "Epoch [3/8], Step [162/750], Loss: 0.1028\n",
      "Epoch [3/8], Step [163/750], Loss: 0.0514\n",
      "Epoch [3/8], Step [164/750], Loss: 0.1593\n",
      "Epoch [3/8], Step [165/750], Loss: 0.0105\n",
      "Epoch [3/8], Step [166/750], Loss: 0.0486\n",
      "Epoch [3/8], Step [167/750], Loss: 0.0563\n",
      "Epoch [3/8], Step [168/750], Loss: 0.0737\n",
      "Epoch [3/8], Step [169/750], Loss: 0.0150\n",
      "Epoch [3/8], Step [170/750], Loss: 0.0699\n",
      "Epoch [3/8], Step [171/750], Loss: 0.0193\n",
      "Epoch [3/8], Step [172/750], Loss: 0.0459\n",
      "Epoch [3/8], Step [173/750], Loss: 0.0817\n",
      "Epoch [3/8], Step [174/750], Loss: 0.0153\n",
      "Epoch [3/8], Step [175/750], Loss: 0.0031\n",
      "Epoch [3/8], Step [176/750], Loss: 0.0398\n",
      "Epoch [3/8], Step [177/750], Loss: 0.0112\n",
      "Epoch [3/8], Step [178/750], Loss: 0.1083\n",
      "Epoch [3/8], Step [179/750], Loss: 0.0354\n",
      "Epoch [3/8], Step [180/750], Loss: 0.0968\n",
      "Epoch [3/8], Step [181/750], Loss: 0.0459\n",
      "Epoch [3/8], Step [182/750], Loss: 0.0375\n",
      "Epoch [3/8], Step [183/750], Loss: 0.0223\n",
      "Epoch [3/8], Step [184/750], Loss: 0.0251\n",
      "Epoch [3/8], Step [185/750], Loss: 0.0262\n",
      "Epoch [3/8], Step [186/750], Loss: 0.1232\n",
      "Epoch [3/8], Step [187/750], Loss: 0.0482\n",
      "Epoch [3/8], Step [188/750], Loss: 0.0733\n",
      "Epoch [3/8], Step [189/750], Loss: 0.0277\n",
      "Epoch [3/8], Step [190/750], Loss: 0.0170\n",
      "Epoch [3/8], Step [191/750], Loss: 0.0947\n",
      "Epoch [3/8], Step [192/750], Loss: 0.0404\n",
      "Epoch [3/8], Step [193/750], Loss: 0.0497\n",
      "Epoch [3/8], Step [194/750], Loss: 0.0364\n",
      "Epoch [3/8], Step [195/750], Loss: 0.0055\n",
      "Epoch [3/8], Step [196/750], Loss: 0.1519\n",
      "Epoch [3/8], Step [197/750], Loss: 0.0114\n",
      "Epoch [3/8], Step [198/750], Loss: 0.0239\n",
      "Epoch [3/8], Step [199/750], Loss: 0.0053\n",
      "Epoch [3/8], Step [200/750], Loss: 0.0085\n",
      "Epoch [3/8], Step [201/750], Loss: 0.0146\n",
      "Epoch [3/8], Step [202/750], Loss: 0.0338\n",
      "Epoch [3/8], Step [203/750], Loss: 0.1094\n",
      "Epoch [3/8], Step [204/750], Loss: 0.0900\n",
      "Epoch [3/8], Step [205/750], Loss: 0.1136\n",
      "Epoch [3/8], Step [206/750], Loss: 0.1917\n",
      "Epoch [3/8], Step [207/750], Loss: 0.0566\n",
      "Epoch [3/8], Step [208/750], Loss: 0.0183\n",
      "Epoch [3/8], Step [209/750], Loss: 0.0051\n",
      "Epoch [3/8], Step [210/750], Loss: 0.0513\n",
      "Epoch [3/8], Step [211/750], Loss: 0.0081\n",
      "Epoch [3/8], Step [212/750], Loss: 0.1294\n",
      "Epoch [3/8], Step [213/750], Loss: 0.0058\n",
      "Epoch [3/8], Step [214/750], Loss: 0.0505\n",
      "Epoch [3/8], Step [215/750], Loss: 0.0238\n",
      "Epoch [3/8], Step [216/750], Loss: 0.0130\n",
      "Epoch [3/8], Step [217/750], Loss: 0.1239\n",
      "Epoch [3/8], Step [218/750], Loss: 0.0229\n",
      "Epoch [3/8], Step [219/750], Loss: 0.0072\n",
      "Epoch [3/8], Step [220/750], Loss: 0.0195\n",
      "Epoch [3/8], Step [221/750], Loss: 0.0273\n",
      "Epoch [3/8], Step [222/750], Loss: 0.0246\n",
      "Epoch [3/8], Step [223/750], Loss: 0.0155\n",
      "Epoch [3/8], Step [224/750], Loss: 0.0168\n",
      "Epoch [3/8], Step [225/750], Loss: 0.0089\n",
      "Epoch [3/8], Step [226/750], Loss: 0.0199\n",
      "Epoch [3/8], Step [227/750], Loss: 0.0480\n",
      "Epoch [3/8], Step [228/750], Loss: 0.0450\n",
      "Epoch [3/8], Step [229/750], Loss: 0.0926\n",
      "Epoch [3/8], Step [230/750], Loss: 0.0403\n",
      "Epoch [3/8], Step [231/750], Loss: 0.0832\n",
      "Epoch [3/8], Step [232/750], Loss: 0.0033\n",
      "Epoch [3/8], Step [233/750], Loss: 0.0037\n",
      "Epoch [3/8], Step [234/750], Loss: 0.0307\n",
      "Epoch [3/8], Step [235/750], Loss: 0.0582\n",
      "Epoch [3/8], Step [236/750], Loss: 0.0949\n",
      "Epoch [3/8], Step [237/750], Loss: 0.0568\n",
      "Epoch [3/8], Step [238/750], Loss: 0.0177\n",
      "Epoch [3/8], Step [239/750], Loss: 0.0091\n",
      "Epoch [3/8], Step [240/750], Loss: 0.0037\n",
      "Epoch [3/8], Step [241/750], Loss: 0.0113\n",
      "Epoch [3/8], Step [242/750], Loss: 0.0700\n",
      "Epoch [3/8], Step [243/750], Loss: 0.0680\n",
      "Epoch [3/8], Step [244/750], Loss: 0.0075\n",
      "Epoch [3/8], Step [245/750], Loss: 0.0128\n",
      "Epoch [3/8], Step [246/750], Loss: 0.0160\n",
      "Epoch [3/8], Step [247/750], Loss: 0.1389\n",
      "Epoch [3/8], Step [248/750], Loss: 0.1328\n",
      "Epoch [3/8], Step [249/750], Loss: 0.0084\n",
      "Epoch [3/8], Step [250/750], Loss: 0.0097\n",
      "Epoch [3/8], Step [251/750], Loss: 0.0043\n",
      "Epoch [3/8], Step [252/750], Loss: 0.0727\n",
      "Epoch [3/8], Step [253/750], Loss: 0.0535\n",
      "Epoch [3/8], Step [254/750], Loss: 0.1078\n",
      "Epoch [3/8], Step [255/750], Loss: 0.0139\n",
      "Epoch [3/8], Step [256/750], Loss: 0.0066\n",
      "Epoch [3/8], Step [257/750], Loss: 0.0046\n",
      "Epoch [3/8], Step [258/750], Loss: 0.0152\n",
      "Epoch [3/8], Step [259/750], Loss: 0.0645\n",
      "Epoch [3/8], Step [260/750], Loss: 0.0102\n",
      "Epoch [3/8], Step [261/750], Loss: 0.0086\n",
      "Epoch [3/8], Step [262/750], Loss: 0.0065\n",
      "Epoch [3/8], Step [263/750], Loss: 0.0113\n",
      "Epoch [3/8], Step [264/750], Loss: 0.0086\n",
      "Epoch [3/8], Step [265/750], Loss: 0.0198\n",
      "Epoch [3/8], Step [266/750], Loss: 0.0683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], Step [267/750], Loss: 0.0058\n",
      "Epoch [3/8], Step [268/750], Loss: 0.0055\n",
      "Epoch [3/8], Step [269/750], Loss: 0.0682\n",
      "Epoch [3/8], Step [270/750], Loss: 0.0380\n",
      "Epoch [3/8], Step [271/750], Loss: 0.0475\n",
      "Epoch [3/8], Step [272/750], Loss: 0.0067\n",
      "Epoch [3/8], Step [273/750], Loss: 0.0426\n",
      "Epoch [3/8], Step [274/750], Loss: 0.0242\n",
      "Epoch [3/8], Step [275/750], Loss: 0.0551\n",
      "Epoch [3/8], Step [276/750], Loss: 0.0083\n",
      "Epoch [3/8], Step [277/750], Loss: 0.0232\n",
      "Epoch [3/8], Step [278/750], Loss: 0.0044\n",
      "Epoch [3/8], Step [279/750], Loss: 0.0663\n",
      "Epoch [3/8], Step [280/750], Loss: 0.0096\n",
      "Epoch [3/8], Step [281/750], Loss: 0.0123\n",
      "Epoch [3/8], Step [282/750], Loss: 0.0317\n",
      "Epoch [3/8], Step [283/750], Loss: 0.0962\n",
      "Epoch [3/8], Step [284/750], Loss: 0.0282\n",
      "Epoch [3/8], Step [285/750], Loss: 0.0053\n",
      "Epoch [3/8], Step [286/750], Loss: 0.0115\n",
      "Epoch [3/8], Step [287/750], Loss: 0.0500\n",
      "Epoch [3/8], Step [288/750], Loss: 0.0492\n",
      "Epoch [3/8], Step [289/750], Loss: 0.0493\n",
      "Epoch [3/8], Step [290/750], Loss: 0.0058\n",
      "Epoch [3/8], Step [291/750], Loss: 0.0607\n",
      "Epoch [3/8], Step [292/750], Loss: 0.0151\n",
      "Epoch [3/8], Step [293/750], Loss: 0.0563\n",
      "Epoch [3/8], Step [294/750], Loss: 0.1363\n",
      "Epoch [3/8], Step [295/750], Loss: 0.0124\n",
      "Epoch [3/8], Step [296/750], Loss: 0.0671\n",
      "Epoch [3/8], Step [297/750], Loss: 0.0787\n",
      "Epoch [3/8], Step [298/750], Loss: 0.0685\n",
      "Epoch [3/8], Step [299/750], Loss: 0.0599\n",
      "Epoch [3/8], Step [300/750], Loss: 0.0184\n",
      "Epoch [3/8], Step [301/750], Loss: 0.0332\n",
      "Epoch [3/8], Step [302/750], Loss: 0.0038\n",
      "Epoch [3/8], Step [303/750], Loss: 0.0492\n",
      "Epoch [3/8], Step [304/750], Loss: 0.0350\n",
      "Epoch [3/8], Step [305/750], Loss: 0.0059\n",
      "Epoch [3/8], Step [306/750], Loss: 0.0043\n",
      "Epoch [3/8], Step [307/750], Loss: 0.0661\n",
      "Epoch [3/8], Step [308/750], Loss: 0.0138\n",
      "Epoch [3/8], Step [309/750], Loss: 0.0043\n",
      "Epoch [3/8], Step [310/750], Loss: 0.0079\n",
      "Epoch [3/8], Step [311/750], Loss: 0.0684\n",
      "Epoch [3/8], Step [312/750], Loss: 0.0177\n",
      "Epoch [3/8], Step [313/750], Loss: 0.0113\n",
      "Epoch [3/8], Step [314/750], Loss: 0.0486\n",
      "Epoch [3/8], Step [315/750], Loss: 0.0490\n",
      "Epoch [3/8], Step [316/750], Loss: 0.0130\n",
      "Epoch [3/8], Step [317/750], Loss: 0.0095\n",
      "Epoch [3/8], Step [318/750], Loss: 0.0072\n",
      "Epoch [3/8], Step [319/750], Loss: 0.1087\n",
      "Epoch [3/8], Step [320/750], Loss: 0.0505\n",
      "Epoch [3/8], Step [321/750], Loss: 0.0217\n",
      "Epoch [3/8], Step [322/750], Loss: 0.0446\n",
      "Epoch [3/8], Step [323/750], Loss: 0.0119\n",
      "Epoch [3/8], Step [324/750], Loss: 0.0147\n",
      "Epoch [3/8], Step [325/750], Loss: 0.0319\n",
      "Epoch [3/8], Step [326/750], Loss: 0.0686\n",
      "Epoch [3/8], Step [327/750], Loss: 0.0021\n",
      "Epoch [3/8], Step [328/750], Loss: 0.0648\n",
      "Epoch [3/8], Step [329/750], Loss: 0.0112\n",
      "Epoch [3/8], Step [330/750], Loss: 0.0076\n",
      "Epoch [3/8], Step [331/750], Loss: 0.0401\n",
      "Epoch [3/8], Step [332/750], Loss: 0.0311\n",
      "Epoch [3/8], Step [333/750], Loss: 0.0168\n",
      "Epoch [3/8], Step [334/750], Loss: 0.0191\n",
      "Epoch [3/8], Step [335/750], Loss: 0.0125\n",
      "Epoch [3/8], Step [336/750], Loss: 0.0704\n",
      "Epoch [3/8], Step [337/750], Loss: 0.0240\n",
      "Epoch [3/8], Step [338/750], Loss: 0.0258\n",
      "Epoch [3/8], Step [339/750], Loss: 0.0150\n",
      "Epoch [3/8], Step [340/750], Loss: 0.0269\n",
      "Epoch [3/8], Step [341/750], Loss: 0.0372\n",
      "Epoch [3/8], Step [342/750], Loss: 0.0185\n",
      "Epoch [3/8], Step [343/750], Loss: 0.0815\n",
      "Epoch [3/8], Step [344/750], Loss: 0.0133\n",
      "Epoch [3/8], Step [345/750], Loss: 0.0057\n",
      "Epoch [3/8], Step [346/750], Loss: 0.0613\n",
      "Epoch [3/8], Step [347/750], Loss: 0.0359\n",
      "Epoch [3/8], Step [348/750], Loss: 0.0052\n",
      "Epoch [3/8], Step [349/750], Loss: 0.0400\n",
      "Epoch [3/8], Step [350/750], Loss: 0.2625\n",
      "Epoch [3/8], Step [351/750], Loss: 0.0130\n",
      "Epoch [3/8], Step [352/750], Loss: 0.0604\n",
      "Epoch [3/8], Step [353/750], Loss: 0.0387\n",
      "Epoch [3/8], Step [354/750], Loss: 0.1138\n",
      "Epoch [3/8], Step [355/750], Loss: 0.0449\n",
      "Epoch [3/8], Step [356/750], Loss: 0.0392\n",
      "Epoch [3/8], Step [357/750], Loss: 0.1084\n",
      "Epoch [3/8], Step [358/750], Loss: 0.0193\n",
      "Epoch [3/8], Step [359/750], Loss: 0.0197\n",
      "Epoch [3/8], Step [360/750], Loss: 0.0931\n",
      "Epoch [3/8], Step [361/750], Loss: 0.0942\n",
      "Epoch [3/8], Step [362/750], Loss: 0.1050\n",
      "Epoch [3/8], Step [363/750], Loss: 0.0614\n",
      "Epoch [3/8], Step [364/750], Loss: 0.0083\n",
      "Epoch [3/8], Step [365/750], Loss: 0.0658\n",
      "Epoch [3/8], Step [366/750], Loss: 0.0328\n",
      "Epoch [3/8], Step [367/750], Loss: 0.0944\n",
      "Epoch [3/8], Step [368/750], Loss: 0.0464\n",
      "Epoch [3/8], Step [369/750], Loss: 0.0509\n",
      "Epoch [3/8], Step [370/750], Loss: 0.0352\n",
      "Epoch [3/8], Step [371/750], Loss: 0.0486\n",
      "Epoch [3/8], Step [372/750], Loss: 0.0075\n",
      "Epoch [3/8], Step [373/750], Loss: 0.0797\n",
      "Epoch [3/8], Step [374/750], Loss: 0.0061\n",
      "Epoch [3/8], Step [375/750], Loss: 0.0409\n",
      "Epoch [3/8], Step [376/750], Loss: 0.0072\n",
      "Epoch [3/8], Step [377/750], Loss: 0.1095\n",
      "Epoch [3/8], Step [378/750], Loss: 0.0194\n",
      "Epoch [3/8], Step [379/750], Loss: 0.0283\n",
      "Epoch [3/8], Step [380/750], Loss: 0.0248\n",
      "Epoch [3/8], Step [381/750], Loss: 0.0360\n",
      "Epoch [3/8], Step [382/750], Loss: 0.0310\n",
      "Epoch [3/8], Step [383/750], Loss: 0.0345\n",
      "Epoch [3/8], Step [384/750], Loss: 0.0166\n",
      "Epoch [3/8], Step [385/750], Loss: 0.0071\n",
      "Epoch [3/8], Step [386/750], Loss: 0.0271\n",
      "Epoch [3/8], Step [387/750], Loss: 0.0659\n",
      "Epoch [3/8], Step [388/750], Loss: 0.0544\n",
      "Epoch [3/8], Step [389/750], Loss: 0.0115\n",
      "Epoch [3/8], Step [390/750], Loss: 0.0294\n",
      "Epoch [3/8], Step [391/750], Loss: 0.0811\n",
      "Epoch [3/8], Step [392/750], Loss: 0.0249\n",
      "Epoch [3/8], Step [393/750], Loss: 0.0137\n",
      "Epoch [3/8], Step [394/750], Loss: 0.0380\n",
      "Epoch [3/8], Step [395/750], Loss: 0.0245\n",
      "Epoch [3/8], Step [396/750], Loss: 0.0294\n",
      "Epoch [3/8], Step [397/750], Loss: 0.0278\n",
      "Epoch [3/8], Step [398/750], Loss: 0.0499\n",
      "Epoch [3/8], Step [399/750], Loss: 0.0316\n",
      "Epoch [3/8], Step [400/750], Loss: 0.0286\n",
      "Epoch [3/8], Step [401/750], Loss: 0.0268\n",
      "Epoch [3/8], Step [402/750], Loss: 0.0385\n",
      "Epoch [3/8], Step [403/750], Loss: 0.0038\n",
      "Epoch [3/8], Step [404/750], Loss: 0.0294\n",
      "Epoch [3/8], Step [405/750], Loss: 0.0530\n",
      "Epoch [3/8], Step [406/750], Loss: 0.0033\n",
      "Epoch [3/8], Step [407/750], Loss: 0.0202\n",
      "Epoch [3/8], Step [408/750], Loss: 0.0068\n",
      "Epoch [3/8], Step [409/750], Loss: 0.0676\n",
      "Epoch [3/8], Step [410/750], Loss: 0.0587\n",
      "Epoch [3/8], Step [411/750], Loss: 0.0120\n",
      "Epoch [3/8], Step [412/750], Loss: 0.0104\n",
      "Epoch [3/8], Step [413/750], Loss: 0.0496\n",
      "Epoch [3/8], Step [414/750], Loss: 0.0662\n",
      "Epoch [3/8], Step [415/750], Loss: 0.0158\n",
      "Epoch [3/8], Step [416/750], Loss: 0.0318\n",
      "Epoch [3/8], Step [417/750], Loss: 0.1093\n",
      "Epoch [3/8], Step [418/750], Loss: 0.0485\n",
      "Epoch [3/8], Step [419/750], Loss: 0.0071\n",
      "Epoch [3/8], Step [420/750], Loss: 0.1701\n",
      "Epoch [3/8], Step [421/750], Loss: 0.0617\n",
      "Epoch [3/8], Step [422/750], Loss: 0.0449\n",
      "Epoch [3/8], Step [423/750], Loss: 0.0100\n",
      "Epoch [3/8], Step [424/750], Loss: 0.0241\n",
      "Epoch [3/8], Step [425/750], Loss: 0.0401\n",
      "Epoch [3/8], Step [426/750], Loss: 0.0336\n",
      "Epoch [3/8], Step [427/750], Loss: 0.0133\n",
      "Epoch [3/8], Step [428/750], Loss: 0.0133\n",
      "Epoch [3/8], Step [429/750], Loss: 0.0317\n",
      "Epoch [3/8], Step [430/750], Loss: 0.0306\n",
      "Epoch [3/8], Step [431/750], Loss: 0.1336\n",
      "Epoch [3/8], Step [432/750], Loss: 0.1529\n",
      "Epoch [3/8], Step [433/750], Loss: 0.0375\n",
      "Epoch [3/8], Step [434/750], Loss: 0.0502\n",
      "Epoch [3/8], Step [435/750], Loss: 0.0615\n",
      "Epoch [3/8], Step [436/750], Loss: 0.0311\n",
      "Epoch [3/8], Step [437/750], Loss: 0.0222\n",
      "Epoch [3/8], Step [438/750], Loss: 0.0153\n",
      "Epoch [3/8], Step [439/750], Loss: 0.0242\n",
      "Epoch [3/8], Step [440/750], Loss: 0.0786\n",
      "Epoch [3/8], Step [441/750], Loss: 0.1638\n",
      "Epoch [3/8], Step [442/750], Loss: 0.0127\n",
      "Epoch [3/8], Step [443/750], Loss: 0.1093\n",
      "Epoch [3/8], Step [444/750], Loss: 0.0806\n",
      "Epoch [3/8], Step [445/750], Loss: 0.0365\n",
      "Epoch [3/8], Step [446/750], Loss: 0.0221\n",
      "Epoch [3/8], Step [447/750], Loss: 0.0738\n",
      "Epoch [3/8], Step [448/750], Loss: 0.0435\n",
      "Epoch [3/8], Step [449/750], Loss: 0.0052\n",
      "Epoch [3/8], Step [450/750], Loss: 0.0345\n",
      "Epoch [3/8], Step [451/750], Loss: 0.1948\n",
      "Epoch [3/8], Step [452/750], Loss: 0.0075\n",
      "Epoch [3/8], Step [453/750], Loss: 0.0702\n",
      "Epoch [3/8], Step [454/750], Loss: 0.0093\n",
      "Epoch [3/8], Step [455/750], Loss: 0.0342\n",
      "Epoch [3/8], Step [456/750], Loss: 0.0207\n",
      "Epoch [3/8], Step [457/750], Loss: 0.0291\n",
      "Epoch [3/8], Step [458/750], Loss: 0.0299\n",
      "Epoch [3/8], Step [459/750], Loss: 0.0274\n",
      "Epoch [3/8], Step [460/750], Loss: 0.0572\n",
      "Epoch [3/8], Step [461/750], Loss: 0.0396\n",
      "Epoch [3/8], Step [462/750], Loss: 0.0086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], Step [463/750], Loss: 0.0051\n",
      "Epoch [3/8], Step [464/750], Loss: 0.1053\n",
      "Epoch [3/8], Step [465/750], Loss: 0.0346\n",
      "Epoch [3/8], Step [466/750], Loss: 0.1165\n",
      "Epoch [3/8], Step [467/750], Loss: 0.0156\n",
      "Epoch [3/8], Step [468/750], Loss: 0.0045\n",
      "Epoch [3/8], Step [469/750], Loss: 0.0976\n",
      "Epoch [3/8], Step [470/750], Loss: 0.0352\n",
      "Epoch [3/8], Step [471/750], Loss: 0.0521\n",
      "Epoch [3/8], Step [472/750], Loss: 0.0392\n",
      "Epoch [3/8], Step [473/750], Loss: 0.0014\n",
      "Epoch [3/8], Step [474/750], Loss: 0.0777\n",
      "Epoch [3/8], Step [475/750], Loss: 0.0162\n",
      "Epoch [3/8], Step [476/750], Loss: 0.0189\n",
      "Epoch [3/8], Step [477/750], Loss: 0.0461\n",
      "Epoch [3/8], Step [478/750], Loss: 0.0581\n",
      "Epoch [3/8], Step [479/750], Loss: 0.0456\n",
      "Epoch [3/8], Step [480/750], Loss: 0.0041\n",
      "Epoch [3/8], Step [481/750], Loss: 0.0208\n",
      "Epoch [3/8], Step [482/750], Loss: 0.0046\n",
      "Epoch [3/8], Step [483/750], Loss: 0.0675\n",
      "Epoch [3/8], Step [484/750], Loss: 0.0085\n",
      "Epoch [3/8], Step [485/750], Loss: 0.0741\n",
      "Epoch [3/8], Step [486/750], Loss: 0.0055\n",
      "Epoch [3/8], Step [487/750], Loss: 0.0360\n",
      "Epoch [3/8], Step [488/750], Loss: 0.0061\n",
      "Epoch [3/8], Step [489/750], Loss: 0.0188\n",
      "Epoch [3/8], Step [490/750], Loss: 0.1154\n",
      "Epoch [3/8], Step [491/750], Loss: 0.0136\n",
      "Epoch [3/8], Step [492/750], Loss: 0.0717\n",
      "Epoch [3/8], Step [493/750], Loss: 0.0233\n",
      "Epoch [3/8], Step [494/750], Loss: 0.0516\n",
      "Epoch [3/8], Step [495/750], Loss: 0.0084\n",
      "Epoch [3/8], Step [496/750], Loss: 0.0105\n",
      "Epoch [3/8], Step [497/750], Loss: 0.0190\n",
      "Epoch [3/8], Step [498/750], Loss: 0.1213\n",
      "Epoch [3/8], Step [499/750], Loss: 0.0068\n",
      "Epoch [3/8], Step [500/750], Loss: 0.0252\n",
      "Epoch [3/8], Step [501/750], Loss: 0.0119\n",
      "Epoch [3/8], Step [502/750], Loss: 0.0285\n",
      "Epoch [3/8], Step [503/750], Loss: 0.0221\n",
      "Epoch [3/8], Step [504/750], Loss: 0.0075\n",
      "Epoch [3/8], Step [505/750], Loss: 0.0460\n",
      "Epoch [3/8], Step [506/750], Loss: 0.1586\n",
      "Epoch [3/8], Step [507/750], Loss: 0.0201\n",
      "Epoch [3/8], Step [508/750], Loss: 0.0814\n",
      "Epoch [3/8], Step [509/750], Loss: 0.0103\n",
      "Epoch [3/8], Step [510/750], Loss: 0.0634\n",
      "Epoch [3/8], Step [511/750], Loss: 0.2205\n",
      "Epoch [3/8], Step [512/750], Loss: 0.1182\n",
      "Epoch [3/8], Step [513/750], Loss: 0.0153\n",
      "Epoch [3/8], Step [514/750], Loss: 0.0293\n",
      "Epoch [3/8], Step [515/750], Loss: 0.0260\n",
      "Epoch [3/8], Step [516/750], Loss: 0.1174\n",
      "Epoch [3/8], Step [517/750], Loss: 0.0310\n",
      "Epoch [3/8], Step [518/750], Loss: 0.0068\n",
      "Epoch [3/8], Step [519/750], Loss: 0.0282\n",
      "Epoch [3/8], Step [520/750], Loss: 0.0821\n",
      "Epoch [3/8], Step [521/750], Loss: 0.0395\n",
      "Epoch [3/8], Step [522/750], Loss: 0.0317\n",
      "Epoch [3/8], Step [523/750], Loss: 0.0240\n",
      "Epoch [3/8], Step [524/750], Loss: 0.0362\n",
      "Epoch [3/8], Step [525/750], Loss: 0.0651\n",
      "Epoch [3/8], Step [526/750], Loss: 0.0192\n",
      "Epoch [3/8], Step [527/750], Loss: 0.0229\n",
      "Epoch [3/8], Step [528/750], Loss: 0.0825\n",
      "Epoch [3/8], Step [529/750], Loss: 0.1009\n",
      "Epoch [3/8], Step [530/750], Loss: 0.0092\n",
      "Epoch [3/8], Step [531/750], Loss: 0.0062\n",
      "Epoch [3/8], Step [532/750], Loss: 0.0816\n",
      "Epoch [3/8], Step [533/750], Loss: 0.0727\n",
      "Epoch [3/8], Step [534/750], Loss: 0.1040\n",
      "Epoch [3/8], Step [535/750], Loss: 0.1010\n",
      "Epoch [3/8], Step [536/750], Loss: 0.0323\n",
      "Epoch [3/8], Step [537/750], Loss: 0.0503\n",
      "Epoch [3/8], Step [538/750], Loss: 0.0125\n",
      "Epoch [3/8], Step [539/750], Loss: 0.0411\n",
      "Epoch [3/8], Step [540/750], Loss: 0.0204\n",
      "Epoch [3/8], Step [541/750], Loss: 0.0267\n",
      "Epoch [3/8], Step [542/750], Loss: 0.0173\n",
      "Epoch [3/8], Step [543/750], Loss: 0.0085\n",
      "Epoch [3/8], Step [544/750], Loss: 0.0509\n",
      "Epoch [3/8], Step [545/750], Loss: 0.0673\n",
      "Epoch [3/8], Step [546/750], Loss: 0.0698\n",
      "Epoch [3/8], Step [547/750], Loss: 0.0089\n",
      "Epoch [3/8], Step [548/750], Loss: 0.0134\n",
      "Epoch [3/8], Step [549/750], Loss: 0.0085\n",
      "Epoch [3/8], Step [550/750], Loss: 0.0384\n",
      "Epoch [3/8], Step [551/750], Loss: 0.1177\n",
      "Epoch [3/8], Step [552/750], Loss: 0.0086\n",
      "Epoch [3/8], Step [553/750], Loss: 0.0376\n",
      "Epoch [3/8], Step [554/750], Loss: 0.0921\n",
      "Epoch [3/8], Step [555/750], Loss: 0.0222\n",
      "Epoch [3/8], Step [556/750], Loss: 0.0034\n",
      "Epoch [3/8], Step [557/750], Loss: 0.0181\n",
      "Epoch [3/8], Step [558/750], Loss: 0.0429\n",
      "Epoch [3/8], Step [559/750], Loss: 0.1100\n",
      "Epoch [3/8], Step [560/750], Loss: 0.0039\n",
      "Epoch [3/8], Step [561/750], Loss: 0.0116\n",
      "Epoch [3/8], Step [562/750], Loss: 0.0027\n",
      "Epoch [3/8], Step [563/750], Loss: 0.0205\n",
      "Epoch [3/8], Step [564/750], Loss: 0.0063\n",
      "Epoch [3/8], Step [565/750], Loss: 0.0074\n",
      "Epoch [3/8], Step [566/750], Loss: 0.0331\n",
      "Epoch [3/8], Step [567/750], Loss: 0.0179\n",
      "Epoch [3/8], Step [568/750], Loss: 0.0423\n",
      "Epoch [3/8], Step [569/750], Loss: 0.0183\n",
      "Epoch [3/8], Step [570/750], Loss: 0.0360\n",
      "Epoch [3/8], Step [571/750], Loss: 0.0395\n",
      "Epoch [3/8], Step [572/750], Loss: 0.0272\n",
      "Epoch [3/8], Step [573/750], Loss: 0.0157\n",
      "Epoch [3/8], Step [574/750], Loss: 0.1006\n",
      "Epoch [3/8], Step [575/750], Loss: 0.0033\n",
      "Epoch [3/8], Step [576/750], Loss: 0.0078\n",
      "Epoch [3/8], Step [577/750], Loss: 0.0380\n",
      "Epoch [3/8], Step [578/750], Loss: 0.0293\n",
      "Epoch [3/8], Step [579/750], Loss: 0.0271\n",
      "Epoch [3/8], Step [580/750], Loss: 0.0203\n",
      "Epoch [3/8], Step [581/750], Loss: 0.0048\n",
      "Epoch [3/8], Step [582/750], Loss: 0.0684\n",
      "Epoch [3/8], Step [583/750], Loss: 0.0280\n",
      "Epoch [3/8], Step [584/750], Loss: 0.0289\n",
      "Epoch [3/8], Step [585/750], Loss: 0.0580\n",
      "Epoch [3/8], Step [586/750], Loss: 0.0086\n",
      "Epoch [3/8], Step [587/750], Loss: 0.0092\n",
      "Epoch [3/8], Step [588/750], Loss: 0.0168\n",
      "Epoch [3/8], Step [589/750], Loss: 0.0265\n",
      "Epoch [3/8], Step [590/750], Loss: 0.0090\n",
      "Epoch [3/8], Step [591/750], Loss: 0.0800\n",
      "Epoch [3/8], Step [592/750], Loss: 0.0519\n",
      "Epoch [3/8], Step [593/750], Loss: 0.0039\n",
      "Epoch [3/8], Step [594/750], Loss: 0.0633\n",
      "Epoch [3/8], Step [595/750], Loss: 0.0417\n",
      "Epoch [3/8], Step [596/750], Loss: 0.0482\n",
      "Epoch [3/8], Step [597/750], Loss: 0.0206\n",
      "Epoch [3/8], Step [598/750], Loss: 0.1592\n",
      "Epoch [3/8], Step [599/750], Loss: 0.0054\n",
      "Epoch [3/8], Step [600/750], Loss: 0.0195\n",
      "Epoch [3/8], Step [601/750], Loss: 0.0268\n",
      "Epoch [3/8], Step [602/750], Loss: 0.0386\n",
      "Epoch [3/8], Step [603/750], Loss: 0.0080\n",
      "Epoch [3/8], Step [604/750], Loss: 0.0073\n",
      "Epoch [3/8], Step [605/750], Loss: 0.1201\n",
      "Epoch [3/8], Step [606/750], Loss: 0.1457\n",
      "Epoch [3/8], Step [607/750], Loss: 0.0615\n",
      "Epoch [3/8], Step [608/750], Loss: 0.1126\n",
      "Epoch [3/8], Step [609/750], Loss: 0.0372\n",
      "Epoch [3/8], Step [610/750], Loss: 0.0239\n",
      "Epoch [3/8], Step [611/750], Loss: 0.1878\n",
      "Epoch [3/8], Step [612/750], Loss: 0.0328\n",
      "Epoch [3/8], Step [613/750], Loss: 0.0057\n",
      "Epoch [3/8], Step [614/750], Loss: 0.0269\n",
      "Epoch [3/8], Step [615/750], Loss: 0.0163\n",
      "Epoch [3/8], Step [616/750], Loss: 0.0125\n",
      "Epoch [3/8], Step [617/750], Loss: 0.0455\n",
      "Epoch [3/8], Step [618/750], Loss: 0.1203\n",
      "Epoch [3/8], Step [619/750], Loss: 0.0309\n",
      "Epoch [3/8], Step [620/750], Loss: 0.0723\n",
      "Epoch [3/8], Step [621/750], Loss: 0.0524\n",
      "Epoch [3/8], Step [622/750], Loss: 0.0990\n",
      "Epoch [3/8], Step [623/750], Loss: 0.0431\n",
      "Epoch [3/8], Step [624/750], Loss: 0.0158\n",
      "Epoch [3/8], Step [625/750], Loss: 0.0277\n",
      "Epoch [3/8], Step [626/750], Loss: 0.0135\n",
      "Epoch [3/8], Step [627/750], Loss: 0.0893\n",
      "Epoch [3/8], Step [628/750], Loss: 0.0413\n",
      "Epoch [3/8], Step [629/750], Loss: 0.0099\n",
      "Epoch [3/8], Step [630/750], Loss: 0.0918\n",
      "Epoch [3/8], Step [631/750], Loss: 0.0045\n",
      "Epoch [3/8], Step [632/750], Loss: 0.0054\n",
      "Epoch [3/8], Step [633/750], Loss: 0.0149\n",
      "Epoch [3/8], Step [634/750], Loss: 0.0108\n",
      "Epoch [3/8], Step [635/750], Loss: 0.0235\n",
      "Epoch [3/8], Step [636/750], Loss: 0.1055\n",
      "Epoch [3/8], Step [637/750], Loss: 0.0479\n",
      "Epoch [3/8], Step [638/750], Loss: 0.0312\n",
      "Epoch [3/8], Step [639/750], Loss: 0.0682\n",
      "Epoch [3/8], Step [640/750], Loss: 0.0140\n",
      "Epoch [3/8], Step [641/750], Loss: 0.0994\n",
      "Epoch [3/8], Step [642/750], Loss: 0.0496\n",
      "Epoch [3/8], Step [643/750], Loss: 0.0743\n",
      "Epoch [3/8], Step [644/750], Loss: 0.0239\n",
      "Epoch [3/8], Step [645/750], Loss: 0.1970\n",
      "Epoch [3/8], Step [646/750], Loss: 0.0091\n",
      "Epoch [3/8], Step [647/750], Loss: 0.0651\n",
      "Epoch [3/8], Step [648/750], Loss: 0.1484\n",
      "Epoch [3/8], Step [649/750], Loss: 0.0073\n",
      "Epoch [3/8], Step [650/750], Loss: 0.0169\n",
      "Epoch [3/8], Step [651/750], Loss: 0.1400\n",
      "Epoch [3/8], Step [652/750], Loss: 0.1367\n",
      "Epoch [3/8], Step [653/750], Loss: 0.0130\n",
      "Epoch [3/8], Step [654/750], Loss: 0.0296\n",
      "Epoch [3/8], Step [655/750], Loss: 0.0182\n",
      "Epoch [3/8], Step [656/750], Loss: 0.1146\n",
      "Epoch [3/8], Step [657/750], Loss: 0.1460\n",
      "Epoch [3/8], Step [658/750], Loss: 0.0774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], Step [659/750], Loss: 0.0761\n",
      "Epoch [3/8], Step [660/750], Loss: 0.0414\n",
      "Epoch [3/8], Step [661/750], Loss: 0.0925\n",
      "Epoch [3/8], Step [662/750], Loss: 0.0107\n",
      "Epoch [3/8], Step [663/750], Loss: 0.0360\n",
      "Epoch [3/8], Step [664/750], Loss: 0.1137\n",
      "Epoch [3/8], Step [665/750], Loss: 0.0448\n",
      "Epoch [3/8], Step [666/750], Loss: 0.0192\n",
      "Epoch [3/8], Step [667/750], Loss: 0.0397\n",
      "Epoch [3/8], Step [668/750], Loss: 0.0393\n",
      "Epoch [3/8], Step [669/750], Loss: 0.1077\n",
      "Epoch [3/8], Step [670/750], Loss: 0.0482\n",
      "Epoch [3/8], Step [671/750], Loss: 0.0161\n",
      "Epoch [3/8], Step [672/750], Loss: 0.0622\n",
      "Epoch [3/8], Step [673/750], Loss: 0.0104\n",
      "Epoch [3/8], Step [674/750], Loss: 0.0726\n",
      "Epoch [3/8], Step [675/750], Loss: 0.0529\n",
      "Epoch [3/8], Step [676/750], Loss: 0.0872\n",
      "Epoch [3/8], Step [677/750], Loss: 0.0112\n",
      "Epoch [3/8], Step [678/750], Loss: 0.0204\n",
      "Epoch [3/8], Step [679/750], Loss: 0.0545\n",
      "Epoch [3/8], Step [680/750], Loss: 0.0227\n",
      "Epoch [3/8], Step [681/750], Loss: 0.0043\n",
      "Epoch [3/8], Step [682/750], Loss: 0.1545\n",
      "Epoch [3/8], Step [683/750], Loss: 0.0040\n",
      "Epoch [3/8], Step [684/750], Loss: 0.1281\n",
      "Epoch [3/8], Step [685/750], Loss: 0.0743\n",
      "Epoch [3/8], Step [686/750], Loss: 0.1219\n",
      "Epoch [3/8], Step [687/750], Loss: 0.0302\n",
      "Epoch [3/8], Step [688/750], Loss: 0.0484\n",
      "Epoch [3/8], Step [689/750], Loss: 0.0288\n",
      "Epoch [3/8], Step [690/750], Loss: 0.0178\n",
      "Epoch [3/8], Step [691/750], Loss: 0.0371\n",
      "Epoch [3/8], Step [692/750], Loss: 0.0168\n",
      "Epoch [3/8], Step [693/750], Loss: 0.0138\n",
      "Epoch [3/8], Step [694/750], Loss: 0.0386\n",
      "Epoch [3/8], Step [695/750], Loss: 0.0363\n",
      "Epoch [3/8], Step [696/750], Loss: 0.0195\n",
      "Epoch [3/8], Step [697/750], Loss: 0.0053\n",
      "Epoch [3/8], Step [698/750], Loss: 0.1459\n",
      "Epoch [3/8], Step [699/750], Loss: 0.0039\n",
      "Epoch [3/8], Step [700/750], Loss: 0.0789\n",
      "Epoch [3/8], Step [701/750], Loss: 0.0042\n",
      "Epoch [3/8], Step [702/750], Loss: 0.0875\n",
      "Epoch [3/8], Step [703/750], Loss: 0.0197\n",
      "Epoch [3/8], Step [704/750], Loss: 0.0472\n",
      "Epoch [3/8], Step [705/750], Loss: 0.0053\n",
      "Epoch [3/8], Step [706/750], Loss: 0.0554\n",
      "Epoch [3/8], Step [707/750], Loss: 0.0226\n",
      "Epoch [3/8], Step [708/750], Loss: 0.0388\n",
      "Epoch [3/8], Step [709/750], Loss: 0.0447\n",
      "Epoch [3/8], Step [710/750], Loss: 0.0875\n",
      "Epoch [3/8], Step [711/750], Loss: 0.1387\n",
      "Epoch [3/8], Step [712/750], Loss: 0.0289\n",
      "Epoch [3/8], Step [713/750], Loss: 0.1101\n",
      "Epoch [3/8], Step [714/750], Loss: 0.0116\n",
      "Epoch [3/8], Step [715/750], Loss: 0.1017\n",
      "Epoch [3/8], Step [716/750], Loss: 0.0810\n",
      "Epoch [3/8], Step [717/750], Loss: 0.0198\n",
      "Epoch [3/8], Step [718/750], Loss: 0.0421\n",
      "Epoch [3/8], Step [719/750], Loss: 0.0056\n",
      "Epoch [3/8], Step [720/750], Loss: 0.0974\n",
      "Epoch [3/8], Step [721/750], Loss: 0.0414\n",
      "Epoch [3/8], Step [722/750], Loss: 0.1633\n",
      "Epoch [3/8], Step [723/750], Loss: 0.1061\n",
      "Epoch [3/8], Step [724/750], Loss: 0.1056\n",
      "Epoch [3/8], Step [725/750], Loss: 0.0302\n",
      "Epoch [3/8], Step [726/750], Loss: 0.0107\n",
      "Epoch [3/8], Step [727/750], Loss: 0.0282\n",
      "Epoch [3/8], Step [728/750], Loss: 0.0775\n",
      "Epoch [3/8], Step [729/750], Loss: 0.0715\n",
      "Epoch [3/8], Step [730/750], Loss: 0.0707\n",
      "Epoch [3/8], Step [731/750], Loss: 0.0656\n",
      "Epoch [3/8], Step [732/750], Loss: 0.0382\n",
      "Epoch [3/8], Step [733/750], Loss: 0.0181\n",
      "Epoch [3/8], Step [734/750], Loss: 0.0112\n",
      "Epoch [3/8], Step [735/750], Loss: 0.1124\n",
      "Epoch [3/8], Step [736/750], Loss: 0.0391\n",
      "Epoch [3/8], Step [737/750], Loss: 0.0758\n",
      "Epoch [3/8], Step [738/750], Loss: 0.0056\n",
      "Epoch [3/8], Step [739/750], Loss: 0.0504\n",
      "Epoch [3/8], Step [740/750], Loss: 0.1048\n",
      "Epoch [3/8], Step [741/750], Loss: 0.1077\n",
      "Epoch [3/8], Step [742/750], Loss: 0.0202\n",
      "Epoch [3/8], Step [743/750], Loss: 0.0176\n",
      "Epoch [3/8], Step [744/750], Loss: 0.0425\n",
      "Epoch [3/8], Step [745/750], Loss: 0.0967\n",
      "Epoch [3/8], Step [746/750], Loss: 0.0576\n",
      "Epoch [3/8], Step [747/750], Loss: 0.0152\n",
      "Epoch [3/8], Step [748/750], Loss: 0.1196\n",
      "Epoch [3/8], Step [749/750], Loss: 0.0426\n",
      "Epoch [3/8], Step [750/750], Loss: 0.0111\n",
      "Epoch [3/8], Tr. loss: 0.2725. Test loss: 0.1914\n",
      "\n",
      "\n",
      "Epoch [4/8], Step [1/750], Loss: 0.0762\n",
      "Epoch [4/8], Step [2/750], Loss: 0.0551\n",
      "Epoch [4/8], Step [3/750], Loss: 0.0227\n",
      "Epoch [4/8], Step [4/750], Loss: 0.1752\n",
      "Epoch [4/8], Step [5/750], Loss: 0.0119\n",
      "Epoch [4/8], Step [6/750], Loss: 0.0439\n",
      "Epoch [4/8], Step [7/750], Loss: 0.0066\n",
      "Epoch [4/8], Step [8/750], Loss: 0.0310\n",
      "Epoch [4/8], Step [9/750], Loss: 0.0255\n",
      "Epoch [4/8], Step [10/750], Loss: 0.1228\n",
      "Epoch [4/8], Step [11/750], Loss: 0.0077\n",
      "Epoch [4/8], Step [12/750], Loss: 0.0042\n",
      "Epoch [4/8], Step [13/750], Loss: 0.1066\n",
      "Epoch [4/8], Step [14/750], Loss: 0.0022\n",
      "Epoch [4/8], Step [15/750], Loss: 0.0692\n",
      "Epoch [4/8], Step [16/750], Loss: 0.0409\n",
      "Epoch [4/8], Step [17/750], Loss: 0.0652\n",
      "Epoch [4/8], Step [18/750], Loss: 0.0851\n",
      "Epoch [4/8], Step [19/750], Loss: 0.0035\n",
      "Epoch [4/8], Step [20/750], Loss: 0.0059\n",
      "Epoch [4/8], Step [21/750], Loss: 0.0557\n",
      "Epoch [4/8], Step [22/750], Loss: 0.0324\n",
      "Epoch [4/8], Step [23/750], Loss: 0.0220\n",
      "Epoch [4/8], Step [24/750], Loss: 0.0078\n",
      "Epoch [4/8], Step [25/750], Loss: 0.0200\n",
      "Epoch [4/8], Step [26/750], Loss: 0.0150\n",
      "Epoch [4/8], Step [27/750], Loss: 0.1416\n",
      "Epoch [4/8], Step [28/750], Loss: 0.0968\n",
      "Epoch [4/8], Step [29/750], Loss: 0.0105\n",
      "Epoch [4/8], Step [30/750], Loss: 0.0569\n",
      "Epoch [4/8], Step [31/750], Loss: 0.0082\n",
      "Epoch [4/8], Step [32/750], Loss: 0.0229\n",
      "Epoch [4/8], Step [33/750], Loss: 0.0722\n",
      "Epoch [4/8], Step [34/750], Loss: 0.0073\n",
      "Epoch [4/8], Step [35/750], Loss: 0.0347\n",
      "Epoch [4/8], Step [36/750], Loss: 0.0395\n",
      "Epoch [4/8], Step [37/750], Loss: 0.1390\n",
      "Epoch [4/8], Step [38/750], Loss: 0.0758\n",
      "Epoch [4/8], Step [39/750], Loss: 0.0046\n",
      "Epoch [4/8], Step [40/750], Loss: 0.0181\n",
      "Epoch [4/8], Step [41/750], Loss: 0.0082\n",
      "Epoch [4/8], Step [42/750], Loss: 0.0070\n",
      "Epoch [4/8], Step [43/750], Loss: 0.0358\n",
      "Epoch [4/8], Step [44/750], Loss: 0.0179\n",
      "Epoch [4/8], Step [45/750], Loss: 0.0667\n",
      "Epoch [4/8], Step [46/750], Loss: 0.0079\n",
      "Epoch [4/8], Step [47/750], Loss: 0.0392\n",
      "Epoch [4/8], Step [48/750], Loss: 0.0661\n",
      "Epoch [4/8], Step [49/750], Loss: 0.0569\n",
      "Epoch [4/8], Step [50/750], Loss: 0.0154\n",
      "Epoch [4/8], Step [51/750], Loss: 0.0010\n",
      "Epoch [4/8], Step [52/750], Loss: 0.0387\n",
      "Epoch [4/8], Step [53/750], Loss: 0.0182\n",
      "Epoch [4/8], Step [54/750], Loss: 0.0202\n",
      "Epoch [4/8], Step [55/750], Loss: 0.0347\n",
      "Epoch [4/8], Step [56/750], Loss: 0.0971\n",
      "Epoch [4/8], Step [57/750], Loss: 0.0561\n",
      "Epoch [4/8], Step [58/750], Loss: 0.0038\n",
      "Epoch [4/8], Step [59/750], Loss: 0.0897\n",
      "Epoch [4/8], Step [60/750], Loss: 0.0043\n",
      "Epoch [4/8], Step [61/750], Loss: 0.0180\n",
      "Epoch [4/8], Step [62/750], Loss: 0.0481\n",
      "Epoch [4/8], Step [63/750], Loss: 0.1855\n",
      "Epoch [4/8], Step [64/750], Loss: 0.0013\n",
      "Epoch [4/8], Step [65/750], Loss: 0.0122\n",
      "Epoch [4/8], Step [66/750], Loss: 0.0616\n",
      "Epoch [4/8], Step [67/750], Loss: 0.0608\n",
      "Epoch [4/8], Step [68/750], Loss: 0.0144\n",
      "Epoch [4/8], Step [69/750], Loss: 0.0050\n",
      "Epoch [4/8], Step [70/750], Loss: 0.0627\n",
      "Epoch [4/8], Step [71/750], Loss: 0.0181\n",
      "Epoch [4/8], Step [72/750], Loss: 0.0190\n",
      "Epoch [4/8], Step [73/750], Loss: 0.0392\n",
      "Epoch [4/8], Step [74/750], Loss: 0.0409\n",
      "Epoch [4/8], Step [75/750], Loss: 0.0437\n",
      "Epoch [4/8], Step [76/750], Loss: 0.0110\n",
      "Epoch [4/8], Step [77/750], Loss: 0.0222\n",
      "Epoch [4/8], Step [78/750], Loss: 0.0029\n",
      "Epoch [4/8], Step [79/750], Loss: 0.0091\n",
      "Epoch [4/8], Step [80/750], Loss: 0.0110\n",
      "Epoch [4/8], Step [81/750], Loss: 0.0316\n",
      "Epoch [4/8], Step [82/750], Loss: 0.0053\n",
      "Epoch [4/8], Step [83/750], Loss: 0.0369\n",
      "Epoch [4/8], Step [84/750], Loss: 0.0233\n",
      "Epoch [4/8], Step [85/750], Loss: 0.0057\n",
      "Epoch [4/8], Step [86/750], Loss: 0.0733\n",
      "Epoch [4/8], Step [87/750], Loss: 0.0031\n",
      "Epoch [4/8], Step [88/750], Loss: 0.0314\n",
      "Epoch [4/8], Step [89/750], Loss: 0.0348\n",
      "Epoch [4/8], Step [90/750], Loss: 0.0014\n",
      "Epoch [4/8], Step [91/750], Loss: 0.0199\n",
      "Epoch [4/8], Step [92/750], Loss: 0.0845\n",
      "Epoch [4/8], Step [93/750], Loss: 0.0328\n",
      "Epoch [4/8], Step [94/750], Loss: 0.0160\n",
      "Epoch [4/8], Step [95/750], Loss: 0.0055\n",
      "Epoch [4/8], Step [96/750], Loss: 0.0378\n",
      "Epoch [4/8], Step [97/750], Loss: 0.0214\n",
      "Epoch [4/8], Step [98/750], Loss: 0.1352\n",
      "Epoch [4/8], Step [99/750], Loss: 0.0871\n",
      "Epoch [4/8], Step [100/750], Loss: 0.0217\n",
      "Epoch [4/8], Step [101/750], Loss: 0.0194\n",
      "Epoch [4/8], Step [102/750], Loss: 0.0490\n",
      "Epoch [4/8], Step [103/750], Loss: 0.0394\n",
      "Epoch [4/8], Step [104/750], Loss: 0.0310\n",
      "Epoch [4/8], Step [105/750], Loss: 0.0962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Step [106/750], Loss: 0.0041\n",
      "Epoch [4/8], Step [107/750], Loss: 0.0358\n",
      "Epoch [4/8], Step [108/750], Loss: 0.0504\n",
      "Epoch [4/8], Step [109/750], Loss: 0.0931\n",
      "Epoch [4/8], Step [110/750], Loss: 0.0664\n",
      "Epoch [4/8], Step [111/750], Loss: 0.0430\n",
      "Epoch [4/8], Step [112/750], Loss: 0.0094\n",
      "Epoch [4/8], Step [113/750], Loss: 0.0059\n",
      "Epoch [4/8], Step [114/750], Loss: 0.0053\n",
      "Epoch [4/8], Step [115/750], Loss: 0.0055\n",
      "Epoch [4/8], Step [116/750], Loss: 0.0161\n",
      "Epoch [4/8], Step [117/750], Loss: 0.0385\n",
      "Epoch [4/8], Step [118/750], Loss: 0.0110\n",
      "Epoch [4/8], Step [119/750], Loss: 0.0373\n",
      "Epoch [4/8], Step [120/750], Loss: 0.0041\n",
      "Epoch [4/8], Step [121/750], Loss: 0.0261\n",
      "Epoch [4/8], Step [122/750], Loss: 0.0090\n",
      "Epoch [4/8], Step [123/750], Loss: 0.0356\n",
      "Epoch [4/8], Step [124/750], Loss: 0.0367\n",
      "Epoch [4/8], Step [125/750], Loss: 0.0113\n",
      "Epoch [4/8], Step [126/750], Loss: 0.0345\n",
      "Epoch [4/8], Step [127/750], Loss: 0.0610\n",
      "Epoch [4/8], Step [128/750], Loss: 0.1017\n",
      "Epoch [4/8], Step [129/750], Loss: 0.0720\n",
      "Epoch [4/8], Step [130/750], Loss: 0.0119\n",
      "Epoch [4/8], Step [131/750], Loss: 0.0282\n",
      "Epoch [4/8], Step [132/750], Loss: 0.0063\n",
      "Epoch [4/8], Step [133/750], Loss: 0.0295\n",
      "Epoch [4/8], Step [134/750], Loss: 0.0580\n",
      "Epoch [4/8], Step [135/750], Loss: 0.0033\n",
      "Epoch [4/8], Step [136/750], Loss: 0.0162\n",
      "Epoch [4/8], Step [137/750], Loss: 0.0310\n",
      "Epoch [4/8], Step [138/750], Loss: 0.0760\n",
      "Epoch [4/8], Step [139/750], Loss: 0.0241\n",
      "Epoch [4/8], Step [140/750], Loss: 0.0974\n",
      "Epoch [4/8], Step [141/750], Loss: 0.0084\n",
      "Epoch [4/8], Step [142/750], Loss: 0.0139\n",
      "Epoch [4/8], Step [143/750], Loss: 0.0436\n",
      "Epoch [4/8], Step [144/750], Loss: 0.0082\n",
      "Epoch [4/8], Step [145/750], Loss: 0.0026\n",
      "Epoch [4/8], Step [146/750], Loss: 0.0048\n",
      "Epoch [4/8], Step [147/750], Loss: 0.0046\n",
      "Epoch [4/8], Step [148/750], Loss: 0.0204\n",
      "Epoch [4/8], Step [149/750], Loss: 0.0095\n",
      "Epoch [4/8], Step [150/750], Loss: 0.0181\n",
      "Epoch [4/8], Step [151/750], Loss: 0.0208\n",
      "Epoch [4/8], Step [152/750], Loss: 0.0059\n",
      "Epoch [4/8], Step [153/750], Loss: 0.0136\n",
      "Epoch [4/8], Step [154/750], Loss: 0.1092\n",
      "Epoch [4/8], Step [155/750], Loss: 0.0062\n",
      "Epoch [4/8], Step [156/750], Loss: 0.0282\n",
      "Epoch [4/8], Step [157/750], Loss: 0.0460\n",
      "Epoch [4/8], Step [158/750], Loss: 0.0024\n",
      "Epoch [4/8], Step [159/750], Loss: 0.0670\n",
      "Epoch [4/8], Step [160/750], Loss: 0.0816\n",
      "Epoch [4/8], Step [161/750], Loss: 0.0131\n",
      "Epoch [4/8], Step [162/750], Loss: 0.0465\n",
      "Epoch [4/8], Step [163/750], Loss: 0.0680\n",
      "Epoch [4/8], Step [164/750], Loss: 0.0630\n",
      "Epoch [4/8], Step [165/750], Loss: 0.1064\n",
      "Epoch [4/8], Step [166/750], Loss: 0.0033\n",
      "Epoch [4/8], Step [167/750], Loss: 0.0210\n",
      "Epoch [4/8], Step [168/750], Loss: 0.0019\n",
      "Epoch [4/8], Step [169/750], Loss: 0.0098\n",
      "Epoch [4/8], Step [170/750], Loss: 0.0013\n",
      "Epoch [4/8], Step [171/750], Loss: 0.0322\n",
      "Epoch [4/8], Step [172/750], Loss: 0.0647\n",
      "Epoch [4/8], Step [173/750], Loss: 0.0433\n",
      "Epoch [4/8], Step [174/750], Loss: 0.0471\n",
      "Epoch [4/8], Step [175/750], Loss: 0.0218\n",
      "Epoch [4/8], Step [176/750], Loss: 0.0325\n",
      "Epoch [4/8], Step [177/750], Loss: 0.0582\n",
      "Epoch [4/8], Step [178/750], Loss: 0.0056\n",
      "Epoch [4/8], Step [179/750], Loss: 0.0261\n",
      "Epoch [4/8], Step [180/750], Loss: 0.0036\n",
      "Epoch [4/8], Step [181/750], Loss: 0.0250\n",
      "Epoch [4/8], Step [182/750], Loss: 0.0321\n",
      "Epoch [4/8], Step [183/750], Loss: 0.0445\n",
      "Epoch [4/8], Step [184/750], Loss: 0.1051\n",
      "Epoch [4/8], Step [185/750], Loss: 0.0618\n",
      "Epoch [4/8], Step [186/750], Loss: 0.1901\n",
      "Epoch [4/8], Step [187/750], Loss: 0.0695\n",
      "Epoch [4/8], Step [188/750], Loss: 0.0314\n",
      "Epoch [4/8], Step [189/750], Loss: 0.0984\n",
      "Epoch [4/8], Step [190/750], Loss: 0.0915\n",
      "Epoch [4/8], Step [191/750], Loss: 0.0470\n",
      "Epoch [4/8], Step [192/750], Loss: 0.0047\n",
      "Epoch [4/8], Step [193/750], Loss: 0.0991\n",
      "Epoch [4/8], Step [194/750], Loss: 0.0365\n",
      "Epoch [4/8], Step [195/750], Loss: 0.0397\n",
      "Epoch [4/8], Step [196/750], Loss: 0.0021\n",
      "Epoch [4/8], Step [197/750], Loss: 0.1116\n",
      "Epoch [4/8], Step [198/750], Loss: 0.0095\n",
      "Epoch [4/8], Step [199/750], Loss: 0.0339\n",
      "Epoch [4/8], Step [200/750], Loss: 0.0900\n",
      "Epoch [4/8], Step [201/750], Loss: 0.0234\n",
      "Epoch [4/8], Step [202/750], Loss: 0.0125\n",
      "Epoch [4/8], Step [203/750], Loss: 0.0825\n",
      "Epoch [4/8], Step [204/750], Loss: 0.1359\n",
      "Epoch [4/8], Step [205/750], Loss: 0.0995\n",
      "Epoch [4/8], Step [206/750], Loss: 0.1044\n",
      "Epoch [4/8], Step [207/750], Loss: 0.0341\n",
      "Epoch [4/8], Step [208/750], Loss: 0.0579\n",
      "Epoch [4/8], Step [209/750], Loss: 0.1030\n",
      "Epoch [4/8], Step [210/750], Loss: 0.0319\n",
      "Epoch [4/8], Step [211/750], Loss: 0.0117\n",
      "Epoch [4/8], Step [212/750], Loss: 0.0512\n",
      "Epoch [4/8], Step [213/750], Loss: 0.0033\n",
      "Epoch [4/8], Step [214/750], Loss: 0.0388\n",
      "Epoch [4/8], Step [215/750], Loss: 0.1358\n",
      "Epoch [4/8], Step [216/750], Loss: 0.0923\n",
      "Epoch [4/8], Step [217/750], Loss: 0.0227\n",
      "Epoch [4/8], Step [218/750], Loss: 0.0409\n",
      "Epoch [4/8], Step [219/750], Loss: 0.0865\n",
      "Epoch [4/8], Step [220/750], Loss: 0.0386\n",
      "Epoch [4/8], Step [221/750], Loss: 0.0365\n",
      "Epoch [4/8], Step [222/750], Loss: 0.0202\n",
      "Epoch [4/8], Step [223/750], Loss: 0.0079\n",
      "Epoch [4/8], Step [224/750], Loss: 0.0653\n",
      "Epoch [4/8], Step [225/750], Loss: 0.0078\n",
      "Epoch [4/8], Step [226/750], Loss: 0.0847\n",
      "Epoch [4/8], Step [227/750], Loss: 0.0266\n",
      "Epoch [4/8], Step [228/750], Loss: 0.0290\n",
      "Epoch [4/8], Step [229/750], Loss: 0.0029\n",
      "Epoch [4/8], Step [230/750], Loss: 0.0515\n",
      "Epoch [4/8], Step [231/750], Loss: 0.0131\n",
      "Epoch [4/8], Step [232/750], Loss: 0.0190\n",
      "Epoch [4/8], Step [233/750], Loss: 0.0057\n",
      "Epoch [4/8], Step [234/750], Loss: 0.0335\n",
      "Epoch [4/8], Step [235/750], Loss: 0.0501\n",
      "Epoch [4/8], Step [236/750], Loss: 0.0336\n",
      "Epoch [4/8], Step [237/750], Loss: 0.0071\n",
      "Epoch [4/8], Step [238/750], Loss: 0.0046\n",
      "Epoch [4/8], Step [239/750], Loss: 0.1054\n",
      "Epoch [4/8], Step [240/750], Loss: 0.0184\n",
      "Epoch [4/8], Step [241/750], Loss: 0.0050\n",
      "Epoch [4/8], Step [242/750], Loss: 0.0155\n",
      "Epoch [4/8], Step [243/750], Loss: 0.0470\n",
      "Epoch [4/8], Step [244/750], Loss: 0.0075\n",
      "Epoch [4/8], Step [245/750], Loss: 0.0044\n",
      "Epoch [4/8], Step [246/750], Loss: 0.0032\n",
      "Epoch [4/8], Step [247/750], Loss: 0.0121\n",
      "Epoch [4/8], Step [248/750], Loss: 0.0457\n",
      "Epoch [4/8], Step [249/750], Loss: 0.0073\n",
      "Epoch [4/8], Step [250/750], Loss: 0.0398\n",
      "Epoch [4/8], Step [251/750], Loss: 0.0012\n",
      "Epoch [4/8], Step [252/750], Loss: 0.0215\n",
      "Epoch [4/8], Step [253/750], Loss: 0.0051\n",
      "Epoch [4/8], Step [254/750], Loss: 0.0043\n",
      "Epoch [4/8], Step [255/750], Loss: 0.0164\n",
      "Epoch [4/8], Step [256/750], Loss: 0.0608\n",
      "Epoch [4/8], Step [257/750], Loss: 0.0627\n",
      "Epoch [4/8], Step [258/750], Loss: 0.0825\n",
      "Epoch [4/8], Step [259/750], Loss: 0.0400\n",
      "Epoch [4/8], Step [260/750], Loss: 0.0446\n",
      "Epoch [4/8], Step [261/750], Loss: 0.0024\n",
      "Epoch [4/8], Step [262/750], Loss: 0.0253\n",
      "Epoch [4/8], Step [263/750], Loss: 0.0028\n",
      "Epoch [4/8], Step [264/750], Loss: 0.0103\n",
      "Epoch [4/8], Step [265/750], Loss: 0.0211\n",
      "Epoch [4/8], Step [266/750], Loss: 0.0337\n",
      "Epoch [4/8], Step [267/750], Loss: 0.0046\n",
      "Epoch [4/8], Step [268/750], Loss: 0.0109\n",
      "Epoch [4/8], Step [269/750], Loss: 0.0124\n",
      "Epoch [4/8], Step [270/750], Loss: 0.0030\n",
      "Epoch [4/8], Step [271/750], Loss: 0.0454\n",
      "Epoch [4/8], Step [272/750], Loss: 0.0011\n",
      "Epoch [4/8], Step [273/750], Loss: 0.0170\n",
      "Epoch [4/8], Step [274/750], Loss: 0.0375\n",
      "Epoch [4/8], Step [275/750], Loss: 0.0146\n",
      "Epoch [4/8], Step [276/750], Loss: 0.0126\n",
      "Epoch [4/8], Step [277/750], Loss: 0.0317\n",
      "Epoch [4/8], Step [278/750], Loss: 0.0029\n",
      "Epoch [4/8], Step [279/750], Loss: 0.0044\n",
      "Epoch [4/8], Step [280/750], Loss: 0.1240\n",
      "Epoch [4/8], Step [281/750], Loss: 0.0046\n",
      "Epoch [4/8], Step [282/750], Loss: 0.0282\n",
      "Epoch [4/8], Step [283/750], Loss: 0.0013\n",
      "Epoch [4/8], Step [284/750], Loss: 0.0036\n",
      "Epoch [4/8], Step [285/750], Loss: 0.0396\n",
      "Epoch [4/8], Step [286/750], Loss: 0.0055\n",
      "Epoch [4/8], Step [287/750], Loss: 0.1084\n",
      "Epoch [4/8], Step [288/750], Loss: 0.0283\n",
      "Epoch [4/8], Step [289/750], Loss: 0.0400\n",
      "Epoch [4/8], Step [290/750], Loss: 0.0834\n",
      "Epoch [4/8], Step [291/750], Loss: 0.0529\n",
      "Epoch [4/8], Step [292/750], Loss: 0.0120\n",
      "Epoch [4/8], Step [293/750], Loss: 0.0022\n",
      "Epoch [4/8], Step [294/750], Loss: 0.0456\n",
      "Epoch [4/8], Step [295/750], Loss: 0.0165\n",
      "Epoch [4/8], Step [296/750], Loss: 0.0226\n",
      "Epoch [4/8], Step [297/750], Loss: 0.1066\n",
      "Epoch [4/8], Step [298/750], Loss: 0.0037\n",
      "Epoch [4/8], Step [299/750], Loss: 0.0145\n",
      "Epoch [4/8], Step [300/750], Loss: 0.0859\n",
      "Epoch [4/8], Step [301/750], Loss: 0.0477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Step [302/750], Loss: 0.0776\n",
      "Epoch [4/8], Step [303/750], Loss: 0.0419\n",
      "Epoch [4/8], Step [304/750], Loss: 0.0182\n",
      "Epoch [4/8], Step [305/750], Loss: 0.0072\n",
      "Epoch [4/8], Step [306/750], Loss: 0.0989\n",
      "Epoch [4/8], Step [307/750], Loss: 0.0069\n",
      "Epoch [4/8], Step [308/750], Loss: 0.0559\n",
      "Epoch [4/8], Step [309/750], Loss: 0.0670\n",
      "Epoch [4/8], Step [310/750], Loss: 0.0830\n",
      "Epoch [4/8], Step [311/750], Loss: 0.0042\n",
      "Epoch [4/8], Step [312/750], Loss: 0.0347\n",
      "Epoch [4/8], Step [313/750], Loss: 0.0237\n",
      "Epoch [4/8], Step [314/750], Loss: 0.0036\n",
      "Epoch [4/8], Step [315/750], Loss: 0.0043\n",
      "Epoch [4/8], Step [316/750], Loss: 0.0079\n",
      "Epoch [4/8], Step [317/750], Loss: 0.0159\n",
      "Epoch [4/8], Step [318/750], Loss: 0.0654\n",
      "Epoch [4/8], Step [319/750], Loss: 0.0577\n",
      "Epoch [4/8], Step [320/750], Loss: 0.0056\n",
      "Epoch [4/8], Step [321/750], Loss: 0.0671\n",
      "Epoch [4/8], Step [322/750], Loss: 0.0024\n",
      "Epoch [4/8], Step [323/750], Loss: 0.0579\n",
      "Epoch [4/8], Step [324/750], Loss: 0.0261\n",
      "Epoch [4/8], Step [325/750], Loss: 0.0081\n",
      "Epoch [4/8], Step [326/750], Loss: 0.0631\n",
      "Epoch [4/8], Step [327/750], Loss: 0.1040\n",
      "Epoch [4/8], Step [328/750], Loss: 0.0736\n",
      "Epoch [4/8], Step [329/750], Loss: 0.0558\n",
      "Epoch [4/8], Step [330/750], Loss: 0.0195\n",
      "Epoch [4/8], Step [331/750], Loss: 0.1316\n",
      "Epoch [4/8], Step [332/750], Loss: 0.0026\n",
      "Epoch [4/8], Step [333/750], Loss: 0.0443\n",
      "Epoch [4/8], Step [334/750], Loss: 0.0352\n",
      "Epoch [4/8], Step [335/750], Loss: 0.0364\n",
      "Epoch [4/8], Step [336/750], Loss: 0.0082\n",
      "Epoch [4/8], Step [337/750], Loss: 0.0139\n",
      "Epoch [4/8], Step [338/750], Loss: 0.0143\n",
      "Epoch [4/8], Step [339/750], Loss: 0.0170\n",
      "Epoch [4/8], Step [340/750], Loss: 0.0356\n",
      "Epoch [4/8], Step [341/750], Loss: 0.0195\n",
      "Epoch [4/8], Step [342/750], Loss: 0.0380\n",
      "Epoch [4/8], Step [343/750], Loss: 0.0336\n",
      "Epoch [4/8], Step [344/750], Loss: 0.0045\n",
      "Epoch [4/8], Step [345/750], Loss: 0.1047\n",
      "Epoch [4/8], Step [346/750], Loss: 0.0102\n",
      "Epoch [4/8], Step [347/750], Loss: 0.0286\n",
      "Epoch [4/8], Step [348/750], Loss: 0.0063\n",
      "Epoch [4/8], Step [349/750], Loss: 0.0059\n",
      "Epoch [4/8], Step [350/750], Loss: 0.0454\n",
      "Epoch [4/8], Step [351/750], Loss: 0.0469\n",
      "Epoch [4/8], Step [352/750], Loss: 0.0504\n",
      "Epoch [4/8], Step [353/750], Loss: 0.1499\n",
      "Epoch [4/8], Step [354/750], Loss: 0.0152\n",
      "Epoch [4/8], Step [355/750], Loss: 0.0550\n",
      "Epoch [4/8], Step [356/750], Loss: 0.1181\n",
      "Epoch [4/8], Step [357/750], Loss: 0.0451\n",
      "Epoch [4/8], Step [358/750], Loss: 0.0549\n",
      "Epoch [4/8], Step [359/750], Loss: 0.0034\n",
      "Epoch [4/8], Step [360/750], Loss: 0.0126\n",
      "Epoch [4/8], Step [361/750], Loss: 0.0258\n",
      "Epoch [4/8], Step [362/750], Loss: 0.0636\n",
      "Epoch [4/8], Step [363/750], Loss: 0.0180\n",
      "Epoch [4/8], Step [364/750], Loss: 0.0241\n",
      "Epoch [4/8], Step [365/750], Loss: 0.0109\n",
      "Epoch [4/8], Step [366/750], Loss: 0.0292\n",
      "Epoch [4/8], Step [367/750], Loss: 0.0622\n",
      "Epoch [4/8], Step [368/750], Loss: 0.0149\n",
      "Epoch [4/8], Step [369/750], Loss: 0.0033\n",
      "Epoch [4/8], Step [370/750], Loss: 0.0225\n",
      "Epoch [4/8], Step [371/750], Loss: 0.0177\n",
      "Epoch [4/8], Step [372/750], Loss: 0.0070\n",
      "Epoch [4/8], Step [373/750], Loss: 0.0238\n",
      "Epoch [4/8], Step [374/750], Loss: 0.0028\n",
      "Epoch [4/8], Step [375/750], Loss: 0.0174\n",
      "Epoch [4/8], Step [376/750], Loss: 0.0388\n",
      "Epoch [4/8], Step [377/750], Loss: 0.0228\n",
      "Epoch [4/8], Step [378/750], Loss: 0.0501\n",
      "Epoch [4/8], Step [379/750], Loss: 0.0482\n",
      "Epoch [4/8], Step [380/750], Loss: 0.0081\n",
      "Epoch [4/8], Step [381/750], Loss: 0.0821\n",
      "Epoch [4/8], Step [382/750], Loss: 0.0516\n",
      "Epoch [4/8], Step [383/750], Loss: 0.0221\n",
      "Epoch [4/8], Step [384/750], Loss: 0.0316\n",
      "Epoch [4/8], Step [385/750], Loss: 0.0186\n",
      "Epoch [4/8], Step [386/750], Loss: 0.0563\n",
      "Epoch [4/8], Step [387/750], Loss: 0.0143\n",
      "Epoch [4/8], Step [388/750], Loss: 0.0901\n",
      "Epoch [4/8], Step [389/750], Loss: 0.0212\n",
      "Epoch [4/8], Step [390/750], Loss: 0.0026\n",
      "Epoch [4/8], Step [391/750], Loss: 0.0310\n",
      "Epoch [4/8], Step [392/750], Loss: 0.0022\n",
      "Epoch [4/8], Step [393/750], Loss: 0.0839\n",
      "Epoch [4/8], Step [394/750], Loss: 0.0120\n",
      "Epoch [4/8], Step [395/750], Loss: 0.0838\n",
      "Epoch [4/8], Step [396/750], Loss: 0.1236\n",
      "Epoch [4/8], Step [397/750], Loss: 0.0798\n",
      "Epoch [4/8], Step [398/750], Loss: 0.0907\n",
      "Epoch [4/8], Step [399/750], Loss: 0.0690\n",
      "Epoch [4/8], Step [400/750], Loss: 0.0052\n",
      "Epoch [4/8], Step [401/750], Loss: 0.0075\n",
      "Epoch [4/8], Step [402/750], Loss: 0.0421\n",
      "Epoch [4/8], Step [403/750], Loss: 0.0383\n",
      "Epoch [4/8], Step [404/750], Loss: 0.0526\n",
      "Epoch [4/8], Step [405/750], Loss: 0.0440\n",
      "Epoch [4/8], Step [406/750], Loss: 0.0513\n",
      "Epoch [4/8], Step [407/750], Loss: 0.0262\n",
      "Epoch [4/8], Step [408/750], Loss: 0.1161\n",
      "Epoch [4/8], Step [409/750], Loss: 0.0071\n",
      "Epoch [4/8], Step [410/750], Loss: 0.0119\n",
      "Epoch [4/8], Step [411/750], Loss: 0.0039\n",
      "Epoch [4/8], Step [412/750], Loss: 0.0554\n",
      "Epoch [4/8], Step [413/750], Loss: 0.0223\n",
      "Epoch [4/8], Step [414/750], Loss: 0.0037\n",
      "Epoch [4/8], Step [415/750], Loss: 0.0916\n",
      "Epoch [4/8], Step [416/750], Loss: 0.0414\n",
      "Epoch [4/8], Step [417/750], Loss: 0.0516\n",
      "Epoch [4/8], Step [418/750], Loss: 0.0251\n",
      "Epoch [4/8], Step [419/750], Loss: 0.0045\n",
      "Epoch [4/8], Step [420/750], Loss: 0.0333\n",
      "Epoch [4/8], Step [421/750], Loss: 0.0103\n",
      "Epoch [4/8], Step [422/750], Loss: 0.1304\n",
      "Epoch [4/8], Step [423/750], Loss: 0.0165\n",
      "Epoch [4/8], Step [424/750], Loss: 0.0358\n",
      "Epoch [4/8], Step [425/750], Loss: 0.1066\n",
      "Epoch [4/8], Step [426/750], Loss: 0.0154\n",
      "Epoch [4/8], Step [427/750], Loss: 0.0174\n",
      "Epoch [4/8], Step [428/750], Loss: 0.0098\n",
      "Epoch [4/8], Step [429/750], Loss: 0.0701\n",
      "Epoch [4/8], Step [430/750], Loss: 0.0114\n",
      "Epoch [4/8], Step [431/750], Loss: 0.0108\n",
      "Epoch [4/8], Step [432/750], Loss: 0.0446\n",
      "Epoch [4/8], Step [433/750], Loss: 0.0601\n",
      "Epoch [4/8], Step [434/750], Loss: 0.1121\n",
      "Epoch [4/8], Step [435/750], Loss: 0.0835\n",
      "Epoch [4/8], Step [436/750], Loss: 0.0043\n",
      "Epoch [4/8], Step [437/750], Loss: 0.0764\n",
      "Epoch [4/8], Step [438/750], Loss: 0.0394\n",
      "Epoch [4/8], Step [439/750], Loss: 0.0215\n",
      "Epoch [4/8], Step [440/750], Loss: 0.0297\n",
      "Epoch [4/8], Step [441/750], Loss: 0.0186\n",
      "Epoch [4/8], Step [442/750], Loss: 0.0861\n",
      "Epoch [4/8], Step [443/750], Loss: 0.0351\n",
      "Epoch [4/8], Step [444/750], Loss: 0.0764\n",
      "Epoch [4/8], Step [445/750], Loss: 0.0229\n",
      "Epoch [4/8], Step [446/750], Loss: 0.0338\n",
      "Epoch [4/8], Step [447/750], Loss: 0.0382\n",
      "Epoch [4/8], Step [448/750], Loss: 0.0450\n",
      "Epoch [4/8], Step [449/750], Loss: 0.0429\n",
      "Epoch [4/8], Step [450/750], Loss: 0.1151\n",
      "Epoch [4/8], Step [451/750], Loss: 0.0575\n",
      "Epoch [4/8], Step [452/750], Loss: 0.0218\n",
      "Epoch [4/8], Step [453/750], Loss: 0.0417\n",
      "Epoch [4/8], Step [454/750], Loss: 0.0099\n",
      "Epoch [4/8], Step [455/750], Loss: 0.0328\n",
      "Epoch [4/8], Step [456/750], Loss: 0.0114\n",
      "Epoch [4/8], Step [457/750], Loss: 0.0438\n",
      "Epoch [4/8], Step [458/750], Loss: 0.0357\n",
      "Epoch [4/8], Step [459/750], Loss: 0.0152\n",
      "Epoch [4/8], Step [460/750], Loss: 0.0141\n",
      "Epoch [4/8], Step [461/750], Loss: 0.1749\n",
      "Epoch [4/8], Step [462/750], Loss: 0.0026\n",
      "Epoch [4/8], Step [463/750], Loss: 0.0026\n",
      "Epoch [4/8], Step [464/750], Loss: 0.0430\n",
      "Epoch [4/8], Step [465/750], Loss: 0.0995\n",
      "Epoch [4/8], Step [466/750], Loss: 0.0540\n",
      "Epoch [4/8], Step [467/750], Loss: 0.0542\n",
      "Epoch [4/8], Step [468/750], Loss: 0.0078\n",
      "Epoch [4/8], Step [469/750], Loss: 0.0759\n",
      "Epoch [4/8], Step [470/750], Loss: 0.1005\n",
      "Epoch [4/8], Step [471/750], Loss: 0.0613\n",
      "Epoch [4/8], Step [472/750], Loss: 0.0454\n",
      "Epoch [4/8], Step [473/750], Loss: 0.0041\n",
      "Epoch [4/8], Step [474/750], Loss: 0.0689\n",
      "Epoch [4/8], Step [475/750], Loss: 0.0241\n",
      "Epoch [4/8], Step [476/750], Loss: 0.0474\n",
      "Epoch [4/8], Step [477/750], Loss: 0.0041\n",
      "Epoch [4/8], Step [478/750], Loss: 0.0072\n",
      "Epoch [4/8], Step [479/750], Loss: 0.0041\n",
      "Epoch [4/8], Step [480/750], Loss: 0.0425\n",
      "Epoch [4/8], Step [481/750], Loss: 0.0061\n",
      "Epoch [4/8], Step [482/750], Loss: 0.1318\n",
      "Epoch [4/8], Step [483/750], Loss: 0.2382\n",
      "Epoch [4/8], Step [484/750], Loss: 0.0029\n",
      "Epoch [4/8], Step [485/750], Loss: 0.0333\n",
      "Epoch [4/8], Step [486/750], Loss: 0.0527\n",
      "Epoch [4/8], Step [487/750], Loss: 0.0018\n",
      "Epoch [4/8], Step [488/750], Loss: 0.0227\n",
      "Epoch [4/8], Step [489/750], Loss: 0.0940\n",
      "Epoch [4/8], Step [490/750], Loss: 0.0370\n",
      "Epoch [4/8], Step [491/750], Loss: 0.0641\n",
      "Epoch [4/8], Step [492/750], Loss: 0.0657\n",
      "Epoch [4/8], Step [493/750], Loss: 0.0193\n",
      "Epoch [4/8], Step [494/750], Loss: 0.0169\n",
      "Epoch [4/8], Step [495/750], Loss: 0.0778\n",
      "Epoch [4/8], Step [496/750], Loss: 0.0199\n",
      "Epoch [4/8], Step [497/750], Loss: 0.0186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Step [498/750], Loss: 0.0084\n",
      "Epoch [4/8], Step [499/750], Loss: 0.0106\n",
      "Epoch [4/8], Step [500/750], Loss: 0.0497\n",
      "Epoch [4/8], Step [501/750], Loss: 0.0401\n",
      "Epoch [4/8], Step [502/750], Loss: 0.0752\n",
      "Epoch [4/8], Step [503/750], Loss: 0.0135\n",
      "Epoch [4/8], Step [504/750], Loss: 0.0722\n",
      "Epoch [4/8], Step [505/750], Loss: 0.1566\n",
      "Epoch [4/8], Step [506/750], Loss: 0.0105\n",
      "Epoch [4/8], Step [507/750], Loss: 0.0284\n",
      "Epoch [4/8], Step [508/750], Loss: 0.0308\n",
      "Epoch [4/8], Step [509/750], Loss: 0.2024\n",
      "Epoch [4/8], Step [510/750], Loss: 0.0357\n",
      "Epoch [4/8], Step [511/750], Loss: 0.0277\n",
      "Epoch [4/8], Step [512/750], Loss: 0.0294\n",
      "Epoch [4/8], Step [513/750], Loss: 0.0457\n",
      "Epoch [4/8], Step [514/750], Loss: 0.0534\n",
      "Epoch [4/8], Step [515/750], Loss: 0.2608\n",
      "Epoch [4/8], Step [516/750], Loss: 0.0377\n",
      "Epoch [4/8], Step [517/750], Loss: 0.0052\n",
      "Epoch [4/8], Step [518/750], Loss: 0.0085\n",
      "Epoch [4/8], Step [519/750], Loss: 0.0402\n",
      "Epoch [4/8], Step [520/750], Loss: 0.2592\n",
      "Epoch [4/8], Step [521/750], Loss: 0.0214\n",
      "Epoch [4/8], Step [522/750], Loss: 0.0160\n",
      "Epoch [4/8], Step [523/750], Loss: 0.0089\n",
      "Epoch [4/8], Step [524/750], Loss: 0.0063\n",
      "Epoch [4/8], Step [525/750], Loss: 0.0217\n",
      "Epoch [4/8], Step [526/750], Loss: 0.1328\n",
      "Epoch [4/8], Step [527/750], Loss: 0.0058\n",
      "Epoch [4/8], Step [528/750], Loss: 0.0347\n",
      "Epoch [4/8], Step [529/750], Loss: 0.0992\n",
      "Epoch [4/8], Step [530/750], Loss: 0.0709\n",
      "Epoch [4/8], Step [531/750], Loss: 0.0307\n",
      "Epoch [4/8], Step [532/750], Loss: 0.0312\n",
      "Epoch [4/8], Step [533/750], Loss: 0.0155\n",
      "Epoch [4/8], Step [534/750], Loss: 0.0240\n",
      "Epoch [4/8], Step [535/750], Loss: 0.0063\n",
      "Epoch [4/8], Step [536/750], Loss: 0.0167\n",
      "Epoch [4/8], Step [537/750], Loss: 0.0299\n",
      "Epoch [4/8], Step [538/750], Loss: 0.0134\n",
      "Epoch [4/8], Step [539/750], Loss: 0.1037\n",
      "Epoch [4/8], Step [540/750], Loss: 0.0095\n",
      "Epoch [4/8], Step [541/750], Loss: 0.0054\n",
      "Epoch [4/8], Step [542/750], Loss: 0.0110\n",
      "Epoch [4/8], Step [543/750], Loss: 0.0685\n",
      "Epoch [4/8], Step [544/750], Loss: 0.0185\n",
      "Epoch [4/8], Step [545/750], Loss: 0.0115\n",
      "Epoch [4/8], Step [546/750], Loss: 0.0560\n",
      "Epoch [4/8], Step [547/750], Loss: 0.0197\n",
      "Epoch [4/8], Step [548/750], Loss: 0.0075\n",
      "Epoch [4/8], Step [549/750], Loss: 0.0491\n",
      "Epoch [4/8], Step [550/750], Loss: 0.0101\n",
      "Epoch [4/8], Step [551/750], Loss: 0.0551\n",
      "Epoch [4/8], Step [552/750], Loss: 0.0129\n",
      "Epoch [4/8], Step [553/750], Loss: 0.0320\n",
      "Epoch [4/8], Step [554/750], Loss: 0.0073\n",
      "Epoch [4/8], Step [555/750], Loss: 0.0023\n",
      "Epoch [4/8], Step [556/750], Loss: 0.0271\n",
      "Epoch [4/8], Step [557/750], Loss: 0.0147\n",
      "Epoch [4/8], Step [558/750], Loss: 0.0103\n",
      "Epoch [4/8], Step [559/750], Loss: 0.0211\n",
      "Epoch [4/8], Step [560/750], Loss: 0.0117\n",
      "Epoch [4/8], Step [561/750], Loss: 0.0229\n",
      "Epoch [4/8], Step [562/750], Loss: 0.1288\n",
      "Epoch [4/8], Step [563/750], Loss: 0.0623\n",
      "Epoch [4/8], Step [564/750], Loss: 0.0579\n",
      "Epoch [4/8], Step [565/750], Loss: 0.0061\n",
      "Epoch [4/8], Step [566/750], Loss: 0.0217\n",
      "Epoch [4/8], Step [567/750], Loss: 0.0360\n",
      "Epoch [4/8], Step [568/750], Loss: 0.0259\n",
      "Epoch [4/8], Step [569/750], Loss: 0.0158\n",
      "Epoch [4/8], Step [570/750], Loss: 0.0848\n",
      "Epoch [4/8], Step [571/750], Loss: 0.0092\n",
      "Epoch [4/8], Step [572/750], Loss: 0.1059\n",
      "Epoch [4/8], Step [573/750], Loss: 0.0991\n",
      "Epoch [4/8], Step [574/750], Loss: 0.0124\n",
      "Epoch [4/8], Step [575/750], Loss: 0.1473\n",
      "Epoch [4/8], Step [576/750], Loss: 0.0061\n",
      "Epoch [4/8], Step [577/750], Loss: 0.0052\n",
      "Epoch [4/8], Step [578/750], Loss: 0.1643\n",
      "Epoch [4/8], Step [579/750], Loss: 0.0058\n",
      "Epoch [4/8], Step [580/750], Loss: 0.0663\n",
      "Epoch [4/8], Step [581/750], Loss: 0.0087\n",
      "Epoch [4/8], Step [582/750], Loss: 0.0163\n",
      "Epoch [4/8], Step [583/750], Loss: 0.0087\n",
      "Epoch [4/8], Step [584/750], Loss: 0.0108\n",
      "Epoch [4/8], Step [585/750], Loss: 0.0121\n",
      "Epoch [4/8], Step [586/750], Loss: 0.0069\n",
      "Epoch [4/8], Step [587/750], Loss: 0.0159\n",
      "Epoch [4/8], Step [588/750], Loss: 0.0589\n",
      "Epoch [4/8], Step [589/750], Loss: 0.0190\n",
      "Epoch [4/8], Step [590/750], Loss: 0.0103\n",
      "Epoch [4/8], Step [591/750], Loss: 0.0414\n",
      "Epoch [4/8], Step [592/750], Loss: 0.0032\n",
      "Epoch [4/8], Step [593/750], Loss: 0.0145\n",
      "Epoch [4/8], Step [594/750], Loss: 0.0114\n",
      "Epoch [4/8], Step [595/750], Loss: 0.0048\n",
      "Epoch [4/8], Step [596/750], Loss: 0.0111\n",
      "Epoch [4/8], Step [597/750], Loss: 0.0103\n",
      "Epoch [4/8], Step [598/750], Loss: 0.0782\n",
      "Epoch [4/8], Step [599/750], Loss: 0.0094\n",
      "Epoch [4/8], Step [600/750], Loss: 0.0566\n",
      "Epoch [4/8], Step [601/750], Loss: 0.0526\n",
      "Epoch [4/8], Step [602/750], Loss: 0.0845\n",
      "Epoch [4/8], Step [603/750], Loss: 0.0047\n",
      "Epoch [4/8], Step [604/750], Loss: 0.1257\n",
      "Epoch [4/8], Step [605/750], Loss: 0.0343\n",
      "Epoch [4/8], Step [606/750], Loss: 0.0040\n",
      "Epoch [4/8], Step [607/750], Loss: 0.0240\n",
      "Epoch [4/8], Step [608/750], Loss: 0.0558\n",
      "Epoch [4/8], Step [609/750], Loss: 0.1064\n",
      "Epoch [4/8], Step [610/750], Loss: 0.0038\n",
      "Epoch [4/8], Step [611/750], Loss: 0.0899\n",
      "Epoch [4/8], Step [612/750], Loss: 0.0127\n",
      "Epoch [4/8], Step [613/750], Loss: 0.0372\n",
      "Epoch [4/8], Step [614/750], Loss: 0.0078\n",
      "Epoch [4/8], Step [615/750], Loss: 0.0987\n",
      "Epoch [4/8], Step [616/750], Loss: 0.0935\n",
      "Epoch [4/8], Step [617/750], Loss: 0.0072\n",
      "Epoch [4/8], Step [618/750], Loss: 0.0465\n",
      "Epoch [4/8], Step [619/750], Loss: 0.0645\n",
      "Epoch [4/8], Step [620/750], Loss: 0.0998\n",
      "Epoch [4/8], Step [621/750], Loss: 0.1337\n",
      "Epoch [4/8], Step [622/750], Loss: 0.0306\n",
      "Epoch [4/8], Step [623/750], Loss: 0.0025\n",
      "Epoch [4/8], Step [624/750], Loss: 0.0268\n",
      "Epoch [4/8], Step [625/750], Loss: 0.0288\n",
      "Epoch [4/8], Step [626/750], Loss: 0.0095\n",
      "Epoch [4/8], Step [627/750], Loss: 0.0024\n",
      "Epoch [4/8], Step [628/750], Loss: 0.0018\n",
      "Epoch [4/8], Step [629/750], Loss: 0.0704\n",
      "Epoch [4/8], Step [630/750], Loss: 0.0604\n",
      "Epoch [4/8], Step [631/750], Loss: 0.0295\n",
      "Epoch [4/8], Step [632/750], Loss: 0.0371\n",
      "Epoch [4/8], Step [633/750], Loss: 0.0308\n",
      "Epoch [4/8], Step [634/750], Loss: 0.0787\n",
      "Epoch [4/8], Step [635/750], Loss: 0.0077\n",
      "Epoch [4/8], Step [636/750], Loss: 0.1098\n",
      "Epoch [4/8], Step [637/750], Loss: 0.0093\n",
      "Epoch [4/8], Step [638/750], Loss: 0.0818\n",
      "Epoch [4/8], Step [639/750], Loss: 0.0584\n",
      "Epoch [4/8], Step [640/750], Loss: 0.1466\n",
      "Epoch [4/8], Step [641/750], Loss: 0.0075\n",
      "Epoch [4/8], Step [642/750], Loss: 0.0083\n",
      "Epoch [4/8], Step [643/750], Loss: 0.0299\n",
      "Epoch [4/8], Step [644/750], Loss: 0.0386\n",
      "Epoch [4/8], Step [645/750], Loss: 0.0202\n",
      "Epoch [4/8], Step [646/750], Loss: 0.0369\n",
      "Epoch [4/8], Step [647/750], Loss: 0.1607\n",
      "Epoch [4/8], Step [648/750], Loss: 0.0624\n",
      "Epoch [4/8], Step [649/750], Loss: 0.0065\n",
      "Epoch [4/8], Step [650/750], Loss: 0.0174\n",
      "Epoch [4/8], Step [651/750], Loss: 0.0514\n",
      "Epoch [4/8], Step [652/750], Loss: 0.0196\n",
      "Epoch [4/8], Step [653/750], Loss: 0.0539\n",
      "Epoch [4/8], Step [654/750], Loss: 0.1103\n",
      "Epoch [4/8], Step [655/750], Loss: 0.0526\n",
      "Epoch [4/8], Step [656/750], Loss: 0.0314\n",
      "Epoch [4/8], Step [657/750], Loss: 0.0173\n",
      "Epoch [4/8], Step [658/750], Loss: 0.0371\n",
      "Epoch [4/8], Step [659/750], Loss: 0.0284\n",
      "Epoch [4/8], Step [660/750], Loss: 0.0145\n",
      "Epoch [4/8], Step [661/750], Loss: 0.0446\n",
      "Epoch [4/8], Step [662/750], Loss: 0.0430\n",
      "Epoch [4/8], Step [663/750], Loss: 0.0167\n",
      "Epoch [4/8], Step [664/750], Loss: 0.0189\n",
      "Epoch [4/8], Step [665/750], Loss: 0.0412\n",
      "Epoch [4/8], Step [666/750], Loss: 0.0085\n",
      "Epoch [4/8], Step [667/750], Loss: 0.0137\n",
      "Epoch [4/8], Step [668/750], Loss: 0.0213\n",
      "Epoch [4/8], Step [669/750], Loss: 0.0547\n",
      "Epoch [4/8], Step [670/750], Loss: 0.0299\n",
      "Epoch [4/8], Step [671/750], Loss: 0.1032\n",
      "Epoch [4/8], Step [672/750], Loss: 0.0384\n",
      "Epoch [4/8], Step [673/750], Loss: 0.0244\n",
      "Epoch [4/8], Step [674/750], Loss: 0.0374\n",
      "Epoch [4/8], Step [675/750], Loss: 0.1017\n",
      "Epoch [4/8], Step [676/750], Loss: 0.0378\n",
      "Epoch [4/8], Step [677/750], Loss: 0.0656\n",
      "Epoch [4/8], Step [678/750], Loss: 0.1145\n",
      "Epoch [4/8], Step [679/750], Loss: 0.0230\n",
      "Epoch [4/8], Step [680/750], Loss: 0.0172\n",
      "Epoch [4/8], Step [681/750], Loss: 0.0596\n",
      "Epoch [4/8], Step [682/750], Loss: 0.0019\n",
      "Epoch [4/8], Step [683/750], Loss: 0.0065\n",
      "Epoch [4/8], Step [684/750], Loss: 0.0955\n",
      "Epoch [4/8], Step [685/750], Loss: 0.0090\n",
      "Epoch [4/8], Step [686/750], Loss: 0.0013\n",
      "Epoch [4/8], Step [687/750], Loss: 0.0074\n",
      "Epoch [4/8], Step [688/750], Loss: 0.0164\n",
      "Epoch [4/8], Step [689/750], Loss: 0.0399\n",
      "Epoch [4/8], Step [690/750], Loss: 0.0219\n",
      "Epoch [4/8], Step [691/750], Loss: 0.0025\n",
      "Epoch [4/8], Step [692/750], Loss: 0.0108\n",
      "Epoch [4/8], Step [693/750], Loss: 0.0063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Step [694/750], Loss: 0.0264\n",
      "Epoch [4/8], Step [695/750], Loss: 0.1023\n",
      "Epoch [4/8], Step [696/750], Loss: 0.0393\n",
      "Epoch [4/8], Step [697/750], Loss: 0.0057\n",
      "Epoch [4/8], Step [698/750], Loss: 0.1041\n",
      "Epoch [4/8], Step [699/750], Loss: 0.0383\n",
      "Epoch [4/8], Step [700/750], Loss: 0.0066\n",
      "Epoch [4/8], Step [701/750], Loss: 0.0238\n",
      "Epoch [4/8], Step [702/750], Loss: 0.0083\n",
      "Epoch [4/8], Step [703/750], Loss: 0.0030\n",
      "Epoch [4/8], Step [704/750], Loss: 0.0233\n",
      "Epoch [4/8], Step [705/750], Loss: 0.0936\n",
      "Epoch [4/8], Step [706/750], Loss: 0.0761\n",
      "Epoch [4/8], Step [707/750], Loss: 0.0330\n",
      "Epoch [4/8], Step [708/750], Loss: 0.0199\n",
      "Epoch [4/8], Step [709/750], Loss: 0.0263\n",
      "Epoch [4/8], Step [710/750], Loss: 0.0195\n",
      "Epoch [4/8], Step [711/750], Loss: 0.0139\n",
      "Epoch [4/8], Step [712/750], Loss: 0.0268\n",
      "Epoch [4/8], Step [713/750], Loss: 0.0800\n",
      "Epoch [4/8], Step [714/750], Loss: 0.0137\n",
      "Epoch [4/8], Step [715/750], Loss: 0.0029\n",
      "Epoch [4/8], Step [716/750], Loss: 0.0074\n",
      "Epoch [4/8], Step [717/750], Loss: 0.0133\n",
      "Epoch [4/8], Step [718/750], Loss: 0.0759\n",
      "Epoch [4/8], Step [719/750], Loss: 0.0544\n",
      "Epoch [4/8], Step [720/750], Loss: 0.0280\n",
      "Epoch [4/8], Step [721/750], Loss: 0.0642\n",
      "Epoch [4/8], Step [722/750], Loss: 0.0305\n",
      "Epoch [4/8], Step [723/750], Loss: 0.0200\n",
      "Epoch [4/8], Step [724/750], Loss: 0.0580\n",
      "Epoch [4/8], Step [725/750], Loss: 0.0331\n",
      "Epoch [4/8], Step [726/750], Loss: 0.0402\n",
      "Epoch [4/8], Step [727/750], Loss: 0.0041\n",
      "Epoch [4/8], Step [728/750], Loss: 0.0043\n",
      "Epoch [4/8], Step [729/750], Loss: 0.0161\n",
      "Epoch [4/8], Step [730/750], Loss: 0.0154\n",
      "Epoch [4/8], Step [731/750], Loss: 0.0683\n",
      "Epoch [4/8], Step [732/750], Loss: 0.0289\n",
      "Epoch [4/8], Step [733/750], Loss: 0.0195\n",
      "Epoch [4/8], Step [734/750], Loss: 0.0327\n",
      "Epoch [4/8], Step [735/750], Loss: 0.0210\n",
      "Epoch [4/8], Step [736/750], Loss: 0.0337\n",
      "Epoch [4/8], Step [737/750], Loss: 0.0382\n",
      "Epoch [4/8], Step [738/750], Loss: 0.0375\n",
      "Epoch [4/8], Step [739/750], Loss: 0.0066\n",
      "Epoch [4/8], Step [740/750], Loss: 0.0390\n",
      "Epoch [4/8], Step [741/750], Loss: 0.0119\n",
      "Epoch [4/8], Step [742/750], Loss: 0.0914\n",
      "Epoch [4/8], Step [743/750], Loss: 0.0121\n",
      "Epoch [4/8], Step [744/750], Loss: 0.0051\n",
      "Epoch [4/8], Step [745/750], Loss: 0.0283\n",
      "Epoch [4/8], Step [746/750], Loss: 0.0112\n",
      "Epoch [4/8], Step [747/750], Loss: 0.1274\n",
      "Epoch [4/8], Step [748/750], Loss: 0.0199\n",
      "Epoch [4/8], Step [749/750], Loss: 0.0945\n",
      "Epoch [4/8], Step [750/750], Loss: 0.0420\n",
      "Epoch [4/8], Tr. loss: 0.3119. Test loss: 0.2299\n",
      "\n",
      "\n",
      "Epoch [5/8], Step [1/750], Loss: 0.0039\n",
      "Epoch [5/8], Step [2/750], Loss: 0.0324\n",
      "Epoch [5/8], Step [3/750], Loss: 0.0041\n",
      "Epoch [5/8], Step [4/750], Loss: 0.0187\n",
      "Epoch [5/8], Step [5/750], Loss: 0.0030\n",
      "Epoch [5/8], Step [6/750], Loss: 0.0446\n",
      "Epoch [5/8], Step [7/750], Loss: 0.0123\n",
      "Epoch [5/8], Step [8/750], Loss: 0.0032\n",
      "Epoch [5/8], Step [9/750], Loss: 0.0091\n",
      "Epoch [5/8], Step [10/750], Loss: 0.1078\n",
      "Epoch [5/8], Step [11/750], Loss: 0.0133\n",
      "Epoch [5/8], Step [12/750], Loss: 0.0023\n",
      "Epoch [5/8], Step [13/750], Loss: 0.0151\n",
      "Epoch [5/8], Step [14/750], Loss: 0.1010\n",
      "Epoch [5/8], Step [15/750], Loss: 0.0988\n",
      "Epoch [5/8], Step [16/750], Loss: 0.0102\n",
      "Epoch [5/8], Step [17/750], Loss: 0.0040\n",
      "Epoch [5/8], Step [18/750], Loss: 0.0183\n",
      "Epoch [5/8], Step [19/750], Loss: 0.0240\n",
      "Epoch [5/8], Step [20/750], Loss: 0.0695\n",
      "Epoch [5/8], Step [21/750], Loss: 0.0211\n",
      "Epoch [5/8], Step [22/750], Loss: 0.0091\n",
      "Epoch [5/8], Step [23/750], Loss: 0.0075\n",
      "Epoch [5/8], Step [24/750], Loss: 0.0668\n",
      "Epoch [5/8], Step [25/750], Loss: 0.0095\n",
      "Epoch [5/8], Step [26/750], Loss: 0.0125\n",
      "Epoch [5/8], Step [27/750], Loss: 0.0165\n",
      "Epoch [5/8], Step [28/750], Loss: 0.0055\n",
      "Epoch [5/8], Step [29/750], Loss: 0.0082\n",
      "Epoch [5/8], Step [30/750], Loss: 0.0011\n",
      "Epoch [5/8], Step [31/750], Loss: 0.0692\n",
      "Epoch [5/8], Step [32/750], Loss: 0.0452\n",
      "Epoch [5/8], Step [33/750], Loss: 0.0039\n",
      "Epoch [5/8], Step [34/750], Loss: 0.1226\n",
      "Epoch [5/8], Step [35/750], Loss: 0.0399\n",
      "Epoch [5/8], Step [36/750], Loss: 0.0556\n",
      "Epoch [5/8], Step [37/750], Loss: 0.0020\n",
      "Epoch [5/8], Step [38/750], Loss: 0.0242\n",
      "Epoch [5/8], Step [39/750], Loss: 0.0017\n",
      "Epoch [5/8], Step [40/750], Loss: 0.0134\n",
      "Epoch [5/8], Step [41/750], Loss: 0.0158\n",
      "Epoch [5/8], Step [42/750], Loss: 0.0656\n",
      "Epoch [5/8], Step [43/750], Loss: 0.0471\n",
      "Epoch [5/8], Step [44/750], Loss: 0.0610\n",
      "Epoch [5/8], Step [45/750], Loss: 0.0053\n",
      "Epoch [5/8], Step [46/750], Loss: 0.0146\n",
      "Epoch [5/8], Step [47/750], Loss: 0.0278\n",
      "Epoch [5/8], Step [48/750], Loss: 0.0071\n",
      "Epoch [5/8], Step [49/750], Loss: 0.0151\n",
      "Epoch [5/8], Step [50/750], Loss: 0.0373\n",
      "Epoch [5/8], Step [51/750], Loss: 0.0083\n",
      "Epoch [5/8], Step [52/750], Loss: 0.0348\n",
      "Epoch [5/8], Step [53/750], Loss: 0.0296\n",
      "Epoch [5/8], Step [54/750], Loss: 0.0410\n",
      "Epoch [5/8], Step [55/750], Loss: 0.0335\n",
      "Epoch [5/8], Step [56/750], Loss: 0.0120\n",
      "Epoch [5/8], Step [57/750], Loss: 0.0111\n",
      "Epoch [5/8], Step [58/750], Loss: 0.0183\n",
      "Epoch [5/8], Step [59/750], Loss: 0.0436\n",
      "Epoch [5/8], Step [60/750], Loss: 0.0121\n",
      "Epoch [5/8], Step [61/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [62/750], Loss: 0.0020\n",
      "Epoch [5/8], Step [63/750], Loss: 0.0146\n",
      "Epoch [5/8], Step [64/750], Loss: 0.0399\n",
      "Epoch [5/8], Step [65/750], Loss: 0.0220\n",
      "Epoch [5/8], Step [66/750], Loss: 0.0041\n",
      "Epoch [5/8], Step [67/750], Loss: 0.0353\n",
      "Epoch [5/8], Step [68/750], Loss: 0.0242\n",
      "Epoch [5/8], Step [69/750], Loss: 0.0020\n",
      "Epoch [5/8], Step [70/750], Loss: 0.0915\n",
      "Epoch [5/8], Step [71/750], Loss: 0.0043\n",
      "Epoch [5/8], Step [72/750], Loss: 0.0041\n",
      "Epoch [5/8], Step [73/750], Loss: 0.0193\n",
      "Epoch [5/8], Step [74/750], Loss: 0.0193\n",
      "Epoch [5/8], Step [75/750], Loss: 0.0267\n",
      "Epoch [5/8], Step [76/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [77/750], Loss: 0.0049\n",
      "Epoch [5/8], Step [78/750], Loss: 0.0534\n",
      "Epoch [5/8], Step [79/750], Loss: 0.0148\n",
      "Epoch [5/8], Step [80/750], Loss: 0.0099\n",
      "Epoch [5/8], Step [81/750], Loss: 0.0146\n",
      "Epoch [5/8], Step [82/750], Loss: 0.0630\n",
      "Epoch [5/8], Step [83/750], Loss: 0.0089\n",
      "Epoch [5/8], Step [84/750], Loss: 0.0048\n",
      "Epoch [5/8], Step [85/750], Loss: 0.0491\n",
      "Epoch [5/8], Step [86/750], Loss: 0.0142\n",
      "Epoch [5/8], Step [87/750], Loss: 0.0080\n",
      "Epoch [5/8], Step [88/750], Loss: 0.0065\n",
      "Epoch [5/8], Step [89/750], Loss: 0.0205\n",
      "Epoch [5/8], Step [90/750], Loss: 0.0141\n",
      "Epoch [5/8], Step [91/750], Loss: 0.0662\n",
      "Epoch [5/8], Step [92/750], Loss: 0.0574\n",
      "Epoch [5/8], Step [93/750], Loss: 0.0024\n",
      "Epoch [5/8], Step [94/750], Loss: 0.0166\n",
      "Epoch [5/8], Step [95/750], Loss: 0.0047\n",
      "Epoch [5/8], Step [96/750], Loss: 0.1434\n",
      "Epoch [5/8], Step [97/750], Loss: 0.0016\n",
      "Epoch [5/8], Step [98/750], Loss: 0.0147\n",
      "Epoch [5/8], Step [99/750], Loss: 0.0713\n",
      "Epoch [5/8], Step [100/750], Loss: 0.0415\n",
      "Epoch [5/8], Step [101/750], Loss: 0.0550\n",
      "Epoch [5/8], Step [102/750], Loss: 0.0136\n",
      "Epoch [5/8], Step [103/750], Loss: 0.0605\n",
      "Epoch [5/8], Step [104/750], Loss: 0.0079\n",
      "Epoch [5/8], Step [105/750], Loss: 0.0279\n",
      "Epoch [5/8], Step [106/750], Loss: 0.0092\n",
      "Epoch [5/8], Step [107/750], Loss: 0.0059\n",
      "Epoch [5/8], Step [108/750], Loss: 0.0032\n",
      "Epoch [5/8], Step [109/750], Loss: 0.0135\n",
      "Epoch [5/8], Step [110/750], Loss: 0.0083\n",
      "Epoch [5/8], Step [111/750], Loss: 0.0112\n",
      "Epoch [5/8], Step [112/750], Loss: 0.0035\n",
      "Epoch [5/8], Step [113/750], Loss: 0.0027\n",
      "Epoch [5/8], Step [114/750], Loss: 0.0083\n",
      "Epoch [5/8], Step [115/750], Loss: 0.0149\n",
      "Epoch [5/8], Step [116/750], Loss: 0.0019\n",
      "Epoch [5/8], Step [117/750], Loss: 0.0075\n",
      "Epoch [5/8], Step [118/750], Loss: 0.0035\n",
      "Epoch [5/8], Step [119/750], Loss: 0.0441\n",
      "Epoch [5/8], Step [120/750], Loss: 0.0129\n",
      "Epoch [5/8], Step [121/750], Loss: 0.0044\n",
      "Epoch [5/8], Step [122/750], Loss: 0.0051\n",
      "Epoch [5/8], Step [123/750], Loss: 0.0814\n",
      "Epoch [5/8], Step [124/750], Loss: 0.0099\n",
      "Epoch [5/8], Step [125/750], Loss: 0.0022\n",
      "Epoch [5/8], Step [126/750], Loss: 0.0404\n",
      "Epoch [5/8], Step [127/750], Loss: 0.1022\n",
      "Epoch [5/8], Step [128/750], Loss: 0.0015\n",
      "Epoch [5/8], Step [129/750], Loss: 0.0885\n",
      "Epoch [5/8], Step [130/750], Loss: 0.0236\n",
      "Epoch [5/8], Step [131/750], Loss: 0.0201\n",
      "Epoch [5/8], Step [132/750], Loss: 0.0157\n",
      "Epoch [5/8], Step [133/750], Loss: 0.0725\n",
      "Epoch [5/8], Step [134/750], Loss: 0.0226\n",
      "Epoch [5/8], Step [135/750], Loss: 0.1744\n",
      "Epoch [5/8], Step [136/750], Loss: 0.0110\n",
      "Epoch [5/8], Step [137/750], Loss: 0.0339\n",
      "Epoch [5/8], Step [138/750], Loss: 0.0123\n",
      "Epoch [5/8], Step [139/750], Loss: 0.1258\n",
      "Epoch [5/8], Step [140/750], Loss: 0.0945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Step [141/750], Loss: 0.0171\n",
      "Epoch [5/8], Step [142/750], Loss: 0.0824\n",
      "Epoch [5/8], Step [143/750], Loss: 0.0539\n",
      "Epoch [5/8], Step [144/750], Loss: 0.0070\n",
      "Epoch [5/8], Step [145/750], Loss: 0.0123\n",
      "Epoch [5/8], Step [146/750], Loss: 0.0730\n",
      "Epoch [5/8], Step [147/750], Loss: 0.0214\n",
      "Epoch [5/8], Step [148/750], Loss: 0.0493\n",
      "Epoch [5/8], Step [149/750], Loss: 0.0137\n",
      "Epoch [5/8], Step [150/750], Loss: 0.0114\n",
      "Epoch [5/8], Step [151/750], Loss: 0.0207\n",
      "Epoch [5/8], Step [152/750], Loss: 0.0142\n",
      "Epoch [5/8], Step [153/750], Loss: 0.0118\n",
      "Epoch [5/8], Step [154/750], Loss: 0.0258\n",
      "Epoch [5/8], Step [155/750], Loss: 0.0203\n",
      "Epoch [5/8], Step [156/750], Loss: 0.0186\n",
      "Epoch [5/8], Step [157/750], Loss: 0.0179\n",
      "Epoch [5/8], Step [158/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [159/750], Loss: 0.0027\n",
      "Epoch [5/8], Step [160/750], Loss: 0.0757\n",
      "Epoch [5/8], Step [161/750], Loss: 0.0205\n",
      "Epoch [5/8], Step [162/750], Loss: 0.0025\n",
      "Epoch [5/8], Step [163/750], Loss: 0.0028\n",
      "Epoch [5/8], Step [164/750], Loss: 0.0063\n",
      "Epoch [5/8], Step [165/750], Loss: 0.0029\n",
      "Epoch [5/8], Step [166/750], Loss: 0.0103\n",
      "Epoch [5/8], Step [167/750], Loss: 0.0710\n",
      "Epoch [5/8], Step [168/750], Loss: 0.0253\n",
      "Epoch [5/8], Step [169/750], Loss: 0.0343\n",
      "Epoch [5/8], Step [170/750], Loss: 0.0345\n",
      "Epoch [5/8], Step [171/750], Loss: 0.0337\n",
      "Epoch [5/8], Step [172/750], Loss: 0.0469\n",
      "Epoch [5/8], Step [173/750], Loss: 0.0319\n",
      "Epoch [5/8], Step [174/750], Loss: 0.0083\n",
      "Epoch [5/8], Step [175/750], Loss: 0.0529\n",
      "Epoch [5/8], Step [176/750], Loss: 0.0630\n",
      "Epoch [5/8], Step [177/750], Loss: 0.0526\n",
      "Epoch [5/8], Step [178/750], Loss: 0.0228\n",
      "Epoch [5/8], Step [179/750], Loss: 0.0153\n",
      "Epoch [5/8], Step [180/750], Loss: 0.0115\n",
      "Epoch [5/8], Step [181/750], Loss: 0.0714\n",
      "Epoch [5/8], Step [182/750], Loss: 0.0330\n",
      "Epoch [5/8], Step [183/750], Loss: 0.0079\n",
      "Epoch [5/8], Step [184/750], Loss: 0.0032\n",
      "Epoch [5/8], Step [185/750], Loss: 0.0099\n",
      "Epoch [5/8], Step [186/750], Loss: 0.0297\n",
      "Epoch [5/8], Step [187/750], Loss: 0.0897\n",
      "Epoch [5/8], Step [188/750], Loss: 0.0395\n",
      "Epoch [5/8], Step [189/750], Loss: 0.0081\n",
      "Epoch [5/8], Step [190/750], Loss: 0.0217\n",
      "Epoch [5/8], Step [191/750], Loss: 0.0105\n",
      "Epoch [5/8], Step [192/750], Loss: 0.0066\n",
      "Epoch [5/8], Step [193/750], Loss: 0.0312\n",
      "Epoch [5/8], Step [194/750], Loss: 0.0183\n",
      "Epoch [5/8], Step [195/750], Loss: 0.0047\n",
      "Epoch [5/8], Step [196/750], Loss: 0.0116\n",
      "Epoch [5/8], Step [197/750], Loss: 0.0304\n",
      "Epoch [5/8], Step [198/750], Loss: 0.0094\n",
      "Epoch [5/8], Step [199/750], Loss: 0.0196\n",
      "Epoch [5/8], Step [200/750], Loss: 0.0053\n",
      "Epoch [5/8], Step [201/750], Loss: 0.0559\n",
      "Epoch [5/8], Step [202/750], Loss: 0.0038\n",
      "Epoch [5/8], Step [203/750], Loss: 0.0564\n",
      "Epoch [5/8], Step [204/750], Loss: 0.0028\n",
      "Epoch [5/8], Step [205/750], Loss: 0.0029\n",
      "Epoch [5/8], Step [206/750], Loss: 0.0470\n",
      "Epoch [5/8], Step [207/750], Loss: 0.0510\n",
      "Epoch [5/8], Step [208/750], Loss: 0.0413\n",
      "Epoch [5/8], Step [209/750], Loss: 0.0078\n",
      "Epoch [5/8], Step [210/750], Loss: 0.0303\n",
      "Epoch [5/8], Step [211/750], Loss: 0.0027\n",
      "Epoch [5/8], Step [212/750], Loss: 0.0021\n",
      "Epoch [5/8], Step [213/750], Loss: 0.0046\n",
      "Epoch [5/8], Step [214/750], Loss: 0.0032\n",
      "Epoch [5/8], Step [215/750], Loss: 0.0278\n",
      "Epoch [5/8], Step [216/750], Loss: 0.0245\n",
      "Epoch [5/8], Step [217/750], Loss: 0.0115\n",
      "Epoch [5/8], Step [218/750], Loss: 0.0011\n",
      "Epoch [5/8], Step [219/750], Loss: 0.0825\n",
      "Epoch [5/8], Step [220/750], Loss: 0.0012\n",
      "Epoch [5/8], Step [221/750], Loss: 0.0518\n",
      "Epoch [5/8], Step [222/750], Loss: 0.0011\n",
      "Epoch [5/8], Step [223/750], Loss: 0.0143\n",
      "Epoch [5/8], Step [224/750], Loss: 0.0260\n",
      "Epoch [5/8], Step [225/750], Loss: 0.0073\n",
      "Epoch [5/8], Step [226/750], Loss: 0.0126\n",
      "Epoch [5/8], Step [227/750], Loss: 0.0344\n",
      "Epoch [5/8], Step [228/750], Loss: 0.0374\n",
      "Epoch [5/8], Step [229/750], Loss: 0.0623\n",
      "Epoch [5/8], Step [230/750], Loss: 0.0043\n",
      "Epoch [5/8], Step [231/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [232/750], Loss: 0.0040\n",
      "Epoch [5/8], Step [233/750], Loss: 0.0062\n",
      "Epoch [5/8], Step [234/750], Loss: 0.0575\n",
      "Epoch [5/8], Step [235/750], Loss: 0.0735\n",
      "Epoch [5/8], Step [236/750], Loss: 0.0293\n",
      "Epoch [5/8], Step [237/750], Loss: 0.0231\n",
      "Epoch [5/8], Step [238/750], Loss: 0.1441\n",
      "Epoch [5/8], Step [239/750], Loss: 0.0354\n",
      "Epoch [5/8], Step [240/750], Loss: 0.0221\n",
      "Epoch [5/8], Step [241/750], Loss: 0.0399\n",
      "Epoch [5/8], Step [242/750], Loss: 0.0428\n",
      "Epoch [5/8], Step [243/750], Loss: 0.0846\n",
      "Epoch [5/8], Step [244/750], Loss: 0.0095\n",
      "Epoch [5/8], Step [245/750], Loss: 0.0120\n",
      "Epoch [5/8], Step [246/750], Loss: 0.0838\n",
      "Epoch [5/8], Step [247/750], Loss: 0.0324\n",
      "Epoch [5/8], Step [248/750], Loss: 0.0205\n",
      "Epoch [5/8], Step [249/750], Loss: 0.0679\n",
      "Epoch [5/8], Step [250/750], Loss: 0.0028\n",
      "Epoch [5/8], Step [251/750], Loss: 0.0048\n",
      "Epoch [5/8], Step [252/750], Loss: 0.0026\n",
      "Epoch [5/8], Step [253/750], Loss: 0.0095\n",
      "Epoch [5/8], Step [254/750], Loss: 0.0126\n",
      "Epoch [5/8], Step [255/750], Loss: 0.0019\n",
      "Epoch [5/8], Step [256/750], Loss: 0.0025\n",
      "Epoch [5/8], Step [257/750], Loss: 0.0094\n",
      "Epoch [5/8], Step [258/750], Loss: 0.2244\n",
      "Epoch [5/8], Step [259/750], Loss: 0.0084\n",
      "Epoch [5/8], Step [260/750], Loss: 0.0123\n",
      "Epoch [5/8], Step [261/750], Loss: 0.0088\n",
      "Epoch [5/8], Step [262/750], Loss: 0.0064\n",
      "Epoch [5/8], Step [263/750], Loss: 0.0457\n",
      "Epoch [5/8], Step [264/750], Loss: 0.0120\n",
      "Epoch [5/8], Step [265/750], Loss: 0.0759\n",
      "Epoch [5/8], Step [266/750], Loss: 0.0872\n",
      "Epoch [5/8], Step [267/750], Loss: 0.0925\n",
      "Epoch [5/8], Step [268/750], Loss: 0.0043\n",
      "Epoch [5/8], Step [269/750], Loss: 0.1195\n",
      "Epoch [5/8], Step [270/750], Loss: 0.0090\n",
      "Epoch [5/8], Step [271/750], Loss: 0.0237\n",
      "Epoch [5/8], Step [272/750], Loss: 0.0799\n",
      "Epoch [5/8], Step [273/750], Loss: 0.0072\n",
      "Epoch [5/8], Step [274/750], Loss: 0.0373\n",
      "Epoch [5/8], Step [275/750], Loss: 0.0085\n",
      "Epoch [5/8], Step [276/750], Loss: 0.1631\n",
      "Epoch [5/8], Step [277/750], Loss: 0.0533\n",
      "Epoch [5/8], Step [278/750], Loss: 0.0802\n",
      "Epoch [5/8], Step [279/750], Loss: 0.0106\n",
      "Epoch [5/8], Step [280/750], Loss: 0.0252\n",
      "Epoch [5/8], Step [281/750], Loss: 0.0058\n",
      "Epoch [5/8], Step [282/750], Loss: 0.0476\n",
      "Epoch [5/8], Step [283/750], Loss: 0.1357\n",
      "Epoch [5/8], Step [284/750], Loss: 0.0164\n",
      "Epoch [5/8], Step [285/750], Loss: 0.0013\n",
      "Epoch [5/8], Step [286/750], Loss: 0.0118\n",
      "Epoch [5/8], Step [287/750], Loss: 0.0391\n",
      "Epoch [5/8], Step [288/750], Loss: 0.0146\n",
      "Epoch [5/8], Step [289/750], Loss: 0.0470\n",
      "Epoch [5/8], Step [290/750], Loss: 0.0089\n",
      "Epoch [5/8], Step [291/750], Loss: 0.0302\n",
      "Epoch [5/8], Step [292/750], Loss: 0.0760\n",
      "Epoch [5/8], Step [293/750], Loss: 0.0044\n",
      "Epoch [5/8], Step [294/750], Loss: 0.0147\n",
      "Epoch [5/8], Step [295/750], Loss: 0.0463\n",
      "Epoch [5/8], Step [296/750], Loss: 0.0450\n",
      "Epoch [5/8], Step [297/750], Loss: 0.0373\n",
      "Epoch [5/8], Step [298/750], Loss: 0.0072\n",
      "Epoch [5/8], Step [299/750], Loss: 0.0130\n",
      "Epoch [5/8], Step [300/750], Loss: 0.0046\n",
      "Epoch [5/8], Step [301/750], Loss: 0.0326\n",
      "Epoch [5/8], Step [302/750], Loss: 0.0449\n",
      "Epoch [5/8], Step [303/750], Loss: 0.0537\n",
      "Epoch [5/8], Step [304/750], Loss: 0.0865\n",
      "Epoch [5/8], Step [305/750], Loss: 0.0847\n",
      "Epoch [5/8], Step [306/750], Loss: 0.0201\n",
      "Epoch [5/8], Step [307/750], Loss: 0.1129\n",
      "Epoch [5/8], Step [308/750], Loss: 0.0167\n",
      "Epoch [5/8], Step [309/750], Loss: 0.0668\n",
      "Epoch [5/8], Step [310/750], Loss: 0.0057\n",
      "Epoch [5/8], Step [311/750], Loss: 0.0049\n",
      "Epoch [5/8], Step [312/750], Loss: 0.0135\n",
      "Epoch [5/8], Step [313/750], Loss: 0.0409\n",
      "Epoch [5/8], Step [314/750], Loss: 0.0307\n",
      "Epoch [5/8], Step [315/750], Loss: 0.0373\n",
      "Epoch [5/8], Step [316/750], Loss: 0.0198\n",
      "Epoch [5/8], Step [317/750], Loss: 0.0177\n",
      "Epoch [5/8], Step [318/750], Loss: 0.0198\n",
      "Epoch [5/8], Step [319/750], Loss: 0.0075\n",
      "Epoch [5/8], Step [320/750], Loss: 0.0058\n",
      "Epoch [5/8], Step [321/750], Loss: 0.0035\n",
      "Epoch [5/8], Step [322/750], Loss: 0.1032\n",
      "Epoch [5/8], Step [323/750], Loss: 0.0023\n",
      "Epoch [5/8], Step [324/750], Loss: 0.0115\n",
      "Epoch [5/8], Step [325/750], Loss: 0.0521\n",
      "Epoch [5/8], Step [326/750], Loss: 0.0185\n",
      "Epoch [5/8], Step [327/750], Loss: 0.0492\n",
      "Epoch [5/8], Step [328/750], Loss: 0.0318\n",
      "Epoch [5/8], Step [329/750], Loss: 0.0301\n",
      "Epoch [5/8], Step [330/750], Loss: 0.0424\n",
      "Epoch [5/8], Step [331/750], Loss: 0.0036\n",
      "Epoch [5/8], Step [332/750], Loss: 0.1799\n",
      "Epoch [5/8], Step [333/750], Loss: 0.0242\n",
      "Epoch [5/8], Step [334/750], Loss: 0.0543\n",
      "Epoch [5/8], Step [335/750], Loss: 0.0845\n",
      "Epoch [5/8], Step [336/750], Loss: 0.0514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Step [337/750], Loss: 0.0318\n",
      "Epoch [5/8], Step [338/750], Loss: 0.0948\n",
      "Epoch [5/8], Step [339/750], Loss: 0.0091\n",
      "Epoch [5/8], Step [340/750], Loss: 0.0178\n",
      "Epoch [5/8], Step [341/750], Loss: 0.0333\n",
      "Epoch [5/8], Step [342/750], Loss: 0.0116\n",
      "Epoch [5/8], Step [343/750], Loss: 0.0141\n",
      "Epoch [5/8], Step [344/750], Loss: 0.0047\n",
      "Epoch [5/8], Step [345/750], Loss: 0.1005\n",
      "Epoch [5/8], Step [346/750], Loss: 0.0013\n",
      "Epoch [5/8], Step [347/750], Loss: 0.0071\n",
      "Epoch [5/8], Step [348/750], Loss: 0.0127\n",
      "Epoch [5/8], Step [349/750], Loss: 0.0481\n",
      "Epoch [5/8], Step [350/750], Loss: 0.0055\n",
      "Epoch [5/8], Step [351/750], Loss: 0.0809\n",
      "Epoch [5/8], Step [352/750], Loss: 0.0920\n",
      "Epoch [5/8], Step [353/750], Loss: 0.0052\n",
      "Epoch [5/8], Step [354/750], Loss: 0.0137\n",
      "Epoch [5/8], Step [355/750], Loss: 0.0648\n",
      "Epoch [5/8], Step [356/750], Loss: 0.0752\n",
      "Epoch [5/8], Step [357/750], Loss: 0.1591\n",
      "Epoch [5/8], Step [358/750], Loss: 0.0231\n",
      "Epoch [5/8], Step [359/750], Loss: 0.0274\n",
      "Epoch [5/8], Step [360/750], Loss: 0.0766\n",
      "Epoch [5/8], Step [361/750], Loss: 0.0062\n",
      "Epoch [5/8], Step [362/750], Loss: 0.0583\n",
      "Epoch [5/8], Step [363/750], Loss: 0.0110\n",
      "Epoch [5/8], Step [364/750], Loss: 0.0122\n",
      "Epoch [5/8], Step [365/750], Loss: 0.0158\n",
      "Epoch [5/8], Step [366/750], Loss: 0.0152\n",
      "Epoch [5/8], Step [367/750], Loss: 0.0257\n",
      "Epoch [5/8], Step [368/750], Loss: 0.0369\n",
      "Epoch [5/8], Step [369/750], Loss: 0.1081\n",
      "Epoch [5/8], Step [370/750], Loss: 0.0432\n",
      "Epoch [5/8], Step [371/750], Loss: 0.0212\n",
      "Epoch [5/8], Step [372/750], Loss: 0.1376\n",
      "Epoch [5/8], Step [373/750], Loss: 0.0433\n",
      "Epoch [5/8], Step [374/750], Loss: 0.0078\n",
      "Epoch [5/8], Step [375/750], Loss: 0.0393\n",
      "Epoch [5/8], Step [376/750], Loss: 0.0061\n",
      "Epoch [5/8], Step [377/750], Loss: 0.0466\n",
      "Epoch [5/8], Step [378/750], Loss: 0.0120\n",
      "Epoch [5/8], Step [379/750], Loss: 0.0499\n",
      "Epoch [5/8], Step [380/750], Loss: 0.0346\n",
      "Epoch [5/8], Step [381/750], Loss: 0.0449\n",
      "Epoch [5/8], Step [382/750], Loss: 0.0433\n",
      "Epoch [5/8], Step [383/750], Loss: 0.0148\n",
      "Epoch [5/8], Step [384/750], Loss: 0.0239\n",
      "Epoch [5/8], Step [385/750], Loss: 0.0133\n",
      "Epoch [5/8], Step [386/750], Loss: 0.0289\n",
      "Epoch [5/8], Step [387/750], Loss: 0.0578\n",
      "Epoch [5/8], Step [388/750], Loss: 0.0366\n",
      "Epoch [5/8], Step [389/750], Loss: 0.0035\n",
      "Epoch [5/8], Step [390/750], Loss: 0.0132\n",
      "Epoch [5/8], Step [391/750], Loss: 0.0026\n",
      "Epoch [5/8], Step [392/750], Loss: 0.0049\n",
      "Epoch [5/8], Step [393/750], Loss: 0.0295\n",
      "Epoch [5/8], Step [394/750], Loss: 0.0142\n",
      "Epoch [5/8], Step [395/750], Loss: 0.0736\n",
      "Epoch [5/8], Step [396/750], Loss: 0.0222\n",
      "Epoch [5/8], Step [397/750], Loss: 0.0220\n",
      "Epoch [5/8], Step [398/750], Loss: 0.0588\n",
      "Epoch [5/8], Step [399/750], Loss: 0.0143\n",
      "Epoch [5/8], Step [400/750], Loss: 0.0357\n",
      "Epoch [5/8], Step [401/750], Loss: 0.0175\n",
      "Epoch [5/8], Step [402/750], Loss: 0.0031\n",
      "Epoch [5/8], Step [403/750], Loss: 0.0016\n",
      "Epoch [5/8], Step [404/750], Loss: 0.0820\n",
      "Epoch [5/8], Step [405/750], Loss: 0.0180\n",
      "Epoch [5/8], Step [406/750], Loss: 0.0035\n",
      "Epoch [5/8], Step [407/750], Loss: 0.0264\n",
      "Epoch [5/8], Step [408/750], Loss: 0.0881\n",
      "Epoch [5/8], Step [409/750], Loss: 0.1083\n",
      "Epoch [5/8], Step [410/750], Loss: 0.0290\n",
      "Epoch [5/8], Step [411/750], Loss: 0.0029\n",
      "Epoch [5/8], Step [412/750], Loss: 0.0167\n",
      "Epoch [5/8], Step [413/750], Loss: 0.0030\n",
      "Epoch [5/8], Step [414/750], Loss: 0.0053\n",
      "Epoch [5/8], Step [415/750], Loss: 0.0078\n",
      "Epoch [5/8], Step [416/750], Loss: 0.0798\n",
      "Epoch [5/8], Step [417/750], Loss: 0.0143\n",
      "Epoch [5/8], Step [418/750], Loss: 0.0606\n",
      "Epoch [5/8], Step [419/750], Loss: 0.0220\n",
      "Epoch [5/8], Step [420/750], Loss: 0.0526\n",
      "Epoch [5/8], Step [421/750], Loss: 0.0161\n",
      "Epoch [5/8], Step [422/750], Loss: 0.0189\n",
      "Epoch [5/8], Step [423/750], Loss: 0.0706\n",
      "Epoch [5/8], Step [424/750], Loss: 0.0255\n",
      "Epoch [5/8], Step [425/750], Loss: 0.0456\n",
      "Epoch [5/8], Step [426/750], Loss: 0.0328\n",
      "Epoch [5/8], Step [427/750], Loss: 0.0054\n",
      "Epoch [5/8], Step [428/750], Loss: 0.0715\n",
      "Epoch [5/8], Step [429/750], Loss: 0.0226\n",
      "Epoch [5/8], Step [430/750], Loss: 0.0093\n",
      "Epoch [5/8], Step [431/750], Loss: 0.0043\n",
      "Epoch [5/8], Step [432/750], Loss: 0.0634\n",
      "Epoch [5/8], Step [433/750], Loss: 0.0319\n",
      "Epoch [5/8], Step [434/750], Loss: 0.0301\n",
      "Epoch [5/8], Step [435/750], Loss: 0.0714\n",
      "Epoch [5/8], Step [436/750], Loss: 0.0876\n",
      "Epoch [5/8], Step [437/750], Loss: 0.0114\n",
      "Epoch [5/8], Step [438/750], Loss: 0.0553\n",
      "Epoch [5/8], Step [439/750], Loss: 0.0505\n",
      "Epoch [5/8], Step [440/750], Loss: 0.0146\n",
      "Epoch [5/8], Step [441/750], Loss: 0.0128\n",
      "Epoch [5/8], Step [442/750], Loss: 0.0040\n",
      "Epoch [5/8], Step [443/750], Loss: 0.0173\n",
      "Epoch [5/8], Step [444/750], Loss: 0.0151\n",
      "Epoch [5/8], Step [445/750], Loss: 0.0371\n",
      "Epoch [5/8], Step [446/750], Loss: 0.0920\n",
      "Epoch [5/8], Step [447/750], Loss: 0.0385\n",
      "Epoch [5/8], Step [448/750], Loss: 0.0049\n",
      "Epoch [5/8], Step [449/750], Loss: 0.0252\n",
      "Epoch [5/8], Step [450/750], Loss: 0.0890\n",
      "Epoch [5/8], Step [451/750], Loss: 0.0720\n",
      "Epoch [5/8], Step [452/750], Loss: 0.0079\n",
      "Epoch [5/8], Step [453/750], Loss: 0.0045\n",
      "Epoch [5/8], Step [454/750], Loss: 0.0334\n",
      "Epoch [5/8], Step [455/750], Loss: 0.0032\n",
      "Epoch [5/8], Step [456/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [457/750], Loss: 0.0278\n",
      "Epoch [5/8], Step [458/750], Loss: 0.0023\n",
      "Epoch [5/8], Step [459/750], Loss: 0.1161\n",
      "Epoch [5/8], Step [460/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [461/750], Loss: 0.0021\n",
      "Epoch [5/8], Step [462/750], Loss: 0.0286\n",
      "Epoch [5/8], Step [463/750], Loss: 0.0038\n",
      "Epoch [5/8], Step [464/750], Loss: 0.0262\n",
      "Epoch [5/8], Step [465/750], Loss: 0.0675\n",
      "Epoch [5/8], Step [466/750], Loss: 0.0442\n",
      "Epoch [5/8], Step [467/750], Loss: 0.0145\n",
      "Epoch [5/8], Step [468/750], Loss: 0.0498\n",
      "Epoch [5/8], Step [469/750], Loss: 0.0115\n",
      "Epoch [5/8], Step [470/750], Loss: 0.0343\n",
      "Epoch [5/8], Step [471/750], Loss: 0.0284\n",
      "Epoch [5/8], Step [472/750], Loss: 0.0432\n",
      "Epoch [5/8], Step [473/750], Loss: 0.0127\n",
      "Epoch [5/8], Step [474/750], Loss: 0.0186\n",
      "Epoch [5/8], Step [475/750], Loss: 0.0656\n",
      "Epoch [5/8], Step [476/750], Loss: 0.0245\n",
      "Epoch [5/8], Step [477/750], Loss: 0.0018\n",
      "Epoch [5/8], Step [478/750], Loss: 0.0793\n",
      "Epoch [5/8], Step [479/750], Loss: 0.0025\n",
      "Epoch [5/8], Step [480/750], Loss: 0.0333\n",
      "Epoch [5/8], Step [481/750], Loss: 0.0301\n",
      "Epoch [5/8], Step [482/750], Loss: 0.0021\n",
      "Epoch [5/8], Step [483/750], Loss: 0.0061\n",
      "Epoch [5/8], Step [484/750], Loss: 0.0153\n",
      "Epoch [5/8], Step [485/750], Loss: 0.0380\n",
      "Epoch [5/8], Step [486/750], Loss: 0.0261\n",
      "Epoch [5/8], Step [487/750], Loss: 0.0188\n",
      "Epoch [5/8], Step [488/750], Loss: 0.0053\n",
      "Epoch [5/8], Step [489/750], Loss: 0.0619\n",
      "Epoch [5/8], Step [490/750], Loss: 0.1076\n",
      "Epoch [5/8], Step [491/750], Loss: 0.0275\n",
      "Epoch [5/8], Step [492/750], Loss: 0.0620\n",
      "Epoch [5/8], Step [493/750], Loss: 0.0076\n",
      "Epoch [5/8], Step [494/750], Loss: 0.0904\n",
      "Epoch [5/8], Step [495/750], Loss: 0.0399\n",
      "Epoch [5/8], Step [496/750], Loss: 0.0239\n",
      "Epoch [5/8], Step [497/750], Loss: 0.0804\n",
      "Epoch [5/8], Step [498/750], Loss: 0.0685\n",
      "Epoch [5/8], Step [499/750], Loss: 0.0184\n",
      "Epoch [5/8], Step [500/750], Loss: 0.0114\n",
      "Epoch [5/8], Step [501/750], Loss: 0.0202\n",
      "Epoch [5/8], Step [502/750], Loss: 0.0100\n",
      "Epoch [5/8], Step [503/750], Loss: 0.0834\n",
      "Epoch [5/8], Step [504/750], Loss: 0.0149\n",
      "Epoch [5/8], Step [505/750], Loss: 0.0051\n",
      "Epoch [5/8], Step [506/750], Loss: 0.0039\n",
      "Epoch [5/8], Step [507/750], Loss: 0.0027\n",
      "Epoch [5/8], Step [508/750], Loss: 0.0605\n",
      "Epoch [5/8], Step [509/750], Loss: 0.1018\n",
      "Epoch [5/8], Step [510/750], Loss: 0.0499\n",
      "Epoch [5/8], Step [511/750], Loss: 0.0267\n",
      "Epoch [5/8], Step [512/750], Loss: 0.0428\n",
      "Epoch [5/8], Step [513/750], Loss: 0.0163\n",
      "Epoch [5/8], Step [514/750], Loss: 0.0377\n",
      "Epoch [5/8], Step [515/750], Loss: 0.0540\n",
      "Epoch [5/8], Step [516/750], Loss: 0.0637\n",
      "Epoch [5/8], Step [517/750], Loss: 0.1151\n",
      "Epoch [5/8], Step [518/750], Loss: 0.0040\n",
      "Epoch [5/8], Step [519/750], Loss: 0.0101\n",
      "Epoch [5/8], Step [520/750], Loss: 0.0959\n",
      "Epoch [5/8], Step [521/750], Loss: 0.0035\n",
      "Epoch [5/8], Step [522/750], Loss: 0.0057\n",
      "Epoch [5/8], Step [523/750], Loss: 0.0148\n",
      "Epoch [5/8], Step [524/750], Loss: 0.0939\n",
      "Epoch [5/8], Step [525/750], Loss: 0.0386\n",
      "Epoch [5/8], Step [526/750], Loss: 0.0572\n",
      "Epoch [5/8], Step [527/750], Loss: 0.0015\n",
      "Epoch [5/8], Step [528/750], Loss: 0.0226\n",
      "Epoch [5/8], Step [529/750], Loss: 0.0548\n",
      "Epoch [5/8], Step [530/750], Loss: 0.0220\n",
      "Epoch [5/8], Step [531/750], Loss: 0.0157\n",
      "Epoch [5/8], Step [532/750], Loss: 0.0126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Step [533/750], Loss: 0.0214\n",
      "Epoch [5/8], Step [534/750], Loss: 0.0155\n",
      "Epoch [5/8], Step [535/750], Loss: 0.0078\n",
      "Epoch [5/8], Step [536/750], Loss: 0.0380\n",
      "Epoch [5/8], Step [537/750], Loss: 0.0103\n",
      "Epoch [5/8], Step [538/750], Loss: 0.0161\n",
      "Epoch [5/8], Step [539/750], Loss: 0.0982\n",
      "Epoch [5/8], Step [540/750], Loss: 0.1116\n",
      "Epoch [5/8], Step [541/750], Loss: 0.0056\n",
      "Epoch [5/8], Step [542/750], Loss: 0.0025\n",
      "Epoch [5/8], Step [543/750], Loss: 0.0017\n",
      "Epoch [5/8], Step [544/750], Loss: 0.0397\n",
      "Epoch [5/8], Step [545/750], Loss: 0.0061\n",
      "Epoch [5/8], Step [546/750], Loss: 0.0299\n",
      "Epoch [5/8], Step [547/750], Loss: 0.2303\n",
      "Epoch [5/8], Step [548/750], Loss: 0.0338\n",
      "Epoch [5/8], Step [549/750], Loss: 0.0766\n",
      "Epoch [5/8], Step [550/750], Loss: 0.0838\n",
      "Epoch [5/8], Step [551/750], Loss: 0.0353\n",
      "Epoch [5/8], Step [552/750], Loss: 0.0046\n",
      "Epoch [5/8], Step [553/750], Loss: 0.0216\n",
      "Epoch [5/8], Step [554/750], Loss: 0.0198\n",
      "Epoch [5/8], Step [555/750], Loss: 0.0088\n",
      "Epoch [5/8], Step [556/750], Loss: 0.0026\n",
      "Epoch [5/8], Step [557/750], Loss: 0.0045\n",
      "Epoch [5/8], Step [558/750], Loss: 0.0144\n",
      "Epoch [5/8], Step [559/750], Loss: 0.0376\n",
      "Epoch [5/8], Step [560/750], Loss: 0.0075\n",
      "Epoch [5/8], Step [561/750], Loss: 0.0031\n",
      "Epoch [5/8], Step [562/750], Loss: 0.0659\n",
      "Epoch [5/8], Step [563/750], Loss: 0.0798\n",
      "Epoch [5/8], Step [564/750], Loss: 0.0450\n",
      "Epoch [5/8], Step [565/750], Loss: 0.0072\n",
      "Epoch [5/8], Step [566/750], Loss: 0.0071\n",
      "Epoch [5/8], Step [567/750], Loss: 0.0626\n",
      "Epoch [5/8], Step [568/750], Loss: 0.0771\n",
      "Epoch [5/8], Step [569/750], Loss: 0.0667\n",
      "Epoch [5/8], Step [570/750], Loss: 0.0411\n",
      "Epoch [5/8], Step [571/750], Loss: 0.0112\n",
      "Epoch [5/8], Step [572/750], Loss: 0.0144\n",
      "Epoch [5/8], Step [573/750], Loss: 0.0013\n",
      "Epoch [5/8], Step [574/750], Loss: 0.0033\n",
      "Epoch [5/8], Step [575/750], Loss: 0.0047\n",
      "Epoch [5/8], Step [576/750], Loss: 0.0482\n",
      "Epoch [5/8], Step [577/750], Loss: 0.1230\n",
      "Epoch [5/8], Step [578/750], Loss: 0.0245\n",
      "Epoch [5/8], Step [579/750], Loss: 0.0496\n",
      "Epoch [5/8], Step [580/750], Loss: 0.0641\n",
      "Epoch [5/8], Step [581/750], Loss: 0.0056\n",
      "Epoch [5/8], Step [582/750], Loss: 0.0021\n",
      "Epoch [5/8], Step [583/750], Loss: 0.0320\n",
      "Epoch [5/8], Step [584/750], Loss: 0.0042\n",
      "Epoch [5/8], Step [585/750], Loss: 0.0581\n",
      "Epoch [5/8], Step [586/750], Loss: 0.1340\n",
      "Epoch [5/8], Step [587/750], Loss: 0.0330\n",
      "Epoch [5/8], Step [588/750], Loss: 0.0659\n",
      "Epoch [5/8], Step [589/750], Loss: 0.0341\n",
      "Epoch [5/8], Step [590/750], Loss: 0.0830\n",
      "Epoch [5/8], Step [591/750], Loss: 0.0050\n",
      "Epoch [5/8], Step [592/750], Loss: 0.0066\n",
      "Epoch [5/8], Step [593/750], Loss: 0.0381\n",
      "Epoch [5/8], Step [594/750], Loss: 0.0401\n",
      "Epoch [5/8], Step [595/750], Loss: 0.0494\n",
      "Epoch [5/8], Step [596/750], Loss: 0.0862\n",
      "Epoch [5/8], Step [597/750], Loss: 0.0735\n",
      "Epoch [5/8], Step [598/750], Loss: 0.0126\n",
      "Epoch [5/8], Step [599/750], Loss: 0.0269\n",
      "Epoch [5/8], Step [600/750], Loss: 0.0063\n",
      "Epoch [5/8], Step [601/750], Loss: 0.0491\n",
      "Epoch [5/8], Step [602/750], Loss: 0.0358\n",
      "Epoch [5/8], Step [603/750], Loss: 0.0186\n",
      "Epoch [5/8], Step [604/750], Loss: 0.1348\n",
      "Epoch [5/8], Step [605/750], Loss: 0.0316\n",
      "Epoch [5/8], Step [606/750], Loss: 0.0781\n",
      "Epoch [5/8], Step [607/750], Loss: 0.0239\n",
      "Epoch [5/8], Step [608/750], Loss: 0.0129\n",
      "Epoch [5/8], Step [609/750], Loss: 0.0072\n",
      "Epoch [5/8], Step [610/750], Loss: 0.0189\n",
      "Epoch [5/8], Step [611/750], Loss: 0.0019\n",
      "Epoch [5/8], Step [612/750], Loss: 0.0023\n",
      "Epoch [5/8], Step [613/750], Loss: 0.0139\n",
      "Epoch [5/8], Step [614/750], Loss: 0.0810\n",
      "Epoch [5/8], Step [615/750], Loss: 0.2851\n",
      "Epoch [5/8], Step [616/750], Loss: 0.0601\n",
      "Epoch [5/8], Step [617/750], Loss: 0.1649\n",
      "Epoch [5/8], Step [618/750], Loss: 0.0430\n",
      "Epoch [5/8], Step [619/750], Loss: 0.0144\n",
      "Epoch [5/8], Step [620/750], Loss: 0.0358\n",
      "Epoch [5/8], Step [621/750], Loss: 0.0069\n",
      "Epoch [5/8], Step [622/750], Loss: 0.0543\n",
      "Epoch [5/8], Step [623/750], Loss: 0.0355\n",
      "Epoch [5/8], Step [624/750], Loss: 0.0780\n",
      "Epoch [5/8], Step [625/750], Loss: 0.0393\n",
      "Epoch [5/8], Step [626/750], Loss: 0.0234\n",
      "Epoch [5/8], Step [627/750], Loss: 0.0078\n",
      "Epoch [5/8], Step [628/750], Loss: 0.0075\n",
      "Epoch [5/8], Step [629/750], Loss: 0.0564\n",
      "Epoch [5/8], Step [630/750], Loss: 0.0191\n",
      "Epoch [5/8], Step [631/750], Loss: 0.0484\n",
      "Epoch [5/8], Step [632/750], Loss: 0.0489\n",
      "Epoch [5/8], Step [633/750], Loss: 0.1178\n",
      "Epoch [5/8], Step [634/750], Loss: 0.0126\n",
      "Epoch [5/8], Step [635/750], Loss: 0.0771\n",
      "Epoch [5/8], Step [636/750], Loss: 0.0116\n",
      "Epoch [5/8], Step [637/750], Loss: 0.0227\n",
      "Epoch [5/8], Step [638/750], Loss: 0.0320\n",
      "Epoch [5/8], Step [639/750], Loss: 0.0190\n",
      "Epoch [5/8], Step [640/750], Loss: 0.0129\n",
      "Epoch [5/8], Step [641/750], Loss: 0.0052\n",
      "Epoch [5/8], Step [642/750], Loss: 0.0564\n",
      "Epoch [5/8], Step [643/750], Loss: 0.0711\n",
      "Epoch [5/8], Step [644/750], Loss: 0.0056\n",
      "Epoch [5/8], Step [645/750], Loss: 0.0491\n",
      "Epoch [5/8], Step [646/750], Loss: 0.0317\n",
      "Epoch [5/8], Step [647/750], Loss: 0.0673\n",
      "Epoch [5/8], Step [648/750], Loss: 0.0667\n",
      "Epoch [5/8], Step [649/750], Loss: 0.0047\n",
      "Epoch [5/8], Step [650/750], Loss: 0.0041\n",
      "Epoch [5/8], Step [651/750], Loss: 0.0180\n",
      "Epoch [5/8], Step [652/750], Loss: 0.0057\n",
      "Epoch [5/8], Step [653/750], Loss: 0.0453\n",
      "Epoch [5/8], Step [654/750], Loss: 0.1381\n",
      "Epoch [5/8], Step [655/750], Loss: 0.0678\n",
      "Epoch [5/8], Step [656/750], Loss: 0.0059\n",
      "Epoch [5/8], Step [657/750], Loss: 0.0049\n",
      "Epoch [5/8], Step [658/750], Loss: 0.0909\n",
      "Epoch [5/8], Step [659/750], Loss: 0.0130\n",
      "Epoch [5/8], Step [660/750], Loss: 0.0879\n",
      "Epoch [5/8], Step [661/750], Loss: 0.0171\n",
      "Epoch [5/8], Step [662/750], Loss: 0.0357\n",
      "Epoch [5/8], Step [663/750], Loss: 0.0145\n",
      "Epoch [5/8], Step [664/750], Loss: 0.0059\n",
      "Epoch [5/8], Step [665/750], Loss: 0.0141\n",
      "Epoch [5/8], Step [666/750], Loss: 0.0275\n",
      "Epoch [5/8], Step [667/750], Loss: 0.0316\n",
      "Epoch [5/8], Step [668/750], Loss: 0.0767\n",
      "Epoch [5/8], Step [669/750], Loss: 0.1523\n",
      "Epoch [5/8], Step [670/750], Loss: 0.0020\n",
      "Epoch [5/8], Step [671/750], Loss: 0.0706\n",
      "Epoch [5/8], Step [672/750], Loss: 0.0289\n",
      "Epoch [5/8], Step [673/750], Loss: 0.1600\n",
      "Epoch [5/8], Step [674/750], Loss: 0.0027\n",
      "Epoch [5/8], Step [675/750], Loss: 0.0312\n",
      "Epoch [5/8], Step [676/750], Loss: 0.0285\n",
      "Epoch [5/8], Step [677/750], Loss: 0.0076\n",
      "Epoch [5/8], Step [678/750], Loss: 0.0542\n",
      "Epoch [5/8], Step [679/750], Loss: 0.0033\n",
      "Epoch [5/8], Step [680/750], Loss: 0.1610\n",
      "Epoch [5/8], Step [681/750], Loss: 0.0075\n",
      "Epoch [5/8], Step [682/750], Loss: 0.1023\n",
      "Epoch [5/8], Step [683/750], Loss: 0.0498\n",
      "Epoch [5/8], Step [684/750], Loss: 0.0105\n",
      "Epoch [5/8], Step [685/750], Loss: 0.0480\n",
      "Epoch [5/8], Step [686/750], Loss: 0.0219\n",
      "Epoch [5/8], Step [687/750], Loss: 0.0615\n",
      "Epoch [5/8], Step [688/750], Loss: 0.0273\n",
      "Epoch [5/8], Step [689/750], Loss: 0.0046\n",
      "Epoch [5/8], Step [690/750], Loss: 0.0056\n",
      "Epoch [5/8], Step [691/750], Loss: 0.0052\n",
      "Epoch [5/8], Step [692/750], Loss: 0.0026\n",
      "Epoch [5/8], Step [693/750], Loss: 0.0085\n",
      "Epoch [5/8], Step [694/750], Loss: 0.0355\n",
      "Epoch [5/8], Step [695/750], Loss: 0.0223\n",
      "Epoch [5/8], Step [696/750], Loss: 0.0620\n",
      "Epoch [5/8], Step [697/750], Loss: 0.0193\n",
      "Epoch [5/8], Step [698/750], Loss: 0.0081\n",
      "Epoch [5/8], Step [699/750], Loss: 0.0034\n",
      "Epoch [5/8], Step [700/750], Loss: 0.0515\n",
      "Epoch [5/8], Step [701/750], Loss: 0.1133\n",
      "Epoch [5/8], Step [702/750], Loss: 0.0741\n",
      "Epoch [5/8], Step [703/750], Loss: 0.0346\n",
      "Epoch [5/8], Step [704/750], Loss: 0.0143\n",
      "Epoch [5/8], Step [705/750], Loss: 0.0636\n",
      "Epoch [5/8], Step [706/750], Loss: 0.0498\n",
      "Epoch [5/8], Step [707/750], Loss: 0.0798\n",
      "Epoch [5/8], Step [708/750], Loss: 0.1013\n",
      "Epoch [5/8], Step [709/750], Loss: 0.0600\n",
      "Epoch [5/8], Step [710/750], Loss: 0.0828\n",
      "Epoch [5/8], Step [711/750], Loss: 0.0495\n",
      "Epoch [5/8], Step [712/750], Loss: 0.0265\n",
      "Epoch [5/8], Step [713/750], Loss: 0.0053\n",
      "Epoch [5/8], Step [714/750], Loss: 0.0581\n",
      "Epoch [5/8], Step [715/750], Loss: 0.1372\n",
      "Epoch [5/8], Step [716/750], Loss: 0.0879\n",
      "Epoch [5/8], Step [717/750], Loss: 0.0186\n",
      "Epoch [5/8], Step [718/750], Loss: 0.1418\n",
      "Epoch [5/8], Step [719/750], Loss: 0.0059\n",
      "Epoch [5/8], Step [720/750], Loss: 0.0475\n",
      "Epoch [5/8], Step [721/750], Loss: 0.0150\n",
      "Epoch [5/8], Step [722/750], Loss: 0.0088\n",
      "Epoch [5/8], Step [723/750], Loss: 0.0166\n",
      "Epoch [5/8], Step [724/750], Loss: 0.0896\n",
      "Epoch [5/8], Step [725/750], Loss: 0.0227\n",
      "Epoch [5/8], Step [726/750], Loss: 0.0567\n",
      "Epoch [5/8], Step [727/750], Loss: 0.0245\n",
      "Epoch [5/8], Step [728/750], Loss: 0.0767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Step [729/750], Loss: 0.0306\n",
      "Epoch [5/8], Step [730/750], Loss: 0.0801\n",
      "Epoch [5/8], Step [731/750], Loss: 0.0411\n",
      "Epoch [5/8], Step [732/750], Loss: 0.0043\n",
      "Epoch [5/8], Step [733/750], Loss: 0.0488\n",
      "Epoch [5/8], Step [734/750], Loss: 0.0208\n",
      "Epoch [5/8], Step [735/750], Loss: 0.0810\n",
      "Epoch [5/8], Step [736/750], Loss: 0.0305\n",
      "Epoch [5/8], Step [737/750], Loss: 0.0318\n",
      "Epoch [5/8], Step [738/750], Loss: 0.0015\n",
      "Epoch [5/8], Step [739/750], Loss: 0.0799\n",
      "Epoch [5/8], Step [740/750], Loss: 0.0140\n",
      "Epoch [5/8], Step [741/750], Loss: 0.0291\n",
      "Epoch [5/8], Step [742/750], Loss: 0.1051\n",
      "Epoch [5/8], Step [743/750], Loss: 0.0397\n",
      "Epoch [5/8], Step [744/750], Loss: 0.0630\n",
      "Epoch [5/8], Step [745/750], Loss: 0.0305\n",
      "Epoch [5/8], Step [746/750], Loss: 0.1088\n",
      "Epoch [5/8], Step [747/750], Loss: 0.0113\n",
      "Epoch [5/8], Step [748/750], Loss: 0.0129\n",
      "Epoch [5/8], Step [749/750], Loss: 0.0396\n",
      "Epoch [5/8], Step [750/750], Loss: 0.0080\n",
      "Epoch [5/8], Tr. loss: 0.3470. Test loss: 0.2710\n",
      "\n",
      "\n",
      "Epoch [6/8], Step [1/750], Loss: 0.0873\n",
      "Epoch [6/8], Step [2/750], Loss: 0.0465\n",
      "Epoch [6/8], Step [3/750], Loss: 0.0758\n",
      "Epoch [6/8], Step [4/750], Loss: 0.0023\n",
      "Epoch [6/8], Step [5/750], Loss: 0.0206\n",
      "Epoch [6/8], Step [6/750], Loss: 0.0315\n",
      "Epoch [6/8], Step [7/750], Loss: 0.0182\n",
      "Epoch [6/8], Step [8/750], Loss: 0.0209\n",
      "Epoch [6/8], Step [9/750], Loss: 0.0615\n",
      "Epoch [6/8], Step [10/750], Loss: 0.1071\n",
      "Epoch [6/8], Step [11/750], Loss: 0.0105\n",
      "Epoch [6/8], Step [12/750], Loss: 0.0441\n",
      "Epoch [6/8], Step [13/750], Loss: 0.0069\n",
      "Epoch [6/8], Step [14/750], Loss: 0.0274\n",
      "Epoch [6/8], Step [15/750], Loss: 0.0261\n",
      "Epoch [6/8], Step [16/750], Loss: 0.0842\n",
      "Epoch [6/8], Step [17/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [18/750], Loss: 0.0715\n",
      "Epoch [6/8], Step [19/750], Loss: 0.0145\n",
      "Epoch [6/8], Step [20/750], Loss: 0.0193\n",
      "Epoch [6/8], Step [21/750], Loss: 0.0104\n",
      "Epoch [6/8], Step [22/750], Loss: 0.0024\n",
      "Epoch [6/8], Step [23/750], Loss: 0.0038\n",
      "Epoch [6/8], Step [24/750], Loss: 0.0579\n",
      "Epoch [6/8], Step [25/750], Loss: 0.0124\n",
      "Epoch [6/8], Step [26/750], Loss: 0.0984\n",
      "Epoch [6/8], Step [27/750], Loss: 0.0022\n",
      "Epoch [6/8], Step [28/750], Loss: 0.0181\n",
      "Epoch [6/8], Step [29/750], Loss: 0.0317\n",
      "Epoch [6/8], Step [30/750], Loss: 0.0006\n",
      "Epoch [6/8], Step [31/750], Loss: 0.0369\n",
      "Epoch [6/8], Step [32/750], Loss: 0.0442\n",
      "Epoch [6/8], Step [33/750], Loss: 0.0009\n",
      "Epoch [6/8], Step [34/750], Loss: 0.1045\n",
      "Epoch [6/8], Step [35/750], Loss: 0.0275\n",
      "Epoch [6/8], Step [36/750], Loss: 0.0611\n",
      "Epoch [6/8], Step [37/750], Loss: 0.0126\n",
      "Epoch [6/8], Step [38/750], Loss: 0.0005\n",
      "Epoch [6/8], Step [39/750], Loss: 0.0388\n",
      "Epoch [6/8], Step [40/750], Loss: 0.0373\n",
      "Epoch [6/8], Step [41/750], Loss: 0.0258\n",
      "Epoch [6/8], Step [42/750], Loss: 0.0334\n",
      "Epoch [6/8], Step [43/750], Loss: 0.0088\n",
      "Epoch [6/8], Step [44/750], Loss: 0.0053\n",
      "Epoch [6/8], Step [45/750], Loss: 0.0168\n",
      "Epoch [6/8], Step [46/750], Loss: 0.0271\n",
      "Epoch [6/8], Step [47/750], Loss: 0.0172\n",
      "Epoch [6/8], Step [48/750], Loss: 0.0677\n",
      "Epoch [6/8], Step [49/750], Loss: 0.0390\n",
      "Epoch [6/8], Step [50/750], Loss: 0.0218\n",
      "Epoch [6/8], Step [51/750], Loss: 0.0031\n",
      "Epoch [6/8], Step [52/750], Loss: 0.0043\n",
      "Epoch [6/8], Step [53/750], Loss: 0.1418\n",
      "Epoch [6/8], Step [54/750], Loss: 0.0065\n",
      "Epoch [6/8], Step [55/750], Loss: 0.0064\n",
      "Epoch [6/8], Step [56/750], Loss: 0.0057\n",
      "Epoch [6/8], Step [57/750], Loss: 0.0273\n",
      "Epoch [6/8], Step [58/750], Loss: 0.0179\n",
      "Epoch [6/8], Step [59/750], Loss: 0.0774\n",
      "Epoch [6/8], Step [60/750], Loss: 0.0362\n",
      "Epoch [6/8], Step [61/750], Loss: 0.0263\n",
      "Epoch [6/8], Step [62/750], Loss: 0.0421\n",
      "Epoch [6/8], Step [63/750], Loss: 0.0049\n",
      "Epoch [6/8], Step [64/750], Loss: 0.0025\n",
      "Epoch [6/8], Step [65/750], Loss: 0.0080\n",
      "Epoch [6/8], Step [66/750], Loss: 0.0163\n",
      "Epoch [6/8], Step [67/750], Loss: 0.0064\n",
      "Epoch [6/8], Step [68/750], Loss: 0.0413\n",
      "Epoch [6/8], Step [69/750], Loss: 0.0209\n",
      "Epoch [6/8], Step [70/750], Loss: 0.0233\n",
      "Epoch [6/8], Step [71/750], Loss: 0.0093\n",
      "Epoch [6/8], Step [72/750], Loss: 0.0187\n",
      "Epoch [6/8], Step [73/750], Loss: 0.0049\n",
      "Epoch [6/8], Step [74/750], Loss: 0.0561\n",
      "Epoch [6/8], Step [75/750], Loss: 0.0104\n",
      "Epoch [6/8], Step [76/750], Loss: 0.0126\n",
      "Epoch [6/8], Step [77/750], Loss: 0.0025\n",
      "Epoch [6/8], Step [78/750], Loss: 0.0023\n",
      "Epoch [6/8], Step [79/750], Loss: 0.0034\n",
      "Epoch [6/8], Step [80/750], Loss: 0.0045\n",
      "Epoch [6/8], Step [81/750], Loss: 0.1238\n",
      "Epoch [6/8], Step [82/750], Loss: 0.0059\n",
      "Epoch [6/8], Step [83/750], Loss: 0.0204\n",
      "Epoch [6/8], Step [84/750], Loss: 0.0171\n",
      "Epoch [6/8], Step [85/750], Loss: 0.0040\n",
      "Epoch [6/8], Step [86/750], Loss: 0.0041\n",
      "Epoch [6/8], Step [87/750], Loss: 0.0187\n",
      "Epoch [6/8], Step [88/750], Loss: 0.0027\n",
      "Epoch [6/8], Step [89/750], Loss: 0.0066\n",
      "Epoch [6/8], Step [90/750], Loss: 0.0022\n",
      "Epoch [6/8], Step [91/750], Loss: 0.0023\n",
      "Epoch [6/8], Step [92/750], Loss: 0.0141\n",
      "Epoch [6/8], Step [93/750], Loss: 0.0222\n",
      "Epoch [6/8], Step [94/750], Loss: 0.0280\n",
      "Epoch [6/8], Step [95/750], Loss: 0.0280\n",
      "Epoch [6/8], Step [96/750], Loss: 0.0480\n",
      "Epoch [6/8], Step [97/750], Loss: 0.0033\n",
      "Epoch [6/8], Step [98/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [99/750], Loss: 0.0344\n",
      "Epoch [6/8], Step [100/750], Loss: 0.0121\n",
      "Epoch [6/8], Step [101/750], Loss: 0.0014\n",
      "Epoch [6/8], Step [102/750], Loss: 0.0009\n",
      "Epoch [6/8], Step [103/750], Loss: 0.0514\n",
      "Epoch [6/8], Step [104/750], Loss: 0.0123\n",
      "Epoch [6/8], Step [105/750], Loss: 0.0312\n",
      "Epoch [6/8], Step [106/750], Loss: 0.0525\n",
      "Epoch [6/8], Step [107/750], Loss: 0.0054\n",
      "Epoch [6/8], Step [108/750], Loss: 0.0352\n",
      "Epoch [6/8], Step [109/750], Loss: 0.0463\n",
      "Epoch [6/8], Step [110/750], Loss: 0.1211\n",
      "Epoch [6/8], Step [111/750], Loss: 0.0016\n",
      "Epoch [6/8], Step [112/750], Loss: 0.0092\n",
      "Epoch [6/8], Step [113/750], Loss: 0.0118\n",
      "Epoch [6/8], Step [114/750], Loss: 0.0198\n",
      "Epoch [6/8], Step [115/750], Loss: 0.0722\n",
      "Epoch [6/8], Step [116/750], Loss: 0.0025\n",
      "Epoch [6/8], Step [117/750], Loss: 0.0541\n",
      "Epoch [6/8], Step [118/750], Loss: 0.0354\n",
      "Epoch [6/8], Step [119/750], Loss: 0.0079\n",
      "Epoch [6/8], Step [120/750], Loss: 0.0620\n",
      "Epoch [6/8], Step [121/750], Loss: 0.0039\n",
      "Epoch [6/8], Step [122/750], Loss: 0.0410\n",
      "Epoch [6/8], Step [123/750], Loss: 0.0031\n",
      "Epoch [6/8], Step [124/750], Loss: 0.0403\n",
      "Epoch [6/8], Step [125/750], Loss: 0.0065\n",
      "Epoch [6/8], Step [126/750], Loss: 0.0056\n",
      "Epoch [6/8], Step [127/750], Loss: 0.0224\n",
      "Epoch [6/8], Step [128/750], Loss: 0.0535\n",
      "Epoch [6/8], Step [129/750], Loss: 0.0219\n",
      "Epoch [6/8], Step [130/750], Loss: 0.0049\n",
      "Epoch [6/8], Step [131/750], Loss: 0.0208\n",
      "Epoch [6/8], Step [132/750], Loss: 0.0218\n",
      "Epoch [6/8], Step [133/750], Loss: 0.0565\n",
      "Epoch [6/8], Step [134/750], Loss: 0.0589\n",
      "Epoch [6/8], Step [135/750], Loss: 0.0028\n",
      "Epoch [6/8], Step [136/750], Loss: 0.0059\n",
      "Epoch [6/8], Step [137/750], Loss: 0.0048\n",
      "Epoch [6/8], Step [138/750], Loss: 0.0096\n",
      "Epoch [6/8], Step [139/750], Loss: 0.0208\n",
      "Epoch [6/8], Step [140/750], Loss: 0.0469\n",
      "Epoch [6/8], Step [141/750], Loss: 0.0109\n",
      "Epoch [6/8], Step [142/750], Loss: 0.0158\n",
      "Epoch [6/8], Step [143/750], Loss: 0.0060\n",
      "Epoch [6/8], Step [144/750], Loss: 0.0064\n",
      "Epoch [6/8], Step [145/750], Loss: 0.0167\n",
      "Epoch [6/8], Step [146/750], Loss: 0.0174\n",
      "Epoch [6/8], Step [147/750], Loss: 0.0148\n",
      "Epoch [6/8], Step [148/750], Loss: 0.1595\n",
      "Epoch [6/8], Step [149/750], Loss: 0.0458\n",
      "Epoch [6/8], Step [150/750], Loss: 0.0120\n",
      "Epoch [6/8], Step [151/750], Loss: 0.0025\n",
      "Epoch [6/8], Step [152/750], Loss: 0.0049\n",
      "Epoch [6/8], Step [153/750], Loss: 0.0296\n",
      "Epoch [6/8], Step [154/750], Loss: 0.0018\n",
      "Epoch [6/8], Step [155/750], Loss: 0.1179\n",
      "Epoch [6/8], Step [156/750], Loss: 0.0309\n",
      "Epoch [6/8], Step [157/750], Loss: 0.0017\n",
      "Epoch [6/8], Step [158/750], Loss: 0.0016\n",
      "Epoch [6/8], Step [159/750], Loss: 0.0068\n",
      "Epoch [6/8], Step [160/750], Loss: 0.0122\n",
      "Epoch [6/8], Step [161/750], Loss: 0.0469\n",
      "Epoch [6/8], Step [162/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [163/750], Loss: 0.0362\n",
      "Epoch [6/8], Step [164/750], Loss: 0.0419\n",
      "Epoch [6/8], Step [165/750], Loss: 0.0566\n",
      "Epoch [6/8], Step [166/750], Loss: 0.0203\n",
      "Epoch [6/8], Step [167/750], Loss: 0.0397\n",
      "Epoch [6/8], Step [168/750], Loss: 0.0055\n",
      "Epoch [6/8], Step [169/750], Loss: 0.0037\n",
      "Epoch [6/8], Step [170/750], Loss: 0.0700\n",
      "Epoch [6/8], Step [171/750], Loss: 0.0038\n",
      "Epoch [6/8], Step [172/750], Loss: 0.1304\n",
      "Epoch [6/8], Step [173/750], Loss: 0.0183\n",
      "Epoch [6/8], Step [174/750], Loss: 0.0079\n",
      "Epoch [6/8], Step [175/750], Loss: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8], Step [176/750], Loss: 0.0685\n",
      "Epoch [6/8], Step [177/750], Loss: 0.0054\n",
      "Epoch [6/8], Step [178/750], Loss: 0.0331\n",
      "Epoch [6/8], Step [179/750], Loss: 0.0350\n",
      "Epoch [6/8], Step [180/750], Loss: 0.0113\n",
      "Epoch [6/8], Step [181/750], Loss: 0.0375\n",
      "Epoch [6/8], Step [182/750], Loss: 0.0143\n",
      "Epoch [6/8], Step [183/750], Loss: 0.0288\n",
      "Epoch [6/8], Step [184/750], Loss: 0.0331\n",
      "Epoch [6/8], Step [185/750], Loss: 0.0215\n",
      "Epoch [6/8], Step [186/750], Loss: 0.0270\n",
      "Epoch [6/8], Step [187/750], Loss: 0.0182\n",
      "Epoch [6/8], Step [188/750], Loss: 0.0053\n",
      "Epoch [6/8], Step [189/750], Loss: 0.0255\n",
      "Epoch [6/8], Step [190/750], Loss: 0.0104\n",
      "Epoch [6/8], Step [191/750], Loss: 0.0038\n",
      "Epoch [6/8], Step [192/750], Loss: 0.0349\n",
      "Epoch [6/8], Step [193/750], Loss: 0.0345\n",
      "Epoch [6/8], Step [194/750], Loss: 0.0176\n",
      "Epoch [6/8], Step [195/750], Loss: 0.0046\n",
      "Epoch [6/8], Step [196/750], Loss: 0.0046\n",
      "Epoch [6/8], Step [197/750], Loss: 0.0080\n",
      "Epoch [6/8], Step [198/750], Loss: 0.0168\n",
      "Epoch [6/8], Step [199/750], Loss: 0.0293\n",
      "Epoch [6/8], Step [200/750], Loss: 0.0180\n",
      "Epoch [6/8], Step [201/750], Loss: 0.0156\n",
      "Epoch [6/8], Step [202/750], Loss: 0.0032\n",
      "Epoch [6/8], Step [203/750], Loss: 0.0104\n",
      "Epoch [6/8], Step [204/750], Loss: 0.0015\n",
      "Epoch [6/8], Step [205/750], Loss: 0.0256\n",
      "Epoch [6/8], Step [206/750], Loss: 0.0389\n",
      "Epoch [6/8], Step [207/750], Loss: 0.0025\n",
      "Epoch [6/8], Step [208/750], Loss: 0.0063\n",
      "Epoch [6/8], Step [209/750], Loss: 0.0327\n",
      "Epoch [6/8], Step [210/750], Loss: 0.0066\n",
      "Epoch [6/8], Step [211/750], Loss: 0.0316\n",
      "Epoch [6/8], Step [212/750], Loss: 0.0710\n",
      "Epoch [6/8], Step [213/750], Loss: 0.0111\n",
      "Epoch [6/8], Step [214/750], Loss: 0.0638\n",
      "Epoch [6/8], Step [215/750], Loss: 0.0755\n",
      "Epoch [6/8], Step [216/750], Loss: 0.0060\n",
      "Epoch [6/8], Step [217/750], Loss: 0.0009\n",
      "Epoch [6/8], Step [218/750], Loss: 0.0521\n",
      "Epoch [6/8], Step [219/750], Loss: 0.0074\n",
      "Epoch [6/8], Step [220/750], Loss: 0.0078\n",
      "Epoch [6/8], Step [221/750], Loss: 0.0130\n",
      "Epoch [6/8], Step [222/750], Loss: 0.0268\n",
      "Epoch [6/8], Step [223/750], Loss: 0.0193\n",
      "Epoch [6/8], Step [224/750], Loss: 0.0061\n",
      "Epoch [6/8], Step [225/750], Loss: 0.0359\n",
      "Epoch [6/8], Step [226/750], Loss: 0.0058\n",
      "Epoch [6/8], Step [227/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [228/750], Loss: 0.0161\n",
      "Epoch [6/8], Step [229/750], Loss: 0.0025\n",
      "Epoch [6/8], Step [230/750], Loss: 0.0156\n",
      "Epoch [6/8], Step [231/750], Loss: 0.0085\n",
      "Epoch [6/8], Step [232/750], Loss: 0.0194\n",
      "Epoch [6/8], Step [233/750], Loss: 0.0539\n",
      "Epoch [6/8], Step [234/750], Loss: 0.0007\n",
      "Epoch [6/8], Step [235/750], Loss: 0.0127\n",
      "Epoch [6/8], Step [236/750], Loss: 0.0039\n",
      "Epoch [6/8], Step [237/750], Loss: 0.0345\n",
      "Epoch [6/8], Step [238/750], Loss: 0.0156\n",
      "Epoch [6/8], Step [239/750], Loss: 0.0306\n",
      "Epoch [6/8], Step [240/750], Loss: 0.0277\n",
      "Epoch [6/8], Step [241/750], Loss: 0.1434\n",
      "Epoch [6/8], Step [242/750], Loss: 0.0021\n",
      "Epoch [6/8], Step [243/750], Loss: 0.0715\n",
      "Epoch [6/8], Step [244/750], Loss: 0.0018\n",
      "Epoch [6/8], Step [245/750], Loss: 0.0024\n",
      "Epoch [6/8], Step [246/750], Loss: 0.1005\n",
      "Epoch [6/8], Step [247/750], Loss: 0.0101\n",
      "Epoch [6/8], Step [248/750], Loss: 0.0076\n",
      "Epoch [6/8], Step [249/750], Loss: 0.0315\n",
      "Epoch [6/8], Step [250/750], Loss: 0.0066\n",
      "Epoch [6/8], Step [251/750], Loss: 0.0396\n",
      "Epoch [6/8], Step [252/750], Loss: 0.0090\n",
      "Epoch [6/8], Step [253/750], Loss: 0.0207\n",
      "Epoch [6/8], Step [254/750], Loss: 0.0032\n",
      "Epoch [6/8], Step [255/750], Loss: 0.0013\n",
      "Epoch [6/8], Step [256/750], Loss: 0.0052\n",
      "Epoch [6/8], Step [257/750], Loss: 0.0986\n",
      "Epoch [6/8], Step [258/750], Loss: 0.0114\n",
      "Epoch [6/8], Step [259/750], Loss: 0.1102\n",
      "Epoch [6/8], Step [260/750], Loss: 0.0156\n",
      "Epoch [6/8], Step [261/750], Loss: 0.0341\n",
      "Epoch [6/8], Step [262/750], Loss: 0.0112\n",
      "Epoch [6/8], Step [263/750], Loss: 0.0393\n",
      "Epoch [6/8], Step [264/750], Loss: 0.0023\n",
      "Epoch [6/8], Step [265/750], Loss: 0.0069\n",
      "Epoch [6/8], Step [266/750], Loss: 0.0026\n",
      "Epoch [6/8], Step [267/750], Loss: 0.0031\n",
      "Epoch [6/8], Step [268/750], Loss: 0.0983\n",
      "Epoch [6/8], Step [269/750], Loss: 0.0327\n",
      "Epoch [6/8], Step [270/750], Loss: 0.0041\n",
      "Epoch [6/8], Step [271/750], Loss: 0.0157\n",
      "Epoch [6/8], Step [272/750], Loss: 0.0374\n",
      "Epoch [6/8], Step [273/750], Loss: 0.0064\n",
      "Epoch [6/8], Step [274/750], Loss: 0.0491\n",
      "Epoch [6/8], Step [275/750], Loss: 0.0130\n",
      "Epoch [6/8], Step [276/750], Loss: 0.0262\n",
      "Epoch [6/8], Step [277/750], Loss: 0.0225\n",
      "Epoch [6/8], Step [278/750], Loss: 0.0165\n",
      "Epoch [6/8], Step [279/750], Loss: 0.0549\n",
      "Epoch [6/8], Step [280/750], Loss: 0.1303\n",
      "Epoch [6/8], Step [281/750], Loss: 0.1089\n",
      "Epoch [6/8], Step [282/750], Loss: 0.0022\n",
      "Epoch [6/8], Step [283/750], Loss: 0.0334\n",
      "Epoch [6/8], Step [284/750], Loss: 0.0027\n",
      "Epoch [6/8], Step [285/750], Loss: 0.0245\n",
      "Epoch [6/8], Step [286/750], Loss: 0.0295\n",
      "Epoch [6/8], Step [287/750], Loss: 0.0387\n",
      "Epoch [6/8], Step [288/750], Loss: 0.0453\n",
      "Epoch [6/8], Step [289/750], Loss: 0.0261\n",
      "Epoch [6/8], Step [290/750], Loss: 0.0089\n",
      "Epoch [6/8], Step [291/750], Loss: 0.0122\n",
      "Epoch [6/8], Step [292/750], Loss: 0.0360\n",
      "Epoch [6/8], Step [293/750], Loss: 0.0227\n",
      "Epoch [6/8], Step [294/750], Loss: 0.0044\n",
      "Epoch [6/8], Step [295/750], Loss: 0.0559\n",
      "Epoch [6/8], Step [296/750], Loss: 0.0060\n",
      "Epoch [6/8], Step [297/750], Loss: 0.0565\n",
      "Epoch [6/8], Step [298/750], Loss: 0.0322\n",
      "Epoch [6/8], Step [299/750], Loss: 0.0049\n",
      "Epoch [6/8], Step [300/750], Loss: 0.0564\n",
      "Epoch [6/8], Step [301/750], Loss: 0.0444\n",
      "Epoch [6/8], Step [302/750], Loss: 0.0091\n",
      "Epoch [6/8], Step [303/750], Loss: 0.0016\n",
      "Epoch [6/8], Step [304/750], Loss: 0.0132\n",
      "Epoch [6/8], Step [305/750], Loss: 0.1670\n",
      "Epoch [6/8], Step [306/750], Loss: 0.0264\n",
      "Epoch [6/8], Step [307/750], Loss: 0.0034\n",
      "Epoch [6/8], Step [308/750], Loss: 0.0265\n",
      "Epoch [6/8], Step [309/750], Loss: 0.0689\n",
      "Epoch [6/8], Step [310/750], Loss: 0.0107\n",
      "Epoch [6/8], Step [311/750], Loss: 0.0263\n",
      "Epoch [6/8], Step [312/750], Loss: 0.0164\n",
      "Epoch [6/8], Step [313/750], Loss: 0.0163\n",
      "Epoch [6/8], Step [314/750], Loss: 0.0085\n",
      "Epoch [6/8], Step [315/750], Loss: 0.0313\n",
      "Epoch [6/8], Step [316/750], Loss: 0.0218\n",
      "Epoch [6/8], Step [317/750], Loss: 0.0105\n",
      "Epoch [6/8], Step [318/750], Loss: 0.0087\n",
      "Epoch [6/8], Step [319/750], Loss: 0.0074\n",
      "Epoch [6/8], Step [320/750], Loss: 0.1027\n",
      "Epoch [6/8], Step [321/750], Loss: 0.0478\n",
      "Epoch [6/8], Step [322/750], Loss: 0.0185\n",
      "Epoch [6/8], Step [323/750], Loss: 0.0156\n",
      "Epoch [6/8], Step [324/750], Loss: 0.0431\n",
      "Epoch [6/8], Step [325/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [326/750], Loss: 0.1175\n",
      "Epoch [6/8], Step [327/750], Loss: 0.0039\n",
      "Epoch [6/8], Step [328/750], Loss: 0.0708\n",
      "Epoch [6/8], Step [329/750], Loss: 0.0032\n",
      "Epoch [6/8], Step [330/750], Loss: 0.0503\n",
      "Epoch [6/8], Step [331/750], Loss: 0.0544\n",
      "Epoch [6/8], Step [332/750], Loss: 0.0049\n",
      "Epoch [6/8], Step [333/750], Loss: 0.1181\n",
      "Epoch [6/8], Step [334/750], Loss: 0.0484\n",
      "Epoch [6/8], Step [335/750], Loss: 0.0302\n",
      "Epoch [6/8], Step [336/750], Loss: 0.0299\n",
      "Epoch [6/8], Step [337/750], Loss: 0.0829\n",
      "Epoch [6/8], Step [338/750], Loss: 0.0051\n",
      "Epoch [6/8], Step [339/750], Loss: 0.0711\n",
      "Epoch [6/8], Step [340/750], Loss: 0.0637\n",
      "Epoch [6/8], Step [341/750], Loss: 0.0128\n",
      "Epoch [6/8], Step [342/750], Loss: 0.0795\n",
      "Epoch [6/8], Step [343/750], Loss: 0.1146\n",
      "Epoch [6/8], Step [344/750], Loss: 0.0506\n",
      "Epoch [6/8], Step [345/750], Loss: 0.1375\n",
      "Epoch [6/8], Step [346/750], Loss: 0.0495\n",
      "Epoch [6/8], Step [347/750], Loss: 0.0095\n",
      "Epoch [6/8], Step [348/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [349/750], Loss: 0.0158\n",
      "Epoch [6/8], Step [350/750], Loss: 0.0036\n",
      "Epoch [6/8], Step [351/750], Loss: 0.0374\n",
      "Epoch [6/8], Step [352/750], Loss: 0.0022\n",
      "Epoch [6/8], Step [353/750], Loss: 0.0141\n",
      "Epoch [6/8], Step [354/750], Loss: 0.0021\n",
      "Epoch [6/8], Step [355/750], Loss: 0.0244\n",
      "Epoch [6/8], Step [356/750], Loss: 0.1426\n",
      "Epoch [6/8], Step [357/750], Loss: 0.0307\n",
      "Epoch [6/8], Step [358/750], Loss: 0.0363\n",
      "Epoch [6/8], Step [359/750], Loss: 0.0174\n",
      "Epoch [6/8], Step [360/750], Loss: 0.0036\n",
      "Epoch [6/8], Step [361/750], Loss: 0.0071\n",
      "Epoch [6/8], Step [362/750], Loss: 0.0015\n",
      "Epoch [6/8], Step [363/750], Loss: 0.0020\n",
      "Epoch [6/8], Step [364/750], Loss: 0.0146\n",
      "Epoch [6/8], Step [365/750], Loss: 0.0022\n",
      "Epoch [6/8], Step [366/750], Loss: 0.0017\n",
      "Epoch [6/8], Step [367/750], Loss: 0.0009\n",
      "Epoch [6/8], Step [368/750], Loss: 0.0066\n",
      "Epoch [6/8], Step [369/750], Loss: 0.0144\n",
      "Epoch [6/8], Step [370/750], Loss: 0.0330\n",
      "Epoch [6/8], Step [371/750], Loss: 0.0019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8], Step [372/750], Loss: 0.0095\n",
      "Epoch [6/8], Step [373/750], Loss: 0.1159\n",
      "Epoch [6/8], Step [374/750], Loss: 0.0589\n",
      "Epoch [6/8], Step [375/750], Loss: 0.0082\n",
      "Epoch [6/8], Step [376/750], Loss: 0.1009\n",
      "Epoch [6/8], Step [377/750], Loss: 0.0023\n",
      "Epoch [6/8], Step [378/750], Loss: 0.0012\n",
      "Epoch [6/8], Step [379/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [380/750], Loss: 0.0145\n",
      "Epoch [6/8], Step [381/750], Loss: 0.0236\n",
      "Epoch [6/8], Step [382/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [383/750], Loss: 0.0026\n",
      "Epoch [6/8], Step [384/750], Loss: 0.0104\n",
      "Epoch [6/8], Step [385/750], Loss: 0.0209\n",
      "Epoch [6/8], Step [386/750], Loss: 0.0194\n",
      "Epoch [6/8], Step [387/750], Loss: 0.0759\n",
      "Epoch [6/8], Step [388/750], Loss: 0.0014\n",
      "Epoch [6/8], Step [389/750], Loss: 0.0156\n",
      "Epoch [6/8], Step [390/750], Loss: 0.0099\n",
      "Epoch [6/8], Step [391/750], Loss: 0.0838\n",
      "Epoch [6/8], Step [392/750], Loss: 0.0300\n",
      "Epoch [6/8], Step [393/750], Loss: 0.0034\n",
      "Epoch [6/8], Step [394/750], Loss: 0.0011\n",
      "Epoch [6/8], Step [395/750], Loss: 0.0229\n",
      "Epoch [6/8], Step [396/750], Loss: 0.0162\n",
      "Epoch [6/8], Step [397/750], Loss: 0.0027\n",
      "Epoch [6/8], Step [398/750], Loss: 0.0598\n",
      "Epoch [6/8], Step [399/750], Loss: 0.0608\n",
      "Epoch [6/8], Step [400/750], Loss: 0.0073\n",
      "Epoch [6/8], Step [401/750], Loss: 0.0097\n",
      "Epoch [6/8], Step [402/750], Loss: 0.0382\n",
      "Epoch [6/8], Step [403/750], Loss: 0.0059\n",
      "Epoch [6/8], Step [404/750], Loss: 0.0232\n",
      "Epoch [6/8], Step [405/750], Loss: 0.0075\n",
      "Epoch [6/8], Step [406/750], Loss: 0.0049\n",
      "Epoch [6/8], Step [407/750], Loss: 0.0183\n",
      "Epoch [6/8], Step [408/750], Loss: 0.0055\n",
      "Epoch [6/8], Step [409/750], Loss: 0.0042\n",
      "Epoch [6/8], Step [410/750], Loss: 0.0061\n",
      "Epoch [6/8], Step [411/750], Loss: 0.0322\n",
      "Epoch [6/8], Step [412/750], Loss: 0.0845\n",
      "Epoch [6/8], Step [413/750], Loss: 0.0039\n",
      "Epoch [6/8], Step [414/750], Loss: 0.0014\n",
      "Epoch [6/8], Step [415/750], Loss: 0.0049\n",
      "Epoch [6/8], Step [416/750], Loss: 0.0058\n",
      "Epoch [6/8], Step [417/750], Loss: 0.0013\n",
      "Epoch [6/8], Step [418/750], Loss: 0.0083\n",
      "Epoch [6/8], Step [419/750], Loss: 0.0016\n",
      "Epoch [6/8], Step [420/750], Loss: 0.0028\n",
      "Epoch [6/8], Step [421/750], Loss: 0.0117\n",
      "Epoch [6/8], Step [422/750], Loss: 0.0843\n",
      "Epoch [6/8], Step [423/750], Loss: 0.0062\n",
      "Epoch [6/8], Step [424/750], Loss: 0.0385\n",
      "Epoch [6/8], Step [425/750], Loss: 0.0898\n",
      "Epoch [6/8], Step [426/750], Loss: 0.0008\n",
      "Epoch [6/8], Step [427/750], Loss: 0.0010\n",
      "Epoch [6/8], Step [428/750], Loss: 0.0960\n",
      "Epoch [6/8], Step [429/750], Loss: 0.0588\n",
      "Epoch [6/8], Step [430/750], Loss: 0.0370\n",
      "Epoch [6/8], Step [431/750], Loss: 0.0027\n",
      "Epoch [6/8], Step [432/750], Loss: 0.0143\n",
      "Epoch [6/8], Step [433/750], Loss: 0.0282\n",
      "Epoch [6/8], Step [434/750], Loss: 0.0044\n",
      "Epoch [6/8], Step [435/750], Loss: 0.0131\n",
      "Epoch [6/8], Step [436/750], Loss: 0.0192\n",
      "Epoch [6/8], Step [437/750], Loss: 0.0140\n",
      "Epoch [6/8], Step [438/750], Loss: 0.0848\n",
      "Epoch [6/8], Step [439/750], Loss: 0.0061\n",
      "Epoch [6/8], Step [440/750], Loss: 0.0866\n",
      "Epoch [6/8], Step [441/750], Loss: 0.0387\n",
      "Epoch [6/8], Step [442/750], Loss: 0.0897\n",
      "Epoch [6/8], Step [443/750], Loss: 0.0026\n",
      "Epoch [6/8], Step [444/750], Loss: 0.0005\n",
      "Epoch [6/8], Step [445/750], Loss: 0.0028\n",
      "Epoch [6/8], Step [446/750], Loss: 0.1264\n",
      "Epoch [6/8], Step [447/750], Loss: 0.0160\n",
      "Epoch [6/8], Step [448/750], Loss: 0.0013\n",
      "Epoch [6/8], Step [449/750], Loss: 0.0362\n",
      "Epoch [6/8], Step [450/750], Loss: 0.0430\n",
      "Epoch [6/8], Step [451/750], Loss: 0.0108\n",
      "Epoch [6/8], Step [452/750], Loss: 0.0226\n",
      "Epoch [6/8], Step [453/750], Loss: 0.0895\n",
      "Epoch [6/8], Step [454/750], Loss: 0.0362\n",
      "Epoch [6/8], Step [455/750], Loss: 0.0064\n",
      "Epoch [6/8], Step [456/750], Loss: 0.0134\n",
      "Epoch [6/8], Step [457/750], Loss: 0.0676\n",
      "Epoch [6/8], Step [458/750], Loss: 0.0376\n",
      "Epoch [6/8], Step [459/750], Loss: 0.0255\n",
      "Epoch [6/8], Step [460/750], Loss: 0.0115\n",
      "Epoch [6/8], Step [461/750], Loss: 0.0102\n",
      "Epoch [6/8], Step [462/750], Loss: 0.0164\n",
      "Epoch [6/8], Step [463/750], Loss: 0.0149\n",
      "Epoch [6/8], Step [464/750], Loss: 0.0063\n",
      "Epoch [6/8], Step [465/750], Loss: 0.0321\n",
      "Epoch [6/8], Step [466/750], Loss: 0.0249\n",
      "Epoch [6/8], Step [467/750], Loss: 0.0261\n",
      "Epoch [6/8], Step [468/750], Loss: 0.0082\n",
      "Epoch [6/8], Step [469/750], Loss: 0.0139\n",
      "Epoch [6/8], Step [470/750], Loss: 0.0012\n",
      "Epoch [6/8], Step [471/750], Loss: 0.0209\n",
      "Epoch [6/8], Step [472/750], Loss: 0.0330\n",
      "Epoch [6/8], Step [473/750], Loss: 0.0932\n",
      "Epoch [6/8], Step [474/750], Loss: 0.0296\n",
      "Epoch [6/8], Step [475/750], Loss: 0.0283\n",
      "Epoch [6/8], Step [476/750], Loss: 0.0078\n",
      "Epoch [6/8], Step [477/750], Loss: 0.0032\n",
      "Epoch [6/8], Step [478/750], Loss: 0.0150\n",
      "Epoch [6/8], Step [479/750], Loss: 0.0059\n",
      "Epoch [6/8], Step [480/750], Loss: 0.0068\n",
      "Epoch [6/8], Step [481/750], Loss: 0.0015\n",
      "Epoch [6/8], Step [482/750], Loss: 0.0064\n",
      "Epoch [6/8], Step [483/750], Loss: 0.0216\n",
      "Epoch [6/8], Step [484/750], Loss: 0.0174\n",
      "Epoch [6/8], Step [485/750], Loss: 0.0157\n",
      "Epoch [6/8], Step [486/750], Loss: 0.0009\n",
      "Epoch [6/8], Step [487/750], Loss: 0.0632\n",
      "Epoch [6/8], Step [488/750], Loss: 0.0139\n",
      "Epoch [6/8], Step [489/750], Loss: 0.0013\n",
      "Epoch [6/8], Step [490/750], Loss: 0.0026\n",
      "Epoch [6/8], Step [491/750], Loss: 0.0619\n",
      "Epoch [6/8], Step [492/750], Loss: 0.0294\n",
      "Epoch [6/8], Step [493/750], Loss: 0.1775\n",
      "Epoch [6/8], Step [494/750], Loss: 0.0046\n",
      "Epoch [6/8], Step [495/750], Loss: 0.0036\n",
      "Epoch [6/8], Step [496/750], Loss: 0.1102\n",
      "Epoch [6/8], Step [497/750], Loss: 0.0668\n",
      "Epoch [6/8], Step [498/750], Loss: 0.0073\n",
      "Epoch [6/8], Step [499/750], Loss: 0.0015\n",
      "Epoch [6/8], Step [500/750], Loss: 0.0052\n",
      "Epoch [6/8], Step [501/750], Loss: 0.0209\n",
      "Epoch [6/8], Step [502/750], Loss: 0.0941\n",
      "Epoch [6/8], Step [503/750], Loss: 0.0048\n",
      "Epoch [6/8], Step [504/750], Loss: 0.0098\n",
      "Epoch [6/8], Step [505/750], Loss: 0.0138\n",
      "Epoch [6/8], Step [506/750], Loss: 0.0299\n",
      "Epoch [6/8], Step [507/750], Loss: 0.0648\n",
      "Epoch [6/8], Step [508/750], Loss: 0.0032\n",
      "Epoch [6/8], Step [509/750], Loss: 0.0263\n",
      "Epoch [6/8], Step [510/750], Loss: 0.0344\n",
      "Epoch [6/8], Step [511/750], Loss: 0.0691\n",
      "Epoch [6/8], Step [512/750], Loss: 0.0038\n",
      "Epoch [6/8], Step [513/750], Loss: 0.0031\n",
      "Epoch [6/8], Step [514/750], Loss: 0.0054\n",
      "Epoch [6/8], Step [515/750], Loss: 0.0836\n",
      "Epoch [6/8], Step [516/750], Loss: 0.0048\n",
      "Epoch [6/8], Step [517/750], Loss: 0.0027\n",
      "Epoch [6/8], Step [518/750], Loss: 0.0241\n",
      "Epoch [6/8], Step [519/750], Loss: 0.0212\n",
      "Epoch [6/8], Step [520/750], Loss: 0.0098\n",
      "Epoch [6/8], Step [521/750], Loss: 0.0152\n",
      "Epoch [6/8], Step [522/750], Loss: 0.0636\n",
      "Epoch [6/8], Step [523/750], Loss: 0.0961\n",
      "Epoch [6/8], Step [524/750], Loss: 0.0033\n",
      "Epoch [6/8], Step [525/750], Loss: 0.0808\n",
      "Epoch [6/8], Step [526/750], Loss: 0.1070\n",
      "Epoch [6/8], Step [527/750], Loss: 0.0273\n",
      "Epoch [6/8], Step [528/750], Loss: 0.0598\n",
      "Epoch [6/8], Step [529/750], Loss: 0.0167\n",
      "Epoch [6/8], Step [530/750], Loss: 0.0026\n",
      "Epoch [6/8], Step [531/750], Loss: 0.0993\n",
      "Epoch [6/8], Step [532/750], Loss: 0.0273\n",
      "Epoch [6/8], Step [533/750], Loss: 0.0802\n",
      "Epoch [6/8], Step [534/750], Loss: 0.0301\n",
      "Epoch [6/8], Step [535/750], Loss: 0.0192\n",
      "Epoch [6/8], Step [536/750], Loss: 0.0118\n",
      "Epoch [6/8], Step [537/750], Loss: 0.0063\n",
      "Epoch [6/8], Step [538/750], Loss: 0.0667\n",
      "Epoch [6/8], Step [539/750], Loss: 0.0188\n",
      "Epoch [6/8], Step [540/750], Loss: 0.0269\n",
      "Epoch [6/8], Step [541/750], Loss: 0.0224\n",
      "Epoch [6/8], Step [542/750], Loss: 0.0046\n",
      "Epoch [6/8], Step [543/750], Loss: 0.0228\n",
      "Epoch [6/8], Step [544/750], Loss: 0.0259\n",
      "Epoch [6/8], Step [545/750], Loss: 0.0241\n",
      "Epoch [6/8], Step [546/750], Loss: 0.1217\n",
      "Epoch [6/8], Step [547/750], Loss: 0.1487\n",
      "Epoch [6/8], Step [548/750], Loss: 0.0305\n",
      "Epoch [6/8], Step [549/750], Loss: 0.0045\n",
      "Epoch [6/8], Step [550/750], Loss: 0.0162\n",
      "Epoch [6/8], Step [551/750], Loss: 0.0604\n",
      "Epoch [6/8], Step [552/750], Loss: 0.0501\n",
      "Epoch [6/8], Step [553/750], Loss: 0.0145\n",
      "Epoch [6/8], Step [554/750], Loss: 0.0268\n",
      "Epoch [6/8], Step [555/750], Loss: 0.0398\n",
      "Epoch [6/8], Step [556/750], Loss: 0.0317\n",
      "Epoch [6/8], Step [557/750], Loss: 0.0130\n",
      "Epoch [6/8], Step [558/750], Loss: 0.0427\n",
      "Epoch [6/8], Step [559/750], Loss: 0.0081\n",
      "Epoch [6/8], Step [560/750], Loss: 0.0655\n",
      "Epoch [6/8], Step [561/750], Loss: 0.0477\n",
      "Epoch [6/8], Step [562/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [563/750], Loss: 0.0303\n",
      "Epoch [6/8], Step [564/750], Loss: 0.0227\n",
      "Epoch [6/8], Step [565/750], Loss: 0.0539\n",
      "Epoch [6/8], Step [566/750], Loss: 0.0425\n",
      "Epoch [6/8], Step [567/750], Loss: 0.1149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8], Step [568/750], Loss: 0.0071\n",
      "Epoch [6/8], Step [569/750], Loss: 0.0282\n",
      "Epoch [6/8], Step [570/750], Loss: 0.0240\n",
      "Epoch [6/8], Step [571/750], Loss: 0.0887\n",
      "Epoch [6/8], Step [572/750], Loss: 0.0055\n",
      "Epoch [6/8], Step [573/750], Loss: 0.0032\n",
      "Epoch [6/8], Step [574/750], Loss: 0.0206\n",
      "Epoch [6/8], Step [575/750], Loss: 0.0088\n",
      "Epoch [6/8], Step [576/750], Loss: 0.0048\n",
      "Epoch [6/8], Step [577/750], Loss: 0.0055\n",
      "Epoch [6/8], Step [578/750], Loss: 0.0065\n",
      "Epoch [6/8], Step [579/750], Loss: 0.0642\n",
      "Epoch [6/8], Step [580/750], Loss: 0.0210\n",
      "Epoch [6/8], Step [581/750], Loss: 0.0792\n",
      "Epoch [6/8], Step [582/750], Loss: 0.0309\n",
      "Epoch [6/8], Step [583/750], Loss: 0.0653\n",
      "Epoch [6/8], Step [584/750], Loss: 0.0321\n",
      "Epoch [6/8], Step [585/750], Loss: 0.1094\n",
      "Epoch [6/8], Step [586/750], Loss: 0.0229\n",
      "Epoch [6/8], Step [587/750], Loss: 0.0251\n",
      "Epoch [6/8], Step [588/750], Loss: 0.0084\n",
      "Epoch [6/8], Step [589/750], Loss: 0.0118\n",
      "Epoch [6/8], Step [590/750], Loss: 0.0010\n",
      "Epoch [6/8], Step [591/750], Loss: 0.0288\n",
      "Epoch [6/8], Step [592/750], Loss: 0.0322\n",
      "Epoch [6/8], Step [593/750], Loss: 0.0091\n",
      "Epoch [6/8], Step [594/750], Loss: 0.0027\n",
      "Epoch [6/8], Step [595/750], Loss: 0.0430\n",
      "Epoch [6/8], Step [596/750], Loss: 0.0595\n",
      "Epoch [6/8], Step [597/750], Loss: 0.0153\n",
      "Epoch [6/8], Step [598/750], Loss: 0.0021\n",
      "Epoch [6/8], Step [599/750], Loss: 0.0148\n",
      "Epoch [6/8], Step [600/750], Loss: 0.0121\n",
      "Epoch [6/8], Step [601/750], Loss: 0.0266\n",
      "Epoch [6/8], Step [602/750], Loss: 0.0367\n",
      "Epoch [6/8], Step [603/750], Loss: 0.0396\n",
      "Epoch [6/8], Step [604/750], Loss: 0.0010\n",
      "Epoch [6/8], Step [605/750], Loss: 0.0104\n",
      "Epoch [6/8], Step [606/750], Loss: 0.0019\n",
      "Epoch [6/8], Step [607/750], Loss: 0.0168\n",
      "Epoch [6/8], Step [608/750], Loss: 0.0074\n",
      "Epoch [6/8], Step [609/750], Loss: 0.0992\n",
      "Epoch [6/8], Step [610/750], Loss: 0.0145\n",
      "Epoch [6/8], Step [611/750], Loss: 0.0055\n",
      "Epoch [6/8], Step [612/750], Loss: 0.0327\n",
      "Epoch [6/8], Step [613/750], Loss: 0.0358\n",
      "Epoch [6/8], Step [614/750], Loss: 0.0083\n",
      "Epoch [6/8], Step [615/750], Loss: 0.0048\n",
      "Epoch [6/8], Step [616/750], Loss: 0.0302\n",
      "Epoch [6/8], Step [617/750], Loss: 0.0064\n",
      "Epoch [6/8], Step [618/750], Loss: 0.0176\n",
      "Epoch [6/8], Step [619/750], Loss: 0.0859\n",
      "Epoch [6/8], Step [620/750], Loss: 0.0330\n",
      "Epoch [6/8], Step [621/750], Loss: 0.0057\n",
      "Epoch [6/8], Step [622/750], Loss: 0.0041\n",
      "Epoch [6/8], Step [623/750], Loss: 0.0758\n",
      "Epoch [6/8], Step [624/750], Loss: 0.0616\n",
      "Epoch [6/8], Step [625/750], Loss: 0.0625\n",
      "Epoch [6/8], Step [626/750], Loss: 0.1027\n",
      "Epoch [6/8], Step [627/750], Loss: 0.0980\n",
      "Epoch [6/8], Step [628/750], Loss: 0.0148\n",
      "Epoch [6/8], Step [629/750], Loss: 0.0019\n",
      "Epoch [6/8], Step [630/750], Loss: 0.0409\n",
      "Epoch [6/8], Step [631/750], Loss: 0.0287\n",
      "Epoch [6/8], Step [632/750], Loss: 0.0347\n",
      "Epoch [6/8], Step [633/750], Loss: 0.0308\n",
      "Epoch [6/8], Step [634/750], Loss: 0.0717\n",
      "Epoch [6/8], Step [635/750], Loss: 0.0467\n",
      "Epoch [6/8], Step [636/750], Loss: 0.0618\n",
      "Epoch [6/8], Step [637/750], Loss: 0.1009\n",
      "Epoch [6/8], Step [638/750], Loss: 0.0031\n",
      "Epoch [6/8], Step [639/750], Loss: 0.0210\n",
      "Epoch [6/8], Step [640/750], Loss: 0.0692\n",
      "Epoch [6/8], Step [641/750], Loss: 0.0029\n",
      "Epoch [6/8], Step [642/750], Loss: 0.0229\n",
      "Epoch [6/8], Step [643/750], Loss: 0.0113\n",
      "Epoch [6/8], Step [644/750], Loss: 0.0171\n",
      "Epoch [6/8], Step [645/750], Loss: 0.0073\n",
      "Epoch [6/8], Step [646/750], Loss: 0.0377\n",
      "Epoch [6/8], Step [647/750], Loss: 0.0434\n",
      "Epoch [6/8], Step [648/750], Loss: 0.0031\n",
      "Epoch [6/8], Step [649/750], Loss: 0.0161\n",
      "Epoch [6/8], Step [650/750], Loss: 0.0338\n",
      "Epoch [6/8], Step [651/750], Loss: 0.0294\n",
      "Epoch [6/8], Step [652/750], Loss: 0.0102\n",
      "Epoch [6/8], Step [653/750], Loss: 0.0432\n",
      "Epoch [6/8], Step [654/750], Loss: 0.0030\n",
      "Epoch [6/8], Step [655/750], Loss: 0.0360\n",
      "Epoch [6/8], Step [656/750], Loss: 0.0510\n",
      "Epoch [6/8], Step [657/750], Loss: 0.0946\n",
      "Epoch [6/8], Step [658/750], Loss: 0.1492\n",
      "Epoch [6/8], Step [659/750], Loss: 0.0033\n",
      "Epoch [6/8], Step [660/750], Loss: 0.0481\n",
      "Epoch [6/8], Step [661/750], Loss: 0.0154\n",
      "Epoch [6/8], Step [662/750], Loss: 0.0231\n",
      "Epoch [6/8], Step [663/750], Loss: 0.0018\n",
      "Epoch [6/8], Step [664/750], Loss: 0.0100\n",
      "Epoch [6/8], Step [665/750], Loss: 0.0304\n",
      "Epoch [6/8], Step [666/750], Loss: 0.0045\n",
      "Epoch [6/8], Step [667/750], Loss: 0.0831\n",
      "Epoch [6/8], Step [668/750], Loss: 0.0897\n",
      "Epoch [6/8], Step [669/750], Loss: 0.0213\n",
      "Epoch [6/8], Step [670/750], Loss: 0.0641\n",
      "Epoch [6/8], Step [671/750], Loss: 0.0760\n",
      "Epoch [6/8], Step [672/750], Loss: 0.0734\n",
      "Epoch [6/8], Step [673/750], Loss: 0.0088\n",
      "Epoch [6/8], Step [674/750], Loss: 0.0238\n",
      "Epoch [6/8], Step [675/750], Loss: 0.0513\n",
      "Epoch [6/8], Step [676/750], Loss: 0.0147\n",
      "Epoch [6/8], Step [677/750], Loss: 0.0088\n",
      "Epoch [6/8], Step [678/750], Loss: 0.1186\n",
      "Epoch [6/8], Step [679/750], Loss: 0.0218\n",
      "Epoch [6/8], Step [680/750], Loss: 0.0052\n",
      "Epoch [6/8], Step [681/750], Loss: 0.0788\n",
      "Epoch [6/8], Step [682/750], Loss: 0.0401\n",
      "Epoch [6/8], Step [683/750], Loss: 0.0112\n",
      "Epoch [6/8], Step [684/750], Loss: 0.0629\n",
      "Epoch [6/8], Step [685/750], Loss: 0.0229\n",
      "Epoch [6/8], Step [686/750], Loss: 0.0778\n",
      "Epoch [6/8], Step [687/750], Loss: 0.0793\n",
      "Epoch [6/8], Step [688/750], Loss: 0.0028\n",
      "Epoch [6/8], Step [689/750], Loss: 0.0021\n",
      "Epoch [6/8], Step [690/750], Loss: 0.1209\n",
      "Epoch [6/8], Step [691/750], Loss: 0.0094\n",
      "Epoch [6/8], Step [692/750], Loss: 0.0417\n",
      "Epoch [6/8], Step [693/750], Loss: 0.0893\n",
      "Epoch [6/8], Step [694/750], Loss: 0.0217\n",
      "Epoch [6/8], Step [695/750], Loss: 0.0226\n",
      "Epoch [6/8], Step [696/750], Loss: 0.0139\n",
      "Epoch [6/8], Step [697/750], Loss: 0.1300\n",
      "Epoch [6/8], Step [698/750], Loss: 0.0125\n",
      "Epoch [6/8], Step [699/750], Loss: 0.0021\n",
      "Epoch [6/8], Step [700/750], Loss: 0.0144\n",
      "Epoch [6/8], Step [701/750], Loss: 0.0118\n",
      "Epoch [6/8], Step [702/750], Loss: 0.0626\n",
      "Epoch [6/8], Step [703/750], Loss: 0.0145\n",
      "Epoch [6/8], Step [704/750], Loss: 0.0050\n",
      "Epoch [6/8], Step [705/750], Loss: 0.0186\n",
      "Epoch [6/8], Step [706/750], Loss: 0.0686\n",
      "Epoch [6/8], Step [707/750], Loss: 0.0267\n",
      "Epoch [6/8], Step [708/750], Loss: 0.0483\n",
      "Epoch [6/8], Step [709/750], Loss: 0.0181\n",
      "Epoch [6/8], Step [710/750], Loss: 0.0462\n",
      "Epoch [6/8], Step [711/750], Loss: 0.0991\n",
      "Epoch [6/8], Step [712/750], Loss: 0.0083\n",
      "Epoch [6/8], Step [713/750], Loss: 0.1191\n",
      "Epoch [6/8], Step [714/750], Loss: 0.1429\n",
      "Epoch [6/8], Step [715/750], Loss: 0.0055\n",
      "Epoch [6/8], Step [716/750], Loss: 0.0228\n",
      "Epoch [6/8], Step [717/750], Loss: 0.0103\n",
      "Epoch [6/8], Step [718/750], Loss: 0.0140\n",
      "Epoch [6/8], Step [719/750], Loss: 0.0048\n",
      "Epoch [6/8], Step [720/750], Loss: 0.0033\n",
      "Epoch [6/8], Step [721/750], Loss: 0.0036\n",
      "Epoch [6/8], Step [722/750], Loss: 0.0056\n",
      "Epoch [6/8], Step [723/750], Loss: 0.0269\n",
      "Epoch [6/8], Step [724/750], Loss: 0.0909\n",
      "Epoch [6/8], Step [725/750], Loss: 0.0031\n",
      "Epoch [6/8], Step [726/750], Loss: 0.1561\n",
      "Epoch [6/8], Step [727/750], Loss: 0.0024\n",
      "Epoch [6/8], Step [728/750], Loss: 0.0718\n",
      "Epoch [6/8], Step [729/750], Loss: 0.0107\n",
      "Epoch [6/8], Step [730/750], Loss: 0.0035\n",
      "Epoch [6/8], Step [731/750], Loss: 0.0618\n",
      "Epoch [6/8], Step [732/750], Loss: 0.0087\n",
      "Epoch [6/8], Step [733/750], Loss: 0.0296\n",
      "Epoch [6/8], Step [734/750], Loss: 0.0671\n",
      "Epoch [6/8], Step [735/750], Loss: 0.0833\n",
      "Epoch [6/8], Step [736/750], Loss: 0.0071\n",
      "Epoch [6/8], Step [737/750], Loss: 0.0452\n",
      "Epoch [6/8], Step [738/750], Loss: 0.0478\n",
      "Epoch [6/8], Step [739/750], Loss: 0.0240\n",
      "Epoch [6/8], Step [740/750], Loss: 0.0223\n",
      "Epoch [6/8], Step [741/750], Loss: 0.0475\n",
      "Epoch [6/8], Step [742/750], Loss: 0.0248\n",
      "Epoch [6/8], Step [743/750], Loss: 0.0607\n",
      "Epoch [6/8], Step [744/750], Loss: 0.0458\n",
      "Epoch [6/8], Step [745/750], Loss: 0.0059\n",
      "Epoch [6/8], Step [746/750], Loss: 0.0172\n",
      "Epoch [6/8], Step [747/750], Loss: 0.0343\n",
      "Epoch [6/8], Step [748/750], Loss: 0.0041\n",
      "Epoch [6/8], Step [749/750], Loss: 0.0859\n",
      "Epoch [6/8], Step [750/750], Loss: 0.0028\n",
      "Epoch [6/8], Tr. loss: 0.3774. Test loss: 0.3103\n",
      "\n",
      "\n",
      "Epoch [7/8], Step [1/750], Loss: 0.0072\n",
      "Epoch [7/8], Step [2/750], Loss: 0.0306\n",
      "Epoch [7/8], Step [3/750], Loss: 0.0130\n",
      "Epoch [7/8], Step [4/750], Loss: 0.0487\n",
      "Epoch [7/8], Step [5/750], Loss: 0.0076\n",
      "Epoch [7/8], Step [6/750], Loss: 0.0025\n",
      "Epoch [7/8], Step [7/750], Loss: 0.0076\n",
      "Epoch [7/8], Step [8/750], Loss: 0.0085\n",
      "Epoch [7/8], Step [9/750], Loss: 0.0075\n",
      "Epoch [7/8], Step [10/750], Loss: 0.0404\n",
      "Epoch [7/8], Step [11/750], Loss: 0.0721\n",
      "Epoch [7/8], Step [12/750], Loss: 0.0910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Step [13/750], Loss: 0.0020\n",
      "Epoch [7/8], Step [14/750], Loss: 0.0144\n",
      "Epoch [7/8], Step [15/750], Loss: 0.0029\n",
      "Epoch [7/8], Step [16/750], Loss: 0.0847\n",
      "Epoch [7/8], Step [17/750], Loss: 0.0209\n",
      "Epoch [7/8], Step [18/750], Loss: 0.0206\n",
      "Epoch [7/8], Step [19/750], Loss: 0.0070\n",
      "Epoch [7/8], Step [20/750], Loss: 0.0011\n",
      "Epoch [7/8], Step [21/750], Loss: 0.0675\n",
      "Epoch [7/8], Step [22/750], Loss: 0.0013\n",
      "Epoch [7/8], Step [23/750], Loss: 0.0088\n",
      "Epoch [7/8], Step [24/750], Loss: 0.0045\n",
      "Epoch [7/8], Step [25/750], Loss: 0.0102\n",
      "Epoch [7/8], Step [26/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [27/750], Loss: 0.0067\n",
      "Epoch [7/8], Step [28/750], Loss: 0.0464\n",
      "Epoch [7/8], Step [29/750], Loss: 0.0394\n",
      "Epoch [7/8], Step [30/750], Loss: 0.0184\n",
      "Epoch [7/8], Step [31/750], Loss: 0.0258\n",
      "Epoch [7/8], Step [32/750], Loss: 0.0157\n",
      "Epoch [7/8], Step [33/750], Loss: 0.0030\n",
      "Epoch [7/8], Step [34/750], Loss: 0.0091\n",
      "Epoch [7/8], Step [35/750], Loss: 0.0068\n",
      "Epoch [7/8], Step [36/750], Loss: 0.0027\n",
      "Epoch [7/8], Step [37/750], Loss: 0.0088\n",
      "Epoch [7/8], Step [38/750], Loss: 0.0271\n",
      "Epoch [7/8], Step [39/750], Loss: 0.0057\n",
      "Epoch [7/8], Step [40/750], Loss: 0.0008\n",
      "Epoch [7/8], Step [41/750], Loss: 0.0065\n",
      "Epoch [7/8], Step [42/750], Loss: 0.0025\n",
      "Epoch [7/8], Step [43/750], Loss: 0.0036\n",
      "Epoch [7/8], Step [44/750], Loss: 0.0013\n",
      "Epoch [7/8], Step [45/750], Loss: 0.0122\n",
      "Epoch [7/8], Step [46/750], Loss: 0.0247\n",
      "Epoch [7/8], Step [47/750], Loss: 0.0548\n",
      "Epoch [7/8], Step [48/750], Loss: 0.0007\n",
      "Epoch [7/8], Step [49/750], Loss: 0.0023\n",
      "Epoch [7/8], Step [50/750], Loss: 0.0003\n",
      "Epoch [7/8], Step [51/750], Loss: 0.0009\n",
      "Epoch [7/8], Step [52/750], Loss: 0.0009\n",
      "Epoch [7/8], Step [53/750], Loss: 0.0019\n",
      "Epoch [7/8], Step [54/750], Loss: 0.0075\n",
      "Epoch [7/8], Step [55/750], Loss: 0.0580\n",
      "Epoch [7/8], Step [56/750], Loss: 0.0048\n",
      "Epoch [7/8], Step [57/750], Loss: 0.0011\n",
      "Epoch [7/8], Step [58/750], Loss: 0.0214\n",
      "Epoch [7/8], Step [59/750], Loss: 0.0315\n",
      "Epoch [7/8], Step [60/750], Loss: 0.0093\n",
      "Epoch [7/8], Step [61/750], Loss: 0.0072\n",
      "Epoch [7/8], Step [62/750], Loss: 0.0834\n",
      "Epoch [7/8], Step [63/750], Loss: 0.0014\n",
      "Epoch [7/8], Step [64/750], Loss: 0.0369\n",
      "Epoch [7/8], Step [65/750], Loss: 0.0605\n",
      "Epoch [7/8], Step [66/750], Loss: 0.0009\n",
      "Epoch [7/8], Step [67/750], Loss: 0.0319\n",
      "Epoch [7/8], Step [68/750], Loss: 0.0097\n",
      "Epoch [7/8], Step [69/750], Loss: 0.0061\n",
      "Epoch [7/8], Step [70/750], Loss: 0.0020\n",
      "Epoch [7/8], Step [71/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [72/750], Loss: 0.1080\n",
      "Epoch [7/8], Step [73/750], Loss: 0.1638\n",
      "Epoch [7/8], Step [74/750], Loss: 0.0035\n",
      "Epoch [7/8], Step [75/750], Loss: 0.0059\n",
      "Epoch [7/8], Step [76/750], Loss: 0.0982\n",
      "Epoch [7/8], Step [77/750], Loss: 0.0437\n",
      "Epoch [7/8], Step [78/750], Loss: 0.0007\n",
      "Epoch [7/8], Step [79/750], Loss: 0.0020\n",
      "Epoch [7/8], Step [80/750], Loss: 0.0036\n",
      "Epoch [7/8], Step [81/750], Loss: 0.0144\n",
      "Epoch [7/8], Step [82/750], Loss: 0.0374\n",
      "Epoch [7/8], Step [83/750], Loss: 0.0021\n",
      "Epoch [7/8], Step [84/750], Loss: 0.1136\n",
      "Epoch [7/8], Step [85/750], Loss: 0.0026\n",
      "Epoch [7/8], Step [86/750], Loss: 0.0194\n",
      "Epoch [7/8], Step [87/750], Loss: 0.0022\n",
      "Epoch [7/8], Step [88/750], Loss: 0.0024\n",
      "Epoch [7/8], Step [89/750], Loss: 0.0051\n",
      "Epoch [7/8], Step [90/750], Loss: 0.0135\n",
      "Epoch [7/8], Step [91/750], Loss: 0.0745\n",
      "Epoch [7/8], Step [92/750], Loss: 0.0124\n",
      "Epoch [7/8], Step [93/750], Loss: 0.0114\n",
      "Epoch [7/8], Step [94/750], Loss: 0.0338\n",
      "Epoch [7/8], Step [95/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [96/750], Loss: 0.0123\n",
      "Epoch [7/8], Step [97/750], Loss: 0.0351\n",
      "Epoch [7/8], Step [98/750], Loss: 0.0440\n",
      "Epoch [7/8], Step [99/750], Loss: 0.0100\n",
      "Epoch [7/8], Step [100/750], Loss: 0.0341\n",
      "Epoch [7/8], Step [101/750], Loss: 0.0041\n",
      "Epoch [7/8], Step [102/750], Loss: 0.0024\n",
      "Epoch [7/8], Step [103/750], Loss: 0.0021\n",
      "Epoch [7/8], Step [104/750], Loss: 0.0086\n",
      "Epoch [7/8], Step [105/750], Loss: 0.0250\n",
      "Epoch [7/8], Step [106/750], Loss: 0.0333\n",
      "Epoch [7/8], Step [107/750], Loss: 0.0202\n",
      "Epoch [7/8], Step [108/750], Loss: 0.0319\n",
      "Epoch [7/8], Step [109/750], Loss: 0.0221\n",
      "Epoch [7/8], Step [110/750], Loss: 0.0041\n",
      "Epoch [7/8], Step [111/750], Loss: 0.0136\n",
      "Epoch [7/8], Step [112/750], Loss: 0.0150\n",
      "Epoch [7/8], Step [113/750], Loss: 0.0618\n",
      "Epoch [7/8], Step [114/750], Loss: 0.0054\n",
      "Epoch [7/8], Step [115/750], Loss: 0.0192\n",
      "Epoch [7/8], Step [116/750], Loss: 0.0046\n",
      "Epoch [7/8], Step [117/750], Loss: 0.0307\n",
      "Epoch [7/8], Step [118/750], Loss: 0.0279\n",
      "Epoch [7/8], Step [119/750], Loss: 0.0186\n",
      "Epoch [7/8], Step [120/750], Loss: 0.0070\n",
      "Epoch [7/8], Step [121/750], Loss: 0.0458\n",
      "Epoch [7/8], Step [122/750], Loss: 0.0300\n",
      "Epoch [7/8], Step [123/750], Loss: 0.0085\n",
      "Epoch [7/8], Step [124/750], Loss: 0.0019\n",
      "Epoch [7/8], Step [125/750], Loss: 0.0084\n",
      "Epoch [7/8], Step [126/750], Loss: 0.0027\n",
      "Epoch [7/8], Step [127/750], Loss: 0.0024\n",
      "Epoch [7/8], Step [128/750], Loss: 0.0121\n",
      "Epoch [7/8], Step [129/750], Loss: 0.0308\n",
      "Epoch [7/8], Step [130/750], Loss: 0.0356\n",
      "Epoch [7/8], Step [131/750], Loss: 0.0319\n",
      "Epoch [7/8], Step [132/750], Loss: 0.0334\n",
      "Epoch [7/8], Step [133/750], Loss: 0.0023\n",
      "Epoch [7/8], Step [134/750], Loss: 0.0391\n",
      "Epoch [7/8], Step [135/750], Loss: 0.0127\n",
      "Epoch [7/8], Step [136/750], Loss: 0.0960\n",
      "Epoch [7/8], Step [137/750], Loss: 0.0036\n",
      "Epoch [7/8], Step [138/750], Loss: 0.0150\n",
      "Epoch [7/8], Step [139/750], Loss: 0.0116\n",
      "Epoch [7/8], Step [140/750], Loss: 0.0750\n",
      "Epoch [7/8], Step [141/750], Loss: 0.0277\n",
      "Epoch [7/8], Step [142/750], Loss: 0.0030\n",
      "Epoch [7/8], Step [143/750], Loss: 0.0197\n",
      "Epoch [7/8], Step [144/750], Loss: 0.2216\n",
      "Epoch [7/8], Step [145/750], Loss: 0.0078\n",
      "Epoch [7/8], Step [146/750], Loss: 0.0609\n",
      "Epoch [7/8], Step [147/750], Loss: 0.0074\n",
      "Epoch [7/8], Step [148/750], Loss: 0.0191\n",
      "Epoch [7/8], Step [149/750], Loss: 0.0038\n",
      "Epoch [7/8], Step [150/750], Loss: 0.0362\n",
      "Epoch [7/8], Step [151/750], Loss: 0.0034\n",
      "Epoch [7/8], Step [152/750], Loss: 0.0202\n",
      "Epoch [7/8], Step [153/750], Loss: 0.0071\n",
      "Epoch [7/8], Step [154/750], Loss: 0.0130\n",
      "Epoch [7/8], Step [155/750], Loss: 0.0105\n",
      "Epoch [7/8], Step [156/750], Loss: 0.0157\n",
      "Epoch [7/8], Step [157/750], Loss: 0.0058\n",
      "Epoch [7/8], Step [158/750], Loss: 0.0142\n",
      "Epoch [7/8], Step [159/750], Loss: 0.0525\n",
      "Epoch [7/8], Step [160/750], Loss: 0.0061\n",
      "Epoch [7/8], Step [161/750], Loss: 0.0013\n",
      "Epoch [7/8], Step [162/750], Loss: 0.0632\n",
      "Epoch [7/8], Step [163/750], Loss: 0.0094\n",
      "Epoch [7/8], Step [164/750], Loss: 0.0029\n",
      "Epoch [7/8], Step [165/750], Loss: 0.0151\n",
      "Epoch [7/8], Step [166/750], Loss: 0.0032\n",
      "Epoch [7/8], Step [167/750], Loss: 0.0072\n",
      "Epoch [7/8], Step [168/750], Loss: 0.0096\n",
      "Epoch [7/8], Step [169/750], Loss: 0.0110\n",
      "Epoch [7/8], Step [170/750], Loss: 0.0035\n",
      "Epoch [7/8], Step [171/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [172/750], Loss: 0.0110\n",
      "Epoch [7/8], Step [173/750], Loss: 0.0096\n",
      "Epoch [7/8], Step [174/750], Loss: 0.0111\n",
      "Epoch [7/8], Step [175/750], Loss: 0.0024\n",
      "Epoch [7/8], Step [176/750], Loss: 0.0052\n",
      "Epoch [7/8], Step [177/750], Loss: 0.0284\n",
      "Epoch [7/8], Step [178/750], Loss: 0.0277\n",
      "Epoch [7/8], Step [179/750], Loss: 0.0145\n",
      "Epoch [7/8], Step [180/750], Loss: 0.0017\n",
      "Epoch [7/8], Step [181/750], Loss: 0.0066\n",
      "Epoch [7/8], Step [182/750], Loss: 0.0583\n",
      "Epoch [7/8], Step [183/750], Loss: 0.0106\n",
      "Epoch [7/8], Step [184/750], Loss: 0.0012\n",
      "Epoch [7/8], Step [185/750], Loss: 0.0017\n",
      "Epoch [7/8], Step [186/750], Loss: 0.0503\n",
      "Epoch [7/8], Step [187/750], Loss: 0.0518\n",
      "Epoch [7/8], Step [188/750], Loss: 0.0650\n",
      "Epoch [7/8], Step [189/750], Loss: 0.0148\n",
      "Epoch [7/8], Step [190/750], Loss: 0.1476\n",
      "Epoch [7/8], Step [191/750], Loss: 0.0106\n",
      "Epoch [7/8], Step [192/750], Loss: 0.0039\n",
      "Epoch [7/8], Step [193/750], Loss: 0.0025\n",
      "Epoch [7/8], Step [194/750], Loss: 0.0029\n",
      "Epoch [7/8], Step [195/750], Loss: 0.0012\n",
      "Epoch [7/8], Step [196/750], Loss: 0.0291\n",
      "Epoch [7/8], Step [197/750], Loss: 0.0242\n",
      "Epoch [7/8], Step [198/750], Loss: 0.0038\n",
      "Epoch [7/8], Step [199/750], Loss: 0.0030\n",
      "Epoch [7/8], Step [200/750], Loss: 0.0285\n",
      "Epoch [7/8], Step [201/750], Loss: 0.0056\n",
      "Epoch [7/8], Step [202/750], Loss: 0.1409\n",
      "Epoch [7/8], Step [203/750], Loss: 0.0372\n",
      "Epoch [7/8], Step [204/750], Loss: 0.0044\n",
      "Epoch [7/8], Step [205/750], Loss: 0.0492\n",
      "Epoch [7/8], Step [206/750], Loss: 0.0004\n",
      "Epoch [7/8], Step [207/750], Loss: 0.0064\n",
      "Epoch [7/8], Step [208/750], Loss: 0.0202\n",
      "Epoch [7/8], Step [209/750], Loss: 0.0138\n",
      "Epoch [7/8], Step [210/750], Loss: 0.0009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Step [211/750], Loss: 0.0285\n",
      "Epoch [7/8], Step [212/750], Loss: 0.0054\n",
      "Epoch [7/8], Step [213/750], Loss: 0.0279\n",
      "Epoch [7/8], Step [214/750], Loss: 0.0181\n",
      "Epoch [7/8], Step [215/750], Loss: 0.0335\n",
      "Epoch [7/8], Step [216/750], Loss: 0.0236\n",
      "Epoch [7/8], Step [217/750], Loss: 0.0731\n",
      "Epoch [7/8], Step [218/750], Loss: 0.1162\n",
      "Epoch [7/8], Step [219/750], Loss: 0.0527\n",
      "Epoch [7/8], Step [220/750], Loss: 0.0299\n",
      "Epoch [7/8], Step [221/750], Loss: 0.0039\n",
      "Epoch [7/8], Step [222/750], Loss: 0.0050\n",
      "Epoch [7/8], Step [223/750], Loss: 0.0094\n",
      "Epoch [7/8], Step [224/750], Loss: 0.0143\n",
      "Epoch [7/8], Step [225/750], Loss: 0.0372\n",
      "Epoch [7/8], Step [226/750], Loss: 0.0307\n",
      "Epoch [7/8], Step [227/750], Loss: 0.0545\n",
      "Epoch [7/8], Step [228/750], Loss: 0.0677\n",
      "Epoch [7/8], Step [229/750], Loss: 0.0637\n",
      "Epoch [7/8], Step [230/750], Loss: 0.0053\n",
      "Epoch [7/8], Step [231/750], Loss: 0.0020\n",
      "Epoch [7/8], Step [232/750], Loss: 0.0257\n",
      "Epoch [7/8], Step [233/750], Loss: 0.0059\n",
      "Epoch [7/8], Step [234/750], Loss: 0.0249\n",
      "Epoch [7/8], Step [235/750], Loss: 0.0117\n",
      "Epoch [7/8], Step [236/750], Loss: 0.0015\n",
      "Epoch [7/8], Step [237/750], Loss: 0.0065\n",
      "Epoch [7/8], Step [238/750], Loss: 0.0166\n",
      "Epoch [7/8], Step [239/750], Loss: 0.0054\n",
      "Epoch [7/8], Step [240/750], Loss: 0.0045\n",
      "Epoch [7/8], Step [241/750], Loss: 0.0357\n",
      "Epoch [7/8], Step [242/750], Loss: 0.0226\n",
      "Epoch [7/8], Step [243/750], Loss: 0.0061\n",
      "Epoch [7/8], Step [244/750], Loss: 0.0072\n",
      "Epoch [7/8], Step [245/750], Loss: 0.0414\n",
      "Epoch [7/8], Step [246/750], Loss: 0.0019\n",
      "Epoch [7/8], Step [247/750], Loss: 0.0442\n",
      "Epoch [7/8], Step [248/750], Loss: 0.0069\n",
      "Epoch [7/8], Step [249/750], Loss: 0.0071\n",
      "Epoch [7/8], Step [250/750], Loss: 0.0763\n",
      "Epoch [7/8], Step [251/750], Loss: 0.0182\n",
      "Epoch [7/8], Step [252/750], Loss: 0.0045\n",
      "Epoch [7/8], Step [253/750], Loss: 0.0353\n",
      "Epoch [7/8], Step [254/750], Loss: 0.0047\n",
      "Epoch [7/8], Step [255/750], Loss: 0.0205\n",
      "Epoch [7/8], Step [256/750], Loss: 0.0380\n",
      "Epoch [7/8], Step [257/750], Loss: 0.0141\n",
      "Epoch [7/8], Step [258/750], Loss: 0.0144\n",
      "Epoch [7/8], Step [259/750], Loss: 0.0129\n",
      "Epoch [7/8], Step [260/750], Loss: 0.0064\n",
      "Epoch [7/8], Step [261/750], Loss: 0.0086\n",
      "Epoch [7/8], Step [262/750], Loss: 0.0604\n",
      "Epoch [7/8], Step [263/750], Loss: 0.0274\n",
      "Epoch [7/8], Step [264/750], Loss: 0.0191\n",
      "Epoch [7/8], Step [265/750], Loss: 0.0358\n",
      "Epoch [7/8], Step [266/750], Loss: 0.0257\n",
      "Epoch [7/8], Step [267/750], Loss: 0.0279\n",
      "Epoch [7/8], Step [268/750], Loss: 0.0275\n",
      "Epoch [7/8], Step [269/750], Loss: 0.0057\n",
      "Epoch [7/8], Step [270/750], Loss: 0.0789\n",
      "Epoch [7/8], Step [271/750], Loss: 0.0024\n",
      "Epoch [7/8], Step [272/750], Loss: 0.0073\n",
      "Epoch [7/8], Step [273/750], Loss: 0.0226\n",
      "Epoch [7/8], Step [274/750], Loss: 0.0574\n",
      "Epoch [7/8], Step [275/750], Loss: 0.0977\n",
      "Epoch [7/8], Step [276/750], Loss: 0.0184\n",
      "Epoch [7/8], Step [277/750], Loss: 0.0056\n",
      "Epoch [7/8], Step [278/750], Loss: 0.0042\n",
      "Epoch [7/8], Step [279/750], Loss: 0.0153\n",
      "Epoch [7/8], Step [280/750], Loss: 0.0056\n",
      "Epoch [7/8], Step [281/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [282/750], Loss: 0.2083\n",
      "Epoch [7/8], Step [283/750], Loss: 0.0012\n",
      "Epoch [7/8], Step [284/750], Loss: 0.0050\n",
      "Epoch [7/8], Step [285/750], Loss: 0.0180\n",
      "Epoch [7/8], Step [286/750], Loss: 0.0021\n",
      "Epoch [7/8], Step [287/750], Loss: 0.0587\n",
      "Epoch [7/8], Step [288/750], Loss: 0.0054\n",
      "Epoch [7/8], Step [289/750], Loss: 0.0808\n",
      "Epoch [7/8], Step [290/750], Loss: 0.0042\n",
      "Epoch [7/8], Step [291/750], Loss: 0.0034\n",
      "Epoch [7/8], Step [292/750], Loss: 0.0162\n",
      "Epoch [7/8], Step [293/750], Loss: 0.0028\n",
      "Epoch [7/8], Step [294/750], Loss: 0.0051\n",
      "Epoch [7/8], Step [295/750], Loss: 0.0397\n",
      "Epoch [7/8], Step [296/750], Loss: 0.0193\n",
      "Epoch [7/8], Step [297/750], Loss: 0.0079\n",
      "Epoch [7/8], Step [298/750], Loss: 0.0268\n",
      "Epoch [7/8], Step [299/750], Loss: 0.0068\n",
      "Epoch [7/8], Step [300/750], Loss: 0.0042\n",
      "Epoch [7/8], Step [301/750], Loss: 0.0438\n",
      "Epoch [7/8], Step [302/750], Loss: 0.0107\n",
      "Epoch [7/8], Step [303/750], Loss: 0.1017\n",
      "Epoch [7/8], Step [304/750], Loss: 0.0292\n",
      "Epoch [7/8], Step [305/750], Loss: 0.0019\n",
      "Epoch [7/8], Step [306/750], Loss: 0.1030\n",
      "Epoch [7/8], Step [307/750], Loss: 0.0017\n",
      "Epoch [7/8], Step [308/750], Loss: 0.0009\n",
      "Epoch [7/8], Step [309/750], Loss: 0.0635\n",
      "Epoch [7/8], Step [310/750], Loss: 0.0014\n",
      "Epoch [7/8], Step [311/750], Loss: 0.0019\n",
      "Epoch [7/8], Step [312/750], Loss: 0.0706\n",
      "Epoch [7/8], Step [313/750], Loss: 0.0082\n",
      "Epoch [7/8], Step [314/750], Loss: 0.0757\n",
      "Epoch [7/8], Step [315/750], Loss: 0.0333\n",
      "Epoch [7/8], Step [316/750], Loss: 0.0078\n",
      "Epoch [7/8], Step [317/750], Loss: 0.0056\n",
      "Epoch [7/8], Step [318/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [319/750], Loss: 0.0108\n",
      "Epoch [7/8], Step [320/750], Loss: 0.0137\n",
      "Epoch [7/8], Step [321/750], Loss: 0.0776\n",
      "Epoch [7/8], Step [322/750], Loss: 0.0037\n",
      "Epoch [7/8], Step [323/750], Loss: 0.0059\n",
      "Epoch [7/8], Step [324/750], Loss: 0.0105\n",
      "Epoch [7/8], Step [325/750], Loss: 0.0727\n",
      "Epoch [7/8], Step [326/750], Loss: 0.0103\n",
      "Epoch [7/8], Step [327/750], Loss: 0.0007\n",
      "Epoch [7/8], Step [328/750], Loss: 0.0340\n",
      "Epoch [7/8], Step [329/750], Loss: 0.0071\n",
      "Epoch [7/8], Step [330/750], Loss: 0.0039\n",
      "Epoch [7/8], Step [331/750], Loss: 0.0629\n",
      "Epoch [7/8], Step [332/750], Loss: 0.0025\n",
      "Epoch [7/8], Step [333/750], Loss: 0.0056\n",
      "Epoch [7/8], Step [334/750], Loss: 0.0230\n",
      "Epoch [7/8], Step [335/750], Loss: 0.0331\n",
      "Epoch [7/8], Step [336/750], Loss: 0.0779\n",
      "Epoch [7/8], Step [337/750], Loss: 0.0034\n",
      "Epoch [7/8], Step [338/750], Loss: 0.0721\n",
      "Epoch [7/8], Step [339/750], Loss: 0.1069\n",
      "Epoch [7/8], Step [340/750], Loss: 0.0224\n",
      "Epoch [7/8], Step [341/750], Loss: 0.0250\n",
      "Epoch [7/8], Step [342/750], Loss: 0.0596\n",
      "Epoch [7/8], Step [343/750], Loss: 0.0099\n",
      "Epoch [7/8], Step [344/750], Loss: 0.0060\n",
      "Epoch [7/8], Step [345/750], Loss: 0.0293\n",
      "Epoch [7/8], Step [346/750], Loss: 0.0116\n",
      "Epoch [7/8], Step [347/750], Loss: 0.1071\n",
      "Epoch [7/8], Step [348/750], Loss: 0.0638\n",
      "Epoch [7/8], Step [349/750], Loss: 0.0085\n",
      "Epoch [7/8], Step [350/750], Loss: 0.0264\n",
      "Epoch [7/8], Step [351/750], Loss: 0.0022\n",
      "Epoch [7/8], Step [352/750], Loss: 0.0369\n",
      "Epoch [7/8], Step [353/750], Loss: 0.0026\n",
      "Epoch [7/8], Step [354/750], Loss: 0.0200\n",
      "Epoch [7/8], Step [355/750], Loss: 0.0776\n",
      "Epoch [7/8], Step [356/750], Loss: 0.0220\n",
      "Epoch [7/8], Step [357/750], Loss: 0.0761\n",
      "Epoch [7/8], Step [358/750], Loss: 0.0037\n",
      "Epoch [7/8], Step [359/750], Loss: 0.0043\n",
      "Epoch [7/8], Step [360/750], Loss: 0.0154\n",
      "Epoch [7/8], Step [361/750], Loss: 0.0292\n",
      "Epoch [7/8], Step [362/750], Loss: 0.0078\n",
      "Epoch [7/8], Step [363/750], Loss: 0.0156\n",
      "Epoch [7/8], Step [364/750], Loss: 0.0048\n",
      "Epoch [7/8], Step [365/750], Loss: 0.0152\n",
      "Epoch [7/8], Step [366/750], Loss: 0.0658\n",
      "Epoch [7/8], Step [367/750], Loss: 0.0036\n",
      "Epoch [7/8], Step [368/750], Loss: 0.0704\n",
      "Epoch [7/8], Step [369/750], Loss: 0.0143\n",
      "Epoch [7/8], Step [370/750], Loss: 0.0864\n",
      "Epoch [7/8], Step [371/750], Loss: 0.0078\n",
      "Epoch [7/8], Step [372/750], Loss: 0.0016\n",
      "Epoch [7/8], Step [373/750], Loss: 0.0649\n",
      "Epoch [7/8], Step [374/750], Loss: 0.0272\n",
      "Epoch [7/8], Step [375/750], Loss: 0.0026\n",
      "Epoch [7/8], Step [376/750], Loss: 0.0311\n",
      "Epoch [7/8], Step [377/750], Loss: 0.0034\n",
      "Epoch [7/8], Step [378/750], Loss: 0.0078\n",
      "Epoch [7/8], Step [379/750], Loss: 0.0663\n",
      "Epoch [7/8], Step [380/750], Loss: 0.0049\n",
      "Epoch [7/8], Step [381/750], Loss: 0.0209\n",
      "Epoch [7/8], Step [382/750], Loss: 0.0664\n",
      "Epoch [7/8], Step [383/750], Loss: 0.0030\n",
      "Epoch [7/8], Step [384/750], Loss: 0.0219\n",
      "Epoch [7/8], Step [385/750], Loss: 0.0110\n",
      "Epoch [7/8], Step [386/750], Loss: 0.0024\n",
      "Epoch [7/8], Step [387/750], Loss: 0.0091\n",
      "Epoch [7/8], Step [388/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [389/750], Loss: 0.0155\n",
      "Epoch [7/8], Step [390/750], Loss: 0.0114\n",
      "Epoch [7/8], Step [391/750], Loss: 0.0092\n",
      "Epoch [7/8], Step [392/750], Loss: 0.0217\n",
      "Epoch [7/8], Step [393/750], Loss: 0.0027\n",
      "Epoch [7/8], Step [394/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [395/750], Loss: 0.0095\n",
      "Epoch [7/8], Step [396/750], Loss: 0.0149\n",
      "Epoch [7/8], Step [397/750], Loss: 0.0371\n",
      "Epoch [7/8], Step [398/750], Loss: 0.1117\n",
      "Epoch [7/8], Step [399/750], Loss: 0.0068\n",
      "Epoch [7/8], Step [400/750], Loss: 0.0172\n",
      "Epoch [7/8], Step [401/750], Loss: 0.1173\n",
      "Epoch [7/8], Step [402/750], Loss: 0.0386\n",
      "Epoch [7/8], Step [403/750], Loss: 0.0144\n",
      "Epoch [7/8], Step [404/750], Loss: 0.0063\n",
      "Epoch [7/8], Step [405/750], Loss: 0.0085\n",
      "Epoch [7/8], Step [406/750], Loss: 0.0290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Step [407/750], Loss: 0.0203\n",
      "Epoch [7/8], Step [408/750], Loss: 0.0411\n",
      "Epoch [7/8], Step [409/750], Loss: 0.0148\n",
      "Epoch [7/8], Step [410/750], Loss: 0.0027\n",
      "Epoch [7/8], Step [411/750], Loss: 0.0374\n",
      "Epoch [7/8], Step [412/750], Loss: 0.0043\n",
      "Epoch [7/8], Step [413/750], Loss: 0.0019\n",
      "Epoch [7/8], Step [414/750], Loss: 0.0049\n",
      "Epoch [7/8], Step [415/750], Loss: 0.0944\n",
      "Epoch [7/8], Step [416/750], Loss: 0.0048\n",
      "Epoch [7/8], Step [417/750], Loss: 0.0031\n",
      "Epoch [7/8], Step [418/750], Loss: 0.0022\n",
      "Epoch [7/8], Step [419/750], Loss: 0.0810\n",
      "Epoch [7/8], Step [420/750], Loss: 0.0463\n",
      "Epoch [7/8], Step [421/750], Loss: 0.0027\n",
      "Epoch [7/8], Step [422/750], Loss: 0.0064\n",
      "Epoch [7/8], Step [423/750], Loss: 0.0030\n",
      "Epoch [7/8], Step [424/750], Loss: 0.0560\n",
      "Epoch [7/8], Step [425/750], Loss: 0.0326\n",
      "Epoch [7/8], Step [426/750], Loss: 0.1391\n",
      "Epoch [7/8], Step [427/750], Loss: 0.0204\n",
      "Epoch [7/8], Step [428/750], Loss: 0.0107\n",
      "Epoch [7/8], Step [429/750], Loss: 0.0063\n",
      "Epoch [7/8], Step [430/750], Loss: 0.0172\n",
      "Epoch [7/8], Step [431/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [432/750], Loss: 0.0133\n",
      "Epoch [7/8], Step [433/750], Loss: 0.0306\n",
      "Epoch [7/8], Step [434/750], Loss: 0.0414\n",
      "Epoch [7/8], Step [435/750], Loss: 0.0117\n",
      "Epoch [7/8], Step [436/750], Loss: 0.0831\n",
      "Epoch [7/8], Step [437/750], Loss: 0.0124\n",
      "Epoch [7/8], Step [438/750], Loss: 0.0198\n",
      "Epoch [7/8], Step [439/750], Loss: 0.0155\n",
      "Epoch [7/8], Step [440/750], Loss: 0.0119\n",
      "Epoch [7/8], Step [441/750], Loss: 0.0482\n",
      "Epoch [7/8], Step [442/750], Loss: 0.0106\n",
      "Epoch [7/8], Step [443/750], Loss: 0.0076\n",
      "Epoch [7/8], Step [444/750], Loss: 0.0652\n",
      "Epoch [7/8], Step [445/750], Loss: 0.0323\n",
      "Epoch [7/8], Step [446/750], Loss: 0.1426\n",
      "Epoch [7/8], Step [447/750], Loss: 0.0914\n",
      "Epoch [7/8], Step [448/750], Loss: 0.0106\n",
      "Epoch [7/8], Step [449/750], Loss: 0.0070\n",
      "Epoch [7/8], Step [450/750], Loss: 0.0117\n",
      "Epoch [7/8], Step [451/750], Loss: 0.0182\n",
      "Epoch [7/8], Step [452/750], Loss: 0.0066\n",
      "Epoch [7/8], Step [453/750], Loss: 0.0012\n",
      "Epoch [7/8], Step [454/750], Loss: 0.0745\n",
      "Epoch [7/8], Step [455/750], Loss: 0.0418\n",
      "Epoch [7/8], Step [456/750], Loss: 0.0011\n",
      "Epoch [7/8], Step [457/750], Loss: 0.0045\n",
      "Epoch [7/8], Step [458/750], Loss: 0.0083\n",
      "Epoch [7/8], Step [459/750], Loss: 0.0206\n",
      "Epoch [7/8], Step [460/750], Loss: 0.0079\n",
      "Epoch [7/8], Step [461/750], Loss: 0.0045\n",
      "Epoch [7/8], Step [462/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [463/750], Loss: 0.0012\n",
      "Epoch [7/8], Step [464/750], Loss: 0.0376\n",
      "Epoch [7/8], Step [465/750], Loss: 0.0449\n",
      "Epoch [7/8], Step [466/750], Loss: 0.0129\n",
      "Epoch [7/8], Step [467/750], Loss: 0.0051\n",
      "Epoch [7/8], Step [468/750], Loss: 0.1214\n",
      "Epoch [7/8], Step [469/750], Loss: 0.0053\n",
      "Epoch [7/8], Step [470/750], Loss: 0.0833\n",
      "Epoch [7/8], Step [471/750], Loss: 0.0011\n",
      "Epoch [7/8], Step [472/750], Loss: 0.0039\n",
      "Epoch [7/8], Step [473/750], Loss: 0.0768\n",
      "Epoch [7/8], Step [474/750], Loss: 0.0320\n",
      "Epoch [7/8], Step [475/750], Loss: 0.0210\n",
      "Epoch [7/8], Step [476/750], Loss: 0.0023\n",
      "Epoch [7/8], Step [477/750], Loss: 0.0410\n",
      "Epoch [7/8], Step [478/750], Loss: 0.0658\n",
      "Epoch [7/8], Step [479/750], Loss: 0.0061\n",
      "Epoch [7/8], Step [480/750], Loss: 0.0233\n",
      "Epoch [7/8], Step [481/750], Loss: 0.0591\n",
      "Epoch [7/8], Step [482/750], Loss: 0.0085\n",
      "Epoch [7/8], Step [483/750], Loss: 0.0066\n",
      "Epoch [7/8], Step [484/750], Loss: 0.0203\n",
      "Epoch [7/8], Step [485/750], Loss: 0.0051\n",
      "Epoch [7/8], Step [486/750], Loss: 0.0717\n",
      "Epoch [7/8], Step [487/750], Loss: 0.1164\n",
      "Epoch [7/8], Step [488/750], Loss: 0.0054\n",
      "Epoch [7/8], Step [489/750], Loss: 0.0022\n",
      "Epoch [7/8], Step [490/750], Loss: 0.0135\n",
      "Epoch [7/8], Step [491/750], Loss: 0.0337\n",
      "Epoch [7/8], Step [492/750], Loss: 0.0052\n",
      "Epoch [7/8], Step [493/750], Loss: 0.0077\n",
      "Epoch [7/8], Step [494/750], Loss: 0.0092\n",
      "Epoch [7/8], Step [495/750], Loss: 0.0033\n",
      "Epoch [7/8], Step [496/750], Loss: 0.0130\n",
      "Epoch [7/8], Step [497/750], Loss: 0.0035\n",
      "Epoch [7/8], Step [498/750], Loss: 0.0195\n",
      "Epoch [7/8], Step [499/750], Loss: 0.0095\n",
      "Epoch [7/8], Step [500/750], Loss: 0.0347\n",
      "Epoch [7/8], Step [501/750], Loss: 0.0283\n",
      "Epoch [7/8], Step [502/750], Loss: 0.0082\n",
      "Epoch [7/8], Step [503/750], Loss: 0.0274\n",
      "Epoch [7/8], Step [504/750], Loss: 0.0301\n",
      "Epoch [7/8], Step [505/750], Loss: 0.0085\n",
      "Epoch [7/8], Step [506/750], Loss: 0.1373\n",
      "Epoch [7/8], Step [507/750], Loss: 0.0041\n",
      "Epoch [7/8], Step [508/750], Loss: 0.0348\n",
      "Epoch [7/8], Step [509/750], Loss: 0.0022\n",
      "Epoch [7/8], Step [510/750], Loss: 0.0156\n",
      "Epoch [7/8], Step [511/750], Loss: 0.0098\n",
      "Epoch [7/8], Step [512/750], Loss: 0.0027\n",
      "Epoch [7/8], Step [513/750], Loss: 0.1101\n",
      "Epoch [7/8], Step [514/750], Loss: 0.0018\n",
      "Epoch [7/8], Step [515/750], Loss: 0.0193\n",
      "Epoch [7/8], Step [516/750], Loss: 0.0034\n",
      "Epoch [7/8], Step [517/750], Loss: 0.0113\n",
      "Epoch [7/8], Step [518/750], Loss: 0.0468\n",
      "Epoch [7/8], Step [519/750], Loss: 0.0183\n",
      "Epoch [7/8], Step [520/750], Loss: 0.0080\n",
      "Epoch [7/8], Step [521/750], Loss: 0.0626\n",
      "Epoch [7/8], Step [522/750], Loss: 0.0081\n",
      "Epoch [7/8], Step [523/750], Loss: 0.0228\n",
      "Epoch [7/8], Step [524/750], Loss: 0.0450\n",
      "Epoch [7/8], Step [525/750], Loss: 0.0200\n",
      "Epoch [7/8], Step [526/750], Loss: 0.0437\n",
      "Epoch [7/8], Step [527/750], Loss: 0.0304\n",
      "Epoch [7/8], Step [528/750], Loss: 0.0015\n",
      "Epoch [7/8], Step [529/750], Loss: 0.0077\n",
      "Epoch [7/8], Step [530/750], Loss: 0.0113\n",
      "Epoch [7/8], Step [531/750], Loss: 0.0266\n",
      "Epoch [7/8], Step [532/750], Loss: 0.0172\n",
      "Epoch [7/8], Step [533/750], Loss: 0.0147\n",
      "Epoch [7/8], Step [534/750], Loss: 0.0064\n",
      "Epoch [7/8], Step [535/750], Loss: 0.0204\n",
      "Epoch [7/8], Step [536/750], Loss: 0.0323\n",
      "Epoch [7/8], Step [537/750], Loss: 0.0043\n",
      "Epoch [7/8], Step [538/750], Loss: 0.0060\n",
      "Epoch [7/8], Step [539/750], Loss: 0.0489\n",
      "Epoch [7/8], Step [540/750], Loss: 0.0070\n",
      "Epoch [7/8], Step [541/750], Loss: 0.0090\n",
      "Epoch [7/8], Step [542/750], Loss: 0.0035\n",
      "Epoch [7/8], Step [543/750], Loss: 0.0260\n",
      "Epoch [7/8], Step [544/750], Loss: 0.1020\n",
      "Epoch [7/8], Step [545/750], Loss: 0.0011\n",
      "Epoch [7/8], Step [546/750], Loss: 0.1254\n",
      "Epoch [7/8], Step [547/750], Loss: 0.0012\n",
      "Epoch [7/8], Step [548/750], Loss: 0.0173\n",
      "Epoch [7/8], Step [549/750], Loss: 0.0433\n",
      "Epoch [7/8], Step [550/750], Loss: 0.1017\n",
      "Epoch [7/8], Step [551/750], Loss: 0.0129\n",
      "Epoch [7/8], Step [552/750], Loss: 0.0066\n",
      "Epoch [7/8], Step [553/750], Loss: 0.0328\n",
      "Epoch [7/8], Step [554/750], Loss: 0.0306\n",
      "Epoch [7/8], Step [555/750], Loss: 0.1263\n",
      "Epoch [7/8], Step [556/750], Loss: 0.0336\n",
      "Epoch [7/8], Step [557/750], Loss: 0.0094\n",
      "Epoch [7/8], Step [558/750], Loss: 0.0032\n",
      "Epoch [7/8], Step [559/750], Loss: 0.0050\n",
      "Epoch [7/8], Step [560/750], Loss: 0.0494\n",
      "Epoch [7/8], Step [561/750], Loss: 0.0379\n",
      "Epoch [7/8], Step [562/750], Loss: 0.0295\n",
      "Epoch [7/8], Step [563/750], Loss: 0.0153\n",
      "Epoch [7/8], Step [564/750], Loss: 0.0075\n",
      "Epoch [7/8], Step [565/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [566/750], Loss: 0.0164\n",
      "Epoch [7/8], Step [567/750], Loss: 0.0017\n",
      "Epoch [7/8], Step [568/750], Loss: 0.0314\n",
      "Epoch [7/8], Step [569/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [570/750], Loss: 0.0031\n",
      "Epoch [7/8], Step [571/750], Loss: 0.0045\n",
      "Epoch [7/8], Step [572/750], Loss: 0.0077\n",
      "Epoch [7/8], Step [573/750], Loss: 0.0333\n",
      "Epoch [7/8], Step [574/750], Loss: 0.0057\n",
      "Epoch [7/8], Step [575/750], Loss: 0.0135\n",
      "Epoch [7/8], Step [576/750], Loss: 0.0280\n",
      "Epoch [7/8], Step [577/750], Loss: 0.0073\n",
      "Epoch [7/8], Step [578/750], Loss: 0.0032\n",
      "Epoch [7/8], Step [579/750], Loss: 0.0369\n",
      "Epoch [7/8], Step [580/750], Loss: 0.0034\n",
      "Epoch [7/8], Step [581/750], Loss: 0.0662\n",
      "Epoch [7/8], Step [582/750], Loss: 0.0107\n",
      "Epoch [7/8], Step [583/750], Loss: 0.0546\n",
      "Epoch [7/8], Step [584/750], Loss: 0.0042\n",
      "Epoch [7/8], Step [585/750], Loss: 0.0938\n",
      "Epoch [7/8], Step [586/750], Loss: 0.0099\n",
      "Epoch [7/8], Step [587/750], Loss: 0.0027\n",
      "Epoch [7/8], Step [588/750], Loss: 0.0317\n",
      "Epoch [7/8], Step [589/750], Loss: 0.0125\n",
      "Epoch [7/8], Step [590/750], Loss: 0.0142\n",
      "Epoch [7/8], Step [591/750], Loss: 0.0718\n",
      "Epoch [7/8], Step [592/750], Loss: 0.0609\n",
      "Epoch [7/8], Step [593/750], Loss: 0.0454\n",
      "Epoch [7/8], Step [594/750], Loss: 0.0026\n",
      "Epoch [7/8], Step [595/750], Loss: 0.0139\n",
      "Epoch [7/8], Step [596/750], Loss: 0.0035\n",
      "Epoch [7/8], Step [597/750], Loss: 0.0092\n",
      "Epoch [7/8], Step [598/750], Loss: 0.0499\n",
      "Epoch [7/8], Step [599/750], Loss: 0.0096\n",
      "Epoch [7/8], Step [600/750], Loss: 0.0241\n",
      "Epoch [7/8], Step [601/750], Loss: 0.0114\n",
      "Epoch [7/8], Step [602/750], Loss: 0.0451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Step [603/750], Loss: 0.0139\n",
      "Epoch [7/8], Step [604/750], Loss: 0.1106\n",
      "Epoch [7/8], Step [605/750], Loss: 0.0532\n",
      "Epoch [7/8], Step [606/750], Loss: 0.0115\n",
      "Epoch [7/8], Step [607/750], Loss: 0.0068\n",
      "Epoch [7/8], Step [608/750], Loss: 0.0077\n",
      "Epoch [7/8], Step [609/750], Loss: 0.0148\n",
      "Epoch [7/8], Step [610/750], Loss: 0.0099\n",
      "Epoch [7/8], Step [611/750], Loss: 0.0062\n",
      "Epoch [7/8], Step [612/750], Loss: 0.0063\n",
      "Epoch [7/8], Step [613/750], Loss: 0.1100\n",
      "Epoch [7/8], Step [614/750], Loss: 0.0429\n",
      "Epoch [7/8], Step [615/750], Loss: 0.0013\n",
      "Epoch [7/8], Step [616/750], Loss: 0.0379\n",
      "Epoch [7/8], Step [617/750], Loss: 0.0096\n",
      "Epoch [7/8], Step [618/750], Loss: 0.0162\n",
      "Epoch [7/8], Step [619/750], Loss: 0.0052\n",
      "Epoch [7/8], Step [620/750], Loss: 0.1567\n",
      "Epoch [7/8], Step [621/750], Loss: 0.0752\n",
      "Epoch [7/8], Step [622/750], Loss: 0.0113\n",
      "Epoch [7/8], Step [623/750], Loss: 0.0468\n",
      "Epoch [7/8], Step [624/750], Loss: 0.0050\n",
      "Epoch [7/8], Step [625/750], Loss: 0.0121\n",
      "Epoch [7/8], Step [626/750], Loss: 0.0043\n",
      "Epoch [7/8], Step [627/750], Loss: 0.0471\n",
      "Epoch [7/8], Step [628/750], Loss: 0.0216\n",
      "Epoch [7/8], Step [629/750], Loss: 0.0126\n",
      "Epoch [7/8], Step [630/750], Loss: 0.0277\n",
      "Epoch [7/8], Step [631/750], Loss: 0.0076\n",
      "Epoch [7/8], Step [632/750], Loss: 0.0138\n",
      "Epoch [7/8], Step [633/750], Loss: 0.0883\n",
      "Epoch [7/8], Step [634/750], Loss: 0.0174\n",
      "Epoch [7/8], Step [635/750], Loss: 0.0049\n",
      "Epoch [7/8], Step [636/750], Loss: 0.0056\n",
      "Epoch [7/8], Step [637/750], Loss: 0.0072\n",
      "Epoch [7/8], Step [638/750], Loss: 0.0023\n",
      "Epoch [7/8], Step [639/750], Loss: 0.0537\n",
      "Epoch [7/8], Step [640/750], Loss: 0.0205\n",
      "Epoch [7/8], Step [641/750], Loss: 0.0155\n",
      "Epoch [7/8], Step [642/750], Loss: 0.0184\n",
      "Epoch [7/8], Step [643/750], Loss: 0.0015\n",
      "Epoch [7/8], Step [644/750], Loss: 0.0017\n",
      "Epoch [7/8], Step [645/750], Loss: 0.0010\n",
      "Epoch [7/8], Step [646/750], Loss: 0.0035\n",
      "Epoch [7/8], Step [647/750], Loss: 0.0486\n",
      "Epoch [7/8], Step [648/750], Loss: 0.0441\n",
      "Epoch [7/8], Step [649/750], Loss: 0.0131\n",
      "Epoch [7/8], Step [650/750], Loss: 0.0110\n",
      "Epoch [7/8], Step [651/750], Loss: 0.0182\n",
      "Epoch [7/8], Step [652/750], Loss: 0.0148\n",
      "Epoch [7/8], Step [653/750], Loss: 0.0085\n",
      "Epoch [7/8], Step [654/750], Loss: 0.0398\n",
      "Epoch [7/8], Step [655/750], Loss: 0.0038\n",
      "Epoch [7/8], Step [656/750], Loss: 0.0136\n",
      "Epoch [7/8], Step [657/750], Loss: 0.0039\n",
      "Epoch [7/8], Step [658/750], Loss: 0.0200\n",
      "Epoch [7/8], Step [659/750], Loss: 0.0866\n",
      "Epoch [7/8], Step [660/750], Loss: 0.0015\n",
      "Epoch [7/8], Step [661/750], Loss: 0.0054\n",
      "Epoch [7/8], Step [662/750], Loss: 0.0130\n",
      "Epoch [7/8], Step [663/750], Loss: 0.0266\n",
      "Epoch [7/8], Step [664/750], Loss: 0.0492\n",
      "Epoch [7/8], Step [665/750], Loss: 0.0020\n",
      "Epoch [7/8], Step [666/750], Loss: 0.0008\n",
      "Epoch [7/8], Step [667/750], Loss: 0.0049\n",
      "Epoch [7/8], Step [668/750], Loss: 0.0053\n",
      "Epoch [7/8], Step [669/750], Loss: 0.0092\n",
      "Epoch [7/8], Step [670/750], Loss: 0.0205\n",
      "Epoch [7/8], Step [671/750], Loss: 0.0215\n",
      "Epoch [7/8], Step [672/750], Loss: 0.0230\n",
      "Epoch [7/8], Step [673/750], Loss: 0.0429\n",
      "Epoch [7/8], Step [674/750], Loss: 0.0624\n",
      "Epoch [7/8], Step [675/750], Loss: 0.0065\n",
      "Epoch [7/8], Step [676/750], Loss: 0.0034\n",
      "Epoch [7/8], Step [677/750], Loss: 0.0360\n",
      "Epoch [7/8], Step [678/750], Loss: 0.0043\n",
      "Epoch [7/8], Step [679/750], Loss: 0.0195\n",
      "Epoch [7/8], Step [680/750], Loss: 0.0599\n",
      "Epoch [7/8], Step [681/750], Loss: 0.0428\n",
      "Epoch [7/8], Step [682/750], Loss: 0.0154\n",
      "Epoch [7/8], Step [683/750], Loss: 0.0192\n",
      "Epoch [7/8], Step [684/750], Loss: 0.0069\n",
      "Epoch [7/8], Step [685/750], Loss: 0.0662\n",
      "Epoch [7/8], Step [686/750], Loss: 0.0107\n",
      "Epoch [7/8], Step [687/750], Loss: 0.1130\n",
      "Epoch [7/8], Step [688/750], Loss: 0.0148\n",
      "Epoch [7/8], Step [689/750], Loss: 0.0151\n",
      "Epoch [7/8], Step [690/750], Loss: 0.0229\n",
      "Epoch [7/8], Step [691/750], Loss: 0.0009\n",
      "Epoch [7/8], Step [692/750], Loss: 0.0008\n",
      "Epoch [7/8], Step [693/750], Loss: 0.0356\n",
      "Epoch [7/8], Step [694/750], Loss: 0.0048\n",
      "Epoch [7/8], Step [695/750], Loss: 0.0040\n",
      "Epoch [7/8], Step [696/750], Loss: 0.0007\n",
      "Epoch [7/8], Step [697/750], Loss: 0.0385\n",
      "Epoch [7/8], Step [698/750], Loss: 0.0319\n",
      "Epoch [7/8], Step [699/750], Loss: 0.0598\n",
      "Epoch [7/8], Step [700/750], Loss: 0.0419\n",
      "Epoch [7/8], Step [701/750], Loss: 0.0057\n",
      "Epoch [7/8], Step [702/750], Loss: 0.0062\n",
      "Epoch [7/8], Step [703/750], Loss: 0.1865\n",
      "Epoch [7/8], Step [704/750], Loss: 0.0639\n",
      "Epoch [7/8], Step [705/750], Loss: 0.0395\n",
      "Epoch [7/8], Step [706/750], Loss: 0.0038\n",
      "Epoch [7/8], Step [707/750], Loss: 0.0424\n",
      "Epoch [7/8], Step [708/750], Loss: 0.0132\n",
      "Epoch [7/8], Step [709/750], Loss: 0.0046\n",
      "Epoch [7/8], Step [710/750], Loss: 0.0033\n",
      "Epoch [7/8], Step [711/750], Loss: 0.0382\n",
      "Epoch [7/8], Step [712/750], Loss: 0.0203\n",
      "Epoch [7/8], Step [713/750], Loss: 0.0017\n",
      "Epoch [7/8], Step [714/750], Loss: 0.0306\n",
      "Epoch [7/8], Step [715/750], Loss: 0.1318\n",
      "Epoch [7/8], Step [716/750], Loss: 0.0821\n",
      "Epoch [7/8], Step [717/750], Loss: 0.0578\n",
      "Epoch [7/8], Step [718/750], Loss: 0.0231\n",
      "Epoch [7/8], Step [719/750], Loss: 0.0262\n",
      "Epoch [7/8], Step [720/750], Loss: 0.0249\n",
      "Epoch [7/8], Step [721/750], Loss: 0.0674\n",
      "Epoch [7/8], Step [722/750], Loss: 0.0007\n",
      "Epoch [7/8], Step [723/750], Loss: 0.0699\n",
      "Epoch [7/8], Step [724/750], Loss: 0.0282\n",
      "Epoch [7/8], Step [725/750], Loss: 0.0044\n",
      "Epoch [7/8], Step [726/750], Loss: 0.0067\n",
      "Epoch [7/8], Step [727/750], Loss: 0.0238\n",
      "Epoch [7/8], Step [728/750], Loss: 0.0121\n",
      "Epoch [7/8], Step [729/750], Loss: 0.1019\n",
      "Epoch [7/8], Step [730/750], Loss: 0.0333\n",
      "Epoch [7/8], Step [731/750], Loss: 0.0060\n",
      "Epoch [7/8], Step [732/750], Loss: 0.1018\n",
      "Epoch [7/8], Step [733/750], Loss: 0.1857\n",
      "Epoch [7/8], Step [734/750], Loss: 0.0145\n",
      "Epoch [7/8], Step [735/750], Loss: 0.0026\n",
      "Epoch [7/8], Step [736/750], Loss: 0.0089\n",
      "Epoch [7/8], Step [737/750], Loss: 0.0346\n",
      "Epoch [7/8], Step [738/750], Loss: 0.0439\n",
      "Epoch [7/8], Step [739/750], Loss: 0.0383\n",
      "Epoch [7/8], Step [740/750], Loss: 0.0096\n",
      "Epoch [7/8], Step [741/750], Loss: 0.0037\n",
      "Epoch [7/8], Step [742/750], Loss: 0.0481\n",
      "Epoch [7/8], Step [743/750], Loss: 0.1471\n",
      "Epoch [7/8], Step [744/750], Loss: 0.0606\n",
      "Epoch [7/8], Step [745/750], Loss: 0.0167\n",
      "Epoch [7/8], Step [746/750], Loss: 0.0442\n",
      "Epoch [7/8], Step [747/750], Loss: 0.0087\n",
      "Epoch [7/8], Step [748/750], Loss: 0.0144\n",
      "Epoch [7/8], Step [749/750], Loss: 0.1169\n",
      "Epoch [7/8], Step [750/750], Loss: 0.1337\n",
      "Epoch [7/8], Tr. loss: 0.4037. Test loss: 0.3499\n",
      "\n",
      "\n",
      "Epoch [8/8], Step [1/750], Loss: 0.0060\n",
      "Epoch [8/8], Step [2/750], Loss: 0.0066\n",
      "Epoch [8/8], Step [3/750], Loss: 0.0222\n",
      "Epoch [8/8], Step [4/750], Loss: 0.0178\n",
      "Epoch [8/8], Step [5/750], Loss: 0.0189\n",
      "Epoch [8/8], Step [6/750], Loss: 0.0161\n",
      "Epoch [8/8], Step [7/750], Loss: 0.0373\n",
      "Epoch [8/8], Step [8/750], Loss: 0.0027\n",
      "Epoch [8/8], Step [9/750], Loss: 0.0276\n",
      "Epoch [8/8], Step [10/750], Loss: 0.0191\n",
      "Epoch [8/8], Step [11/750], Loss: 0.0357\n",
      "Epoch [8/8], Step [12/750], Loss: 0.0146\n",
      "Epoch [8/8], Step [13/750], Loss: 0.0066\n",
      "Epoch [8/8], Step [14/750], Loss: 0.0025\n",
      "Epoch [8/8], Step [15/750], Loss: 0.0037\n",
      "Epoch [8/8], Step [16/750], Loss: 0.0746\n",
      "Epoch [8/8], Step [17/750], Loss: 0.0068\n",
      "Epoch [8/8], Step [18/750], Loss: 0.1248\n",
      "Epoch [8/8], Step [19/750], Loss: 0.0818\n",
      "Epoch [8/8], Step [20/750], Loss: 0.0239\n",
      "Epoch [8/8], Step [21/750], Loss: 0.0020\n",
      "Epoch [8/8], Step [22/750], Loss: 0.0120\n",
      "Epoch [8/8], Step [23/750], Loss: 0.0145\n",
      "Epoch [8/8], Step [24/750], Loss: 0.0359\n",
      "Epoch [8/8], Step [25/750], Loss: 0.0603\n",
      "Epoch [8/8], Step [26/750], Loss: 0.0390\n",
      "Epoch [8/8], Step [27/750], Loss: 0.0442\n",
      "Epoch [8/8], Step [28/750], Loss: 0.0037\n",
      "Epoch [8/8], Step [29/750], Loss: 0.0359\n",
      "Epoch [8/8], Step [30/750], Loss: 0.0096\n",
      "Epoch [8/8], Step [31/750], Loss: 0.0494\n",
      "Epoch [8/8], Step [32/750], Loss: 0.0058\n",
      "Epoch [8/8], Step [33/750], Loss: 0.0099\n",
      "Epoch [8/8], Step [34/750], Loss: 0.0394\n",
      "Epoch [8/8], Step [35/750], Loss: 0.0055\n",
      "Epoch [8/8], Step [36/750], Loss: 0.0662\n",
      "Epoch [8/8], Step [37/750], Loss: 0.0019\n",
      "Epoch [8/8], Step [38/750], Loss: 0.0026\n",
      "Epoch [8/8], Step [39/750], Loss: 0.0097\n",
      "Epoch [8/8], Step [40/750], Loss: 0.0021\n",
      "Epoch [8/8], Step [41/750], Loss: 0.0378\n",
      "Epoch [8/8], Step [42/750], Loss: 0.0246\n",
      "Epoch [8/8], Step [43/750], Loss: 0.0310\n",
      "Epoch [8/8], Step [44/750], Loss: 0.0257\n",
      "Epoch [8/8], Step [45/750], Loss: 0.0138\n",
      "Epoch [8/8], Step [46/750], Loss: 0.0014\n",
      "Epoch [8/8], Step [47/750], Loss: 0.0092\n",
      "Epoch [8/8], Step [48/750], Loss: 0.0052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Step [49/750], Loss: 0.0101\n",
      "Epoch [8/8], Step [50/750], Loss: 0.0081\n",
      "Epoch [8/8], Step [51/750], Loss: 0.0359\n",
      "Epoch [8/8], Step [52/750], Loss: 0.0074\n",
      "Epoch [8/8], Step [53/750], Loss: 0.0474\n",
      "Epoch [8/8], Step [54/750], Loss: 0.0327\n",
      "Epoch [8/8], Step [55/750], Loss: 0.0235\n",
      "Epoch [8/8], Step [56/750], Loss: 0.0021\n",
      "Epoch [8/8], Step [57/750], Loss: 0.0275\n",
      "Epoch [8/8], Step [58/750], Loss: 0.0021\n",
      "Epoch [8/8], Step [59/750], Loss: 0.0021\n",
      "Epoch [8/8], Step [60/750], Loss: 0.0031\n",
      "Epoch [8/8], Step [61/750], Loss: 0.0305\n",
      "Epoch [8/8], Step [62/750], Loss: 0.0895\n",
      "Epoch [8/8], Step [63/750], Loss: 0.0039\n",
      "Epoch [8/8], Step [64/750], Loss: 0.0115\n",
      "Epoch [8/8], Step [65/750], Loss: 0.0018\n",
      "Epoch [8/8], Step [66/750], Loss: 0.0031\n",
      "Epoch [8/8], Step [67/750], Loss: 0.0351\n",
      "Epoch [8/8], Step [68/750], Loss: 0.0084\n",
      "Epoch [8/8], Step [69/750], Loss: 0.0059\n",
      "Epoch [8/8], Step [70/750], Loss: 0.0274\n",
      "Epoch [8/8], Step [71/750], Loss: 0.0291\n",
      "Epoch [8/8], Step [72/750], Loss: 0.0014\n",
      "Epoch [8/8], Step [73/750], Loss: 0.0230\n",
      "Epoch [8/8], Step [74/750], Loss: 0.0120\n",
      "Epoch [8/8], Step [75/750], Loss: 0.0084\n",
      "Epoch [8/8], Step [76/750], Loss: 0.0056\n",
      "Epoch [8/8], Step [77/750], Loss: 0.0012\n",
      "Epoch [8/8], Step [78/750], Loss: 0.0040\n",
      "Epoch [8/8], Step [79/750], Loss: 0.0033\n",
      "Epoch [8/8], Step [80/750], Loss: 0.0681\n",
      "Epoch [8/8], Step [81/750], Loss: 0.0023\n",
      "Epoch [8/8], Step [82/750], Loss: 0.0646\n",
      "Epoch [8/8], Step [83/750], Loss: 0.0240\n",
      "Epoch [8/8], Step [84/750], Loss: 0.0234\n",
      "Epoch [8/8], Step [85/750], Loss: 0.0226\n",
      "Epoch [8/8], Step [86/750], Loss: 0.0532\n",
      "Epoch [8/8], Step [87/750], Loss: 0.0137\n",
      "Epoch [8/8], Step [88/750], Loss: 0.0103\n",
      "Epoch [8/8], Step [89/750], Loss: 0.0793\n",
      "Epoch [8/8], Step [90/750], Loss: 0.0116\n",
      "Epoch [8/8], Step [91/750], Loss: 0.0021\n",
      "Epoch [8/8], Step [92/750], Loss: 0.0061\n",
      "Epoch [8/8], Step [93/750], Loss: 0.0299\n",
      "Epoch [8/8], Step [94/750], Loss: 0.0020\n",
      "Epoch [8/8], Step [95/750], Loss: 0.0431\n",
      "Epoch [8/8], Step [96/750], Loss: 0.0037\n",
      "Epoch [8/8], Step [97/750], Loss: 0.0031\n",
      "Epoch [8/8], Step [98/750], Loss: 0.0425\n",
      "Epoch [8/8], Step [99/750], Loss: 0.0245\n",
      "Epoch [8/8], Step [100/750], Loss: 0.0299\n",
      "Epoch [8/8], Step [101/750], Loss: 0.0057\n",
      "Epoch [8/8], Step [102/750], Loss: 0.0551\n",
      "Epoch [8/8], Step [103/750], Loss: 0.0962\n",
      "Epoch [8/8], Step [104/750], Loss: 0.0022\n",
      "Epoch [8/8], Step [105/750], Loss: 0.0126\n",
      "Epoch [8/8], Step [106/750], Loss: 0.0110\n",
      "Epoch [8/8], Step [107/750], Loss: 0.0004\n",
      "Epoch [8/8], Step [108/750], Loss: 0.0058\n",
      "Epoch [8/8], Step [109/750], Loss: 0.0033\n",
      "Epoch [8/8], Step [110/750], Loss: 0.0008\n",
      "Epoch [8/8], Step [111/750], Loss: 0.0180\n",
      "Epoch [8/8], Step [112/750], Loss: 0.0019\n",
      "Epoch [8/8], Step [113/750], Loss: 0.0710\n",
      "Epoch [8/8], Step [114/750], Loss: 0.0398\n",
      "Epoch [8/8], Step [115/750], Loss: 0.0250\n",
      "Epoch [8/8], Step [116/750], Loss: 0.0040\n",
      "Epoch [8/8], Step [117/750], Loss: 0.0112\n",
      "Epoch [8/8], Step [118/750], Loss: 0.0125\n",
      "Epoch [8/8], Step [119/750], Loss: 0.0162\n",
      "Epoch [8/8], Step [120/750], Loss: 0.0056\n",
      "Epoch [8/8], Step [121/750], Loss: 0.0279\n",
      "Epoch [8/8], Step [122/750], Loss: 0.0186\n",
      "Epoch [8/8], Step [123/750], Loss: 0.0492\n",
      "Epoch [8/8], Step [124/750], Loss: 0.0103\n",
      "Epoch [8/8], Step [125/750], Loss: 0.0046\n",
      "Epoch [8/8], Step [126/750], Loss: 0.0011\n",
      "Epoch [8/8], Step [127/750], Loss: 0.1153\n",
      "Epoch [8/8], Step [128/750], Loss: 0.0257\n",
      "Epoch [8/8], Step [129/750], Loss: 0.0008\n",
      "Epoch [8/8], Step [130/750], Loss: 0.0020\n",
      "Epoch [8/8], Step [131/750], Loss: 0.0038\n",
      "Epoch [8/8], Step [132/750], Loss: 0.0243\n",
      "Epoch [8/8], Step [133/750], Loss: 0.0009\n",
      "Epoch [8/8], Step [134/750], Loss: 0.0058\n",
      "Epoch [8/8], Step [135/750], Loss: 0.0519\n",
      "Epoch [8/8], Step [136/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [137/750], Loss: 0.0270\n",
      "Epoch [8/8], Step [138/750], Loss: 0.0337\n",
      "Epoch [8/8], Step [139/750], Loss: 0.0045\n",
      "Epoch [8/8], Step [140/750], Loss: 0.0024\n",
      "Epoch [8/8], Step [141/750], Loss: 0.0066\n",
      "Epoch [8/8], Step [142/750], Loss: 0.0008\n",
      "Epoch [8/8], Step [143/750], Loss: 0.0577\n",
      "Epoch [8/8], Step [144/750], Loss: 0.0031\n",
      "Epoch [8/8], Step [145/750], Loss: 0.0037\n",
      "Epoch [8/8], Step [146/750], Loss: 0.0009\n",
      "Epoch [8/8], Step [147/750], Loss: 0.0191\n",
      "Epoch [8/8], Step [148/750], Loss: 0.0424\n",
      "Epoch [8/8], Step [149/750], Loss: 0.0313\n",
      "Epoch [8/8], Step [150/750], Loss: 0.0035\n",
      "Epoch [8/8], Step [151/750], Loss: 0.0656\n",
      "Epoch [8/8], Step [152/750], Loss: 0.0045\n",
      "Epoch [8/8], Step [153/750], Loss: 0.0112\n",
      "Epoch [8/8], Step [154/750], Loss: 0.0017\n",
      "Epoch [8/8], Step [155/750], Loss: 0.0015\n",
      "Epoch [8/8], Step [156/750], Loss: 0.0257\n",
      "Epoch [8/8], Step [157/750], Loss: 0.1014\n",
      "Epoch [8/8], Step [158/750], Loss: 0.0504\n",
      "Epoch [8/8], Step [159/750], Loss: 0.0299\n",
      "Epoch [8/8], Step [160/750], Loss: 0.0258\n",
      "Epoch [8/8], Step [161/750], Loss: 0.0028\n",
      "Epoch [8/8], Step [162/750], Loss: 0.0217\n",
      "Epoch [8/8], Step [163/750], Loss: 0.0063\n",
      "Epoch [8/8], Step [164/750], Loss: 0.0012\n",
      "Epoch [8/8], Step [165/750], Loss: 0.0190\n",
      "Epoch [8/8], Step [166/750], Loss: 0.0090\n",
      "Epoch [8/8], Step [167/750], Loss: 0.0103\n",
      "Epoch [8/8], Step [168/750], Loss: 0.0513\n",
      "Epoch [8/8], Step [169/750], Loss: 0.0431\n",
      "Epoch [8/8], Step [170/750], Loss: 0.0756\n",
      "Epoch [8/8], Step [171/750], Loss: 0.0027\n",
      "Epoch [8/8], Step [172/750], Loss: 0.0790\n",
      "Epoch [8/8], Step [173/750], Loss: 0.0303\n",
      "Epoch [8/8], Step [174/750], Loss: 0.0077\n",
      "Epoch [8/8], Step [175/750], Loss: 0.0138\n",
      "Epoch [8/8], Step [176/750], Loss: 0.0366\n",
      "Epoch [8/8], Step [177/750], Loss: 0.0026\n",
      "Epoch [8/8], Step [178/750], Loss: 0.0040\n",
      "Epoch [8/8], Step [179/750], Loss: 0.0085\n",
      "Epoch [8/8], Step [180/750], Loss: 0.0972\n",
      "Epoch [8/8], Step [181/750], Loss: 0.0010\n",
      "Epoch [8/8], Step [182/750], Loss: 0.0053\n",
      "Epoch [8/8], Step [183/750], Loss: 0.0010\n",
      "Epoch [8/8], Step [184/750], Loss: 0.0417\n",
      "Epoch [8/8], Step [185/750], Loss: 0.0067\n",
      "Epoch [8/8], Step [186/750], Loss: 0.0494\n",
      "Epoch [8/8], Step [187/750], Loss: 0.0105\n",
      "Epoch [8/8], Step [188/750], Loss: 0.0011\n",
      "Epoch [8/8], Step [189/750], Loss: 0.0053\n",
      "Epoch [8/8], Step [190/750], Loss: 0.0106\n",
      "Epoch [8/8], Step [191/750], Loss: 0.0292\n",
      "Epoch [8/8], Step [192/750], Loss: 0.0536\n",
      "Epoch [8/8], Step [193/750], Loss: 0.0036\n",
      "Epoch [8/8], Step [194/750], Loss: 0.0128\n",
      "Epoch [8/8], Step [195/750], Loss: 0.0014\n",
      "Epoch [8/8], Step [196/750], Loss: 0.0112\n",
      "Epoch [8/8], Step [197/750], Loss: 0.0245\n",
      "Epoch [8/8], Step [198/750], Loss: 0.0021\n",
      "Epoch [8/8], Step [199/750], Loss: 0.0057\n",
      "Epoch [8/8], Step [200/750], Loss: 0.0043\n",
      "Epoch [8/8], Step [201/750], Loss: 0.0090\n",
      "Epoch [8/8], Step [202/750], Loss: 0.0091\n",
      "Epoch [8/8], Step [203/750], Loss: 0.0463\n",
      "Epoch [8/8], Step [204/750], Loss: 0.0725\n",
      "Epoch [8/8], Step [205/750], Loss: 0.0025\n",
      "Epoch [8/8], Step [206/750], Loss: 0.0047\n",
      "Epoch [8/8], Step [207/750], Loss: 0.0093\n",
      "Epoch [8/8], Step [208/750], Loss: 0.0489\n",
      "Epoch [8/8], Step [209/750], Loss: 0.0013\n",
      "Epoch [8/8], Step [210/750], Loss: 0.0025\n",
      "Epoch [8/8], Step [211/750], Loss: 0.0121\n",
      "Epoch [8/8], Step [212/750], Loss: 0.0010\n",
      "Epoch [8/8], Step [213/750], Loss: 0.0446\n",
      "Epoch [8/8], Step [214/750], Loss: 0.0135\n",
      "Epoch [8/8], Step [215/750], Loss: 0.0036\n",
      "Epoch [8/8], Step [216/750], Loss: 0.0448\n",
      "Epoch [8/8], Step [217/750], Loss: 0.0073\n",
      "Epoch [8/8], Step [218/750], Loss: 0.0708\n",
      "Epoch [8/8], Step [219/750], Loss: 0.0704\n",
      "Epoch [8/8], Step [220/750], Loss: 0.0233\n",
      "Epoch [8/8], Step [221/750], Loss: 0.0021\n",
      "Epoch [8/8], Step [222/750], Loss: 0.0008\n",
      "Epoch [8/8], Step [223/750], Loss: 0.0095\n",
      "Epoch [8/8], Step [224/750], Loss: 0.0031\n",
      "Epoch [8/8], Step [225/750], Loss: 0.0065\n",
      "Epoch [8/8], Step [226/750], Loss: 0.0241\n",
      "Epoch [8/8], Step [227/750], Loss: 0.0015\n",
      "Epoch [8/8], Step [228/750], Loss: 0.1245\n",
      "Epoch [8/8], Step [229/750], Loss: 0.1169\n",
      "Epoch [8/8], Step [230/750], Loss: 0.0299\n",
      "Epoch [8/8], Step [231/750], Loss: 0.0042\n",
      "Epoch [8/8], Step [232/750], Loss: 0.0175\n",
      "Epoch [8/8], Step [233/750], Loss: 0.0080\n",
      "Epoch [8/8], Step [234/750], Loss: 0.0315\n",
      "Epoch [8/8], Step [235/750], Loss: 0.0648\n",
      "Epoch [8/8], Step [236/750], Loss: 0.0074\n",
      "Epoch [8/8], Step [237/750], Loss: 0.0248\n",
      "Epoch [8/8], Step [238/750], Loss: 0.1121\n",
      "Epoch [8/8], Step [239/750], Loss: 0.0189\n",
      "Epoch [8/8], Step [240/750], Loss: 0.0363\n",
      "Epoch [8/8], Step [241/750], Loss: 0.0026\n",
      "Epoch [8/8], Step [242/750], Loss: 0.0042\n",
      "Epoch [8/8], Step [243/750], Loss: 0.1188\n",
      "Epoch [8/8], Step [244/750], Loss: 0.0156\n",
      "Epoch [8/8], Step [245/750], Loss: 0.0049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Step [246/750], Loss: 0.0118\n",
      "Epoch [8/8], Step [247/750], Loss: 0.0388\n",
      "Epoch [8/8], Step [248/750], Loss: 0.0274\n",
      "Epoch [8/8], Step [249/750], Loss: 0.0488\n",
      "Epoch [8/8], Step [250/750], Loss: 0.0262\n",
      "Epoch [8/8], Step [251/750], Loss: 0.0164\n",
      "Epoch [8/8], Step [252/750], Loss: 0.0493\n",
      "Epoch [8/8], Step [253/750], Loss: 0.0362\n",
      "Epoch [8/8], Step [254/750], Loss: 0.0252\n",
      "Epoch [8/8], Step [255/750], Loss: 0.0067\n",
      "Epoch [8/8], Step [256/750], Loss: 0.0229\n",
      "Epoch [8/8], Step [257/750], Loss: 0.0024\n",
      "Epoch [8/8], Step [258/750], Loss: 0.0493\n",
      "Epoch [8/8], Step [259/750], Loss: 0.0213\n",
      "Epoch [8/8], Step [260/750], Loss: 0.0078\n",
      "Epoch [8/8], Step [261/750], Loss: 0.0206\n",
      "Epoch [8/8], Step [262/750], Loss: 0.0316\n",
      "Epoch [8/8], Step [263/750], Loss: 0.0400\n",
      "Epoch [8/8], Step [264/750], Loss: 0.0389\n",
      "Epoch [8/8], Step [265/750], Loss: 0.0878\n",
      "Epoch [8/8], Step [266/750], Loss: 0.0093\n",
      "Epoch [8/8], Step [267/750], Loss: 0.2157\n",
      "Epoch [8/8], Step [268/750], Loss: 0.0081\n",
      "Epoch [8/8], Step [269/750], Loss: 0.0893\n",
      "Epoch [8/8], Step [270/750], Loss: 0.0046\n",
      "Epoch [8/8], Step [271/750], Loss: 0.0528\n",
      "Epoch [8/8], Step [272/750], Loss: 0.0033\n",
      "Epoch [8/8], Step [273/750], Loss: 0.0280\n",
      "Epoch [8/8], Step [274/750], Loss: 0.0954\n",
      "Epoch [8/8], Step [275/750], Loss: 0.0163\n",
      "Epoch [8/8], Step [276/750], Loss: 0.0094\n",
      "Epoch [8/8], Step [277/750], Loss: 0.1286\n",
      "Epoch [8/8], Step [278/750], Loss: 0.0241\n",
      "Epoch [8/8], Step [279/750], Loss: 0.0544\n",
      "Epoch [8/8], Step [280/750], Loss: 0.0171\n",
      "Epoch [8/8], Step [281/750], Loss: 0.0076\n",
      "Epoch [8/8], Step [282/750], Loss: 0.0033\n",
      "Epoch [8/8], Step [283/750], Loss: 0.0069\n",
      "Epoch [8/8], Step [284/750], Loss: 0.0185\n",
      "Epoch [8/8], Step [285/750], Loss: 0.0730\n",
      "Epoch [8/8], Step [286/750], Loss: 0.0133\n",
      "Epoch [8/8], Step [287/750], Loss: 0.0470\n",
      "Epoch [8/8], Step [288/750], Loss: 0.0115\n",
      "Epoch [8/8], Step [289/750], Loss: 0.0195\n",
      "Epoch [8/8], Step [290/750], Loss: 0.0029\n",
      "Epoch [8/8], Step [291/750], Loss: 0.0249\n",
      "Epoch [8/8], Step [292/750], Loss: 0.1023\n",
      "Epoch [8/8], Step [293/750], Loss: 0.0314\n",
      "Epoch [8/8], Step [294/750], Loss: 0.0406\n",
      "Epoch [8/8], Step [295/750], Loss: 0.0102\n",
      "Epoch [8/8], Step [296/750], Loss: 0.0350\n",
      "Epoch [8/8], Step [297/750], Loss: 0.0019\n",
      "Epoch [8/8], Step [298/750], Loss: 0.0352\n",
      "Epoch [8/8], Step [299/750], Loss: 0.0070\n",
      "Epoch [8/8], Step [300/750], Loss: 0.0120\n",
      "Epoch [8/8], Step [301/750], Loss: 0.0069\n",
      "Epoch [8/8], Step [302/750], Loss: 0.0747\n",
      "Epoch [8/8], Step [303/750], Loss: 0.1292\n",
      "Epoch [8/8], Step [304/750], Loss: 0.0193\n",
      "Epoch [8/8], Step [305/750], Loss: 0.0672\n",
      "Epoch [8/8], Step [306/750], Loss: 0.0048\n",
      "Epoch [8/8], Step [307/750], Loss: 0.0049\n",
      "Epoch [8/8], Step [308/750], Loss: 0.0036\n",
      "Epoch [8/8], Step [309/750], Loss: 0.1406\n",
      "Epoch [8/8], Step [310/750], Loss: 0.0167\n",
      "Epoch [8/8], Step [311/750], Loss: 0.0401\n",
      "Epoch [8/8], Step [312/750], Loss: 0.0136\n",
      "Epoch [8/8], Step [313/750], Loss: 0.0061\n",
      "Epoch [8/8], Step [314/750], Loss: 0.0749\n",
      "Epoch [8/8], Step [315/750], Loss: 0.0045\n",
      "Epoch [8/8], Step [316/750], Loss: 0.0243\n",
      "Epoch [8/8], Step [317/750], Loss: 0.0112\n",
      "Epoch [8/8], Step [318/750], Loss: 0.0177\n",
      "Epoch [8/8], Step [319/750], Loss: 0.0410\n",
      "Epoch [8/8], Step [320/750], Loss: 0.0111\n",
      "Epoch [8/8], Step [321/750], Loss: 0.1026\n",
      "Epoch [8/8], Step [322/750], Loss: 0.0797\n",
      "Epoch [8/8], Step [323/750], Loss: 0.0630\n",
      "Epoch [8/8], Step [324/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [325/750], Loss: 0.0334\n",
      "Epoch [8/8], Step [326/750], Loss: 0.0289\n",
      "Epoch [8/8], Step [327/750], Loss: 0.0231\n",
      "Epoch [8/8], Step [328/750], Loss: 0.0044\n",
      "Epoch [8/8], Step [329/750], Loss: 0.0215\n",
      "Epoch [8/8], Step [330/750], Loss: 0.0046\n",
      "Epoch [8/8], Step [331/750], Loss: 0.0951\n",
      "Epoch [8/8], Step [332/750], Loss: 0.0172\n",
      "Epoch [8/8], Step [333/750], Loss: 0.0118\n",
      "Epoch [8/8], Step [334/750], Loss: 0.0049\n",
      "Epoch [8/8], Step [335/750], Loss: 0.0115\n",
      "Epoch [8/8], Step [336/750], Loss: 0.0058\n",
      "Epoch [8/8], Step [337/750], Loss: 0.0231\n",
      "Epoch [8/8], Step [338/750], Loss: 0.0265\n",
      "Epoch [8/8], Step [339/750], Loss: 0.0199\n",
      "Epoch [8/8], Step [340/750], Loss: 0.0143\n",
      "Epoch [8/8], Step [341/750], Loss: 0.2269\n",
      "Epoch [8/8], Step [342/750], Loss: 0.0010\n",
      "Epoch [8/8], Step [343/750], Loss: 0.0493\n",
      "Epoch [8/8], Step [344/750], Loss: 0.0342\n",
      "Epoch [8/8], Step [345/750], Loss: 0.0143\n",
      "Epoch [8/8], Step [346/750], Loss: 0.0675\n",
      "Epoch [8/8], Step [347/750], Loss: 0.0263\n",
      "Epoch [8/8], Step [348/750], Loss: 0.0131\n",
      "Epoch [8/8], Step [349/750], Loss: 0.0140\n",
      "Epoch [8/8], Step [350/750], Loss: 0.0155\n",
      "Epoch [8/8], Step [351/750], Loss: 0.0490\n",
      "Epoch [8/8], Step [352/750], Loss: 0.0041\n",
      "Epoch [8/8], Step [353/750], Loss: 0.0107\n",
      "Epoch [8/8], Step [354/750], Loss: 0.0092\n",
      "Epoch [8/8], Step [355/750], Loss: 0.0710\n",
      "Epoch [8/8], Step [356/750], Loss: 0.0064\n",
      "Epoch [8/8], Step [357/750], Loss: 0.0056\n",
      "Epoch [8/8], Step [358/750], Loss: 0.0064\n",
      "Epoch [8/8], Step [359/750], Loss: 0.0062\n",
      "Epoch [8/8], Step [360/750], Loss: 0.0578\n",
      "Epoch [8/8], Step [361/750], Loss: 0.0018\n",
      "Epoch [8/8], Step [362/750], Loss: 0.0060\n",
      "Epoch [8/8], Step [363/750], Loss: 0.0044\n",
      "Epoch [8/8], Step [364/750], Loss: 0.0170\n",
      "Epoch [8/8], Step [365/750], Loss: 0.0184\n",
      "Epoch [8/8], Step [366/750], Loss: 0.0105\n",
      "Epoch [8/8], Step [367/750], Loss: 0.0396\n",
      "Epoch [8/8], Step [368/750], Loss: 0.0212\n",
      "Epoch [8/8], Step [369/750], Loss: 0.0480\n",
      "Epoch [8/8], Step [370/750], Loss: 0.0020\n",
      "Epoch [8/8], Step [371/750], Loss: 0.0206\n",
      "Epoch [8/8], Step [372/750], Loss: 0.0124\n",
      "Epoch [8/8], Step [373/750], Loss: 0.0419\n",
      "Epoch [8/8], Step [374/750], Loss: 0.0112\n",
      "Epoch [8/8], Step [375/750], Loss: 0.0060\n",
      "Epoch [8/8], Step [376/750], Loss: 0.0274\n",
      "Epoch [8/8], Step [377/750], Loss: 0.0091\n",
      "Epoch [8/8], Step [378/750], Loss: 0.0217\n",
      "Epoch [8/8], Step [379/750], Loss: 0.0279\n",
      "Epoch [8/8], Step [380/750], Loss: 0.0372\n",
      "Epoch [8/8], Step [381/750], Loss: 0.0037\n",
      "Epoch [8/8], Step [382/750], Loss: 0.0447\n",
      "Epoch [8/8], Step [383/750], Loss: 0.0412\n",
      "Epoch [8/8], Step [384/750], Loss: 0.0061\n",
      "Epoch [8/8], Step [385/750], Loss: 0.0365\n",
      "Epoch [8/8], Step [386/750], Loss: 0.0093\n",
      "Epoch [8/8], Step [387/750], Loss: 0.0589\n",
      "Epoch [8/8], Step [388/750], Loss: 0.0504\n",
      "Epoch [8/8], Step [389/750], Loss: 0.0406\n",
      "Epoch [8/8], Step [390/750], Loss: 0.0299\n",
      "Epoch [8/8], Step [391/750], Loss: 0.1655\n",
      "Epoch [8/8], Step [392/750], Loss: 0.0059\n",
      "Epoch [8/8], Step [393/750], Loss: 0.0035\n",
      "Epoch [8/8], Step [394/750], Loss: 0.0038\n",
      "Epoch [8/8], Step [395/750], Loss: 0.0430\n",
      "Epoch [8/8], Step [396/750], Loss: 0.0322\n",
      "Epoch [8/8], Step [397/750], Loss: 0.0096\n",
      "Epoch [8/8], Step [398/750], Loss: 0.0241\n",
      "Epoch [8/8], Step [399/750], Loss: 0.0352\n",
      "Epoch [8/8], Step [400/750], Loss: 0.0074\n",
      "Epoch [8/8], Step [401/750], Loss: 0.0049\n",
      "Epoch [8/8], Step [402/750], Loss: 0.0093\n",
      "Epoch [8/8], Step [403/750], Loss: 0.0220\n",
      "Epoch [8/8], Step [404/750], Loss: 0.0082\n",
      "Epoch [8/8], Step [405/750], Loss: 0.0023\n",
      "Epoch [8/8], Step [406/750], Loss: 0.0179\n",
      "Epoch [8/8], Step [407/750], Loss: 0.0117\n",
      "Epoch [8/8], Step [408/750], Loss: 0.0048\n",
      "Epoch [8/8], Step [409/750], Loss: 0.0322\n",
      "Epoch [8/8], Step [410/750], Loss: 0.0316\n",
      "Epoch [8/8], Step [411/750], Loss: 0.1082\n",
      "Epoch [8/8], Step [412/750], Loss: 0.0140\n",
      "Epoch [8/8], Step [413/750], Loss: 0.0044\n",
      "Epoch [8/8], Step [414/750], Loss: 0.0174\n",
      "Epoch [8/8], Step [415/750], Loss: 0.0808\n",
      "Epoch [8/8], Step [416/750], Loss: 0.0114\n",
      "Epoch [8/8], Step [417/750], Loss: 0.0010\n",
      "Epoch [8/8], Step [418/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [419/750], Loss: 0.0018\n",
      "Epoch [8/8], Step [420/750], Loss: 0.0022\n",
      "Epoch [8/8], Step [421/750], Loss: 0.0048\n",
      "Epoch [8/8], Step [422/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [423/750], Loss: 0.0820\n",
      "Epoch [8/8], Step [424/750], Loss: 0.0283\n",
      "Epoch [8/8], Step [425/750], Loss: 0.0170\n",
      "Epoch [8/8], Step [426/750], Loss: 0.1040\n",
      "Epoch [8/8], Step [427/750], Loss: 0.0581\n",
      "Epoch [8/8], Step [428/750], Loss: 0.0098\n",
      "Epoch [8/8], Step [429/750], Loss: 0.0147\n",
      "Epoch [8/8], Step [430/750], Loss: 0.0545\n",
      "Epoch [8/8], Step [431/750], Loss: 0.0005\n",
      "Epoch [8/8], Step [432/750], Loss: 0.0320\n",
      "Epoch [8/8], Step [433/750], Loss: 0.0124\n",
      "Epoch [8/8], Step [434/750], Loss: 0.0035\n",
      "Epoch [8/8], Step [435/750], Loss: 0.0463\n",
      "Epoch [8/8], Step [436/750], Loss: 0.0004\n",
      "Epoch [8/8], Step [437/750], Loss: 0.0271\n",
      "Epoch [8/8], Step [438/750], Loss: 0.0013\n",
      "Epoch [8/8], Step [439/750], Loss: 0.0518\n",
      "Epoch [8/8], Step [440/750], Loss: 0.0424\n",
      "Epoch [8/8], Step [441/750], Loss: 0.0206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Step [442/750], Loss: 0.0353\n",
      "Epoch [8/8], Step [443/750], Loss: 0.0130\n",
      "Epoch [8/8], Step [444/750], Loss: 0.0330\n",
      "Epoch [8/8], Step [445/750], Loss: 0.0035\n",
      "Epoch [8/8], Step [446/750], Loss: 0.0108\n",
      "Epoch [8/8], Step [447/750], Loss: 0.0006\n",
      "Epoch [8/8], Step [448/750], Loss: 0.0202\n",
      "Epoch [8/8], Step [449/750], Loss: 0.0008\n",
      "Epoch [8/8], Step [450/750], Loss: 0.0413\n",
      "Epoch [8/8], Step [451/750], Loss: 0.0809\n",
      "Epoch [8/8], Step [452/750], Loss: 0.0073\n",
      "Epoch [8/8], Step [453/750], Loss: 0.0060\n",
      "Epoch [8/8], Step [454/750], Loss: 0.0168\n",
      "Epoch [8/8], Step [455/750], Loss: 0.0043\n",
      "Epoch [8/8], Step [456/750], Loss: 0.0950\n",
      "Epoch [8/8], Step [457/750], Loss: 0.0010\n",
      "Epoch [8/8], Step [458/750], Loss: 0.0111\n",
      "Epoch [8/8], Step [459/750], Loss: 0.0450\n",
      "Epoch [8/8], Step [460/750], Loss: 0.0278\n",
      "Epoch [8/8], Step [461/750], Loss: 0.0521\n",
      "Epoch [8/8], Step [462/750], Loss: 0.0217\n",
      "Epoch [8/8], Step [463/750], Loss: 0.0643\n",
      "Epoch [8/8], Step [464/750], Loss: 0.0029\n",
      "Epoch [8/8], Step [465/750], Loss: 0.0011\n",
      "Epoch [8/8], Step [466/750], Loss: 0.0011\n",
      "Epoch [8/8], Step [467/750], Loss: 0.0008\n",
      "Epoch [8/8], Step [468/750], Loss: 0.0258\n",
      "Epoch [8/8], Step [469/750], Loss: 0.0039\n",
      "Epoch [8/8], Step [470/750], Loss: 0.0119\n",
      "Epoch [8/8], Step [471/750], Loss: 0.0700\n",
      "Epoch [8/8], Step [472/750], Loss: 0.0060\n",
      "Epoch [8/8], Step [473/750], Loss: 0.1534\n",
      "Epoch [8/8], Step [474/750], Loss: 0.0480\n",
      "Epoch [8/8], Step [475/750], Loss: 0.0444\n",
      "Epoch [8/8], Step [476/750], Loss: 0.0078\n",
      "Epoch [8/8], Step [477/750], Loss: 0.0230\n",
      "Epoch [8/8], Step [478/750], Loss: 0.0025\n",
      "Epoch [8/8], Step [479/750], Loss: 0.0252\n",
      "Epoch [8/8], Step [480/750], Loss: 0.0051\n",
      "Epoch [8/8], Step [481/750], Loss: 0.0666\n",
      "Epoch [8/8], Step [482/750], Loss: 0.0188\n",
      "Epoch [8/8], Step [483/750], Loss: 0.0225\n",
      "Epoch [8/8], Step [484/750], Loss: 0.0537\n",
      "Epoch [8/8], Step [485/750], Loss: 0.0851\n",
      "Epoch [8/8], Step [486/750], Loss: 0.0263\n",
      "Epoch [8/8], Step [487/750], Loss: 0.0055\n",
      "Epoch [8/8], Step [488/750], Loss: 0.0571\n",
      "Epoch [8/8], Step [489/750], Loss: 0.1075\n",
      "Epoch [8/8], Step [490/750], Loss: 0.0720\n",
      "Epoch [8/8], Step [491/750], Loss: 0.0499\n",
      "Epoch [8/8], Step [492/750], Loss: 0.0290\n",
      "Epoch [8/8], Step [493/750], Loss: 0.0467\n",
      "Epoch [8/8], Step [494/750], Loss: 0.0076\n",
      "Epoch [8/8], Step [495/750], Loss: 0.0084\n",
      "Epoch [8/8], Step [496/750], Loss: 0.0096\n",
      "Epoch [8/8], Step [497/750], Loss: 0.0885\n",
      "Epoch [8/8], Step [498/750], Loss: 0.0980\n",
      "Epoch [8/8], Step [499/750], Loss: 0.0042\n",
      "Epoch [8/8], Step [500/750], Loss: 0.0102\n",
      "Epoch [8/8], Step [501/750], Loss: 0.0121\n",
      "Epoch [8/8], Step [502/750], Loss: 0.0313\n",
      "Epoch [8/8], Step [503/750], Loss: 0.0197\n",
      "Epoch [8/8], Step [504/750], Loss: 0.0010\n",
      "Epoch [8/8], Step [505/750], Loss: 0.0536\n",
      "Epoch [8/8], Step [506/750], Loss: 0.0103\n",
      "Epoch [8/8], Step [507/750], Loss: 0.0216\n",
      "Epoch [8/8], Step [508/750], Loss: 0.0425\n",
      "Epoch [8/8], Step [509/750], Loss: 0.0218\n",
      "Epoch [8/8], Step [510/750], Loss: 0.0301\n",
      "Epoch [8/8], Step [511/750], Loss: 0.0246\n",
      "Epoch [8/8], Step [512/750], Loss: 0.0380\n",
      "Epoch [8/8], Step [513/750], Loss: 0.0105\n",
      "Epoch [8/8], Step [514/750], Loss: 0.0232\n",
      "Epoch [8/8], Step [515/750], Loss: 0.0448\n",
      "Epoch [8/8], Step [516/750], Loss: 0.0360\n",
      "Epoch [8/8], Step [517/750], Loss: 0.0169\n",
      "Epoch [8/8], Step [518/750], Loss: 0.0174\n",
      "Epoch [8/8], Step [519/750], Loss: 0.0181\n",
      "Epoch [8/8], Step [520/750], Loss: 0.0267\n",
      "Epoch [8/8], Step [521/750], Loss: 0.0206\n",
      "Epoch [8/8], Step [522/750], Loss: 0.0692\n",
      "Epoch [8/8], Step [523/750], Loss: 0.0485\n",
      "Epoch [8/8], Step [524/750], Loss: 0.0038\n",
      "Epoch [8/8], Step [525/750], Loss: 0.0726\n",
      "Epoch [8/8], Step [526/750], Loss: 0.0169\n",
      "Epoch [8/8], Step [527/750], Loss: 0.0409\n",
      "Epoch [8/8], Step [528/750], Loss: 0.0352\n",
      "Epoch [8/8], Step [529/750], Loss: 0.0017\n",
      "Epoch [8/8], Step [530/750], Loss: 0.0059\n",
      "Epoch [8/8], Step [531/750], Loss: 0.0148\n",
      "Epoch [8/8], Step [532/750], Loss: 0.0029\n",
      "Epoch [8/8], Step [533/750], Loss: 0.0153\n",
      "Epoch [8/8], Step [534/750], Loss: 0.0757\n",
      "Epoch [8/8], Step [535/750], Loss: 0.0140\n",
      "Epoch [8/8], Step [536/750], Loss: 0.0582\n",
      "Epoch [8/8], Step [537/750], Loss: 0.0100\n",
      "Epoch [8/8], Step [538/750], Loss: 0.0064\n",
      "Epoch [8/8], Step [539/750], Loss: 0.0853\n",
      "Epoch [8/8], Step [540/750], Loss: 0.0086\n",
      "Epoch [8/8], Step [541/750], Loss: 0.0206\n",
      "Epoch [8/8], Step [542/750], Loss: 0.0120\n",
      "Epoch [8/8], Step [543/750], Loss: 0.0243\n",
      "Epoch [8/8], Step [544/750], Loss: 0.0299\n",
      "Epoch [8/8], Step [545/750], Loss: 0.0128\n",
      "Epoch [8/8], Step [546/750], Loss: 0.1334\n",
      "Epoch [8/8], Step [547/750], Loss: 0.0233\n",
      "Epoch [8/8], Step [548/750], Loss: 0.0038\n",
      "Epoch [8/8], Step [549/750], Loss: 0.1410\n",
      "Epoch [8/8], Step [550/750], Loss: 0.0344\n",
      "Epoch [8/8], Step [551/750], Loss: 0.0970\n",
      "Epoch [8/8], Step [552/750], Loss: 0.0020\n",
      "Epoch [8/8], Step [553/750], Loss: 0.0050\n",
      "Epoch [8/8], Step [554/750], Loss: 0.0090\n",
      "Epoch [8/8], Step [555/750], Loss: 0.0028\n",
      "Epoch [8/8], Step [556/750], Loss: 0.0009\n",
      "Epoch [8/8], Step [557/750], Loss: 0.0207\n",
      "Epoch [8/8], Step [558/750], Loss: 0.0011\n",
      "Epoch [8/8], Step [559/750], Loss: 0.0621\n",
      "Epoch [8/8], Step [560/750], Loss: 0.0109\n",
      "Epoch [8/8], Step [561/750], Loss: 0.0054\n",
      "Epoch [8/8], Step [562/750], Loss: 0.0194\n",
      "Epoch [8/8], Step [563/750], Loss: 0.0265\n",
      "Epoch [8/8], Step [564/750], Loss: 0.0193\n",
      "Epoch [8/8], Step [565/750], Loss: 0.0715\n",
      "Epoch [8/8], Step [566/750], Loss: 0.0124\n",
      "Epoch [8/8], Step [567/750], Loss: 0.0496\n",
      "Epoch [8/8], Step [568/750], Loss: 0.0165\n",
      "Epoch [8/8], Step [569/750], Loss: 0.0023\n",
      "Epoch [8/8], Step [570/750], Loss: 0.0012\n",
      "Epoch [8/8], Step [571/750], Loss: 0.0870\n",
      "Epoch [8/8], Step [572/750], Loss: 0.0008\n",
      "Epoch [8/8], Step [573/750], Loss: 0.0455\n",
      "Epoch [8/8], Step [574/750], Loss: 0.0106\n",
      "Epoch [8/8], Step [575/750], Loss: 0.0070\n",
      "Epoch [8/8], Step [576/750], Loss: 0.0616\n",
      "Epoch [8/8], Step [577/750], Loss: 0.0100\n",
      "Epoch [8/8], Step [578/750], Loss: 0.0893\n",
      "Epoch [8/8], Step [579/750], Loss: 0.0612\n",
      "Epoch [8/8], Step [580/750], Loss: 0.0027\n",
      "Epoch [8/8], Step [581/750], Loss: 0.1307\n",
      "Epoch [8/8], Step [582/750], Loss: 0.0798\n",
      "Epoch [8/8], Step [583/750], Loss: 0.0027\n",
      "Epoch [8/8], Step [584/750], Loss: 0.0124\n",
      "Epoch [8/8], Step [585/750], Loss: 0.0076\n",
      "Epoch [8/8], Step [586/750], Loss: 0.0651\n",
      "Epoch [8/8], Step [587/750], Loss: 0.0791\n",
      "Epoch [8/8], Step [588/750], Loss: 0.0017\n",
      "Epoch [8/8], Step [589/750], Loss: 0.0998\n",
      "Epoch [8/8], Step [590/750], Loss: 0.0433\n",
      "Epoch [8/8], Step [591/750], Loss: 0.0043\n",
      "Epoch [8/8], Step [592/750], Loss: 0.0322\n",
      "Epoch [8/8], Step [593/750], Loss: 0.0060\n",
      "Epoch [8/8], Step [594/750], Loss: 0.1902\n",
      "Epoch [8/8], Step [595/750], Loss: 0.0502\n",
      "Epoch [8/8], Step [596/750], Loss: 0.0562\n",
      "Epoch [8/8], Step [597/750], Loss: 0.0115\n",
      "Epoch [8/8], Step [598/750], Loss: 0.0035\n",
      "Epoch [8/8], Step [599/750], Loss: 0.0178\n",
      "Epoch [8/8], Step [600/750], Loss: 0.0031\n",
      "Epoch [8/8], Step [601/750], Loss: 0.0445\n",
      "Epoch [8/8], Step [602/750], Loss: 0.0564\n",
      "Epoch [8/8], Step [603/750], Loss: 0.2150\n",
      "Epoch [8/8], Step [604/750], Loss: 0.0036\n",
      "Epoch [8/8], Step [605/750], Loss: 0.0031\n",
      "Epoch [8/8], Step [606/750], Loss: 0.0103\n",
      "Epoch [8/8], Step [607/750], Loss: 0.0058\n",
      "Epoch [8/8], Step [608/750], Loss: 0.2001\n",
      "Epoch [8/8], Step [609/750], Loss: 0.0091\n",
      "Epoch [8/8], Step [610/750], Loss: 0.0599\n",
      "Epoch [8/8], Step [611/750], Loss: 0.0093\n",
      "Epoch [8/8], Step [612/750], Loss: 0.0101\n",
      "Epoch [8/8], Step [613/750], Loss: 0.0057\n",
      "Epoch [8/8], Step [614/750], Loss: 0.0037\n",
      "Epoch [8/8], Step [615/750], Loss: 0.0413\n",
      "Epoch [8/8], Step [616/750], Loss: 0.0033\n",
      "Epoch [8/8], Step [617/750], Loss: 0.0235\n",
      "Epoch [8/8], Step [618/750], Loss: 0.0423\n",
      "Epoch [8/8], Step [619/750], Loss: 0.0138\n",
      "Epoch [8/8], Step [620/750], Loss: 0.0251\n",
      "Epoch [8/8], Step [621/750], Loss: 0.0853\n",
      "Epoch [8/8], Step [622/750], Loss: 0.0122\n",
      "Epoch [8/8], Step [623/750], Loss: 0.0365\n",
      "Epoch [8/8], Step [624/750], Loss: 0.0111\n",
      "Epoch [8/8], Step [625/750], Loss: 0.0053\n",
      "Epoch [8/8], Step [626/750], Loss: 0.0441\n",
      "Epoch [8/8], Step [627/750], Loss: 0.0176\n",
      "Epoch [8/8], Step [628/750], Loss: 0.0058\n",
      "Epoch [8/8], Step [629/750], Loss: 0.0487\n",
      "Epoch [8/8], Step [630/750], Loss: 0.0180\n",
      "Epoch [8/8], Step [631/750], Loss: 0.0398\n",
      "Epoch [8/8], Step [632/750], Loss: 0.0447\n",
      "Epoch [8/8], Step [633/750], Loss: 0.0053\n",
      "Epoch [8/8], Step [634/750], Loss: 0.0011\n",
      "Epoch [8/8], Step [635/750], Loss: 0.0073\n",
      "Epoch [8/8], Step [636/750], Loss: 0.0099\n",
      "Epoch [8/8], Step [637/750], Loss: 0.0125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Step [638/750], Loss: 0.1323\n",
      "Epoch [8/8], Step [639/750], Loss: 0.0043\n",
      "Epoch [8/8], Step [640/750], Loss: 0.0007\n",
      "Epoch [8/8], Step [641/750], Loss: 0.0009\n",
      "Epoch [8/8], Step [642/750], Loss: 0.0131\n",
      "Epoch [8/8], Step [643/750], Loss: 0.0004\n",
      "Epoch [8/8], Step [644/750], Loss: 0.0018\n",
      "Epoch [8/8], Step [645/750], Loss: 0.0299\n",
      "Epoch [8/8], Step [646/750], Loss: 0.0372\n",
      "Epoch [8/8], Step [647/750], Loss: 0.0043\n",
      "Epoch [8/8], Step [648/750], Loss: 0.0293\n",
      "Epoch [8/8], Step [649/750], Loss: 0.0074\n",
      "Epoch [8/8], Step [650/750], Loss: 0.0100\n",
      "Epoch [8/8], Step [651/750], Loss: 0.0018\n",
      "Epoch [8/8], Step [652/750], Loss: 0.0061\n",
      "Epoch [8/8], Step [653/750], Loss: 0.0112\n",
      "Epoch [8/8], Step [654/750], Loss: 0.0445\n",
      "Epoch [8/8], Step [655/750], Loss: 0.0356\n",
      "Epoch [8/8], Step [656/750], Loss: 0.0023\n",
      "Epoch [8/8], Step [657/750], Loss: 0.0454\n",
      "Epoch [8/8], Step [658/750], Loss: 0.0013\n",
      "Epoch [8/8], Step [659/750], Loss: 0.0009\n",
      "Epoch [8/8], Step [660/750], Loss: 0.0357\n",
      "Epoch [8/8], Step [661/750], Loss: 0.0058\n",
      "Epoch [8/8], Step [662/750], Loss: 0.0059\n",
      "Epoch [8/8], Step [663/750], Loss: 0.0310\n",
      "Epoch [8/8], Step [664/750], Loss: 0.0131\n",
      "Epoch [8/8], Step [665/750], Loss: 0.0025\n",
      "Epoch [8/8], Step [666/750], Loss: 0.0638\n",
      "Epoch [8/8], Step [667/750], Loss: 0.0045\n",
      "Epoch [8/8], Step [668/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [669/750], Loss: 0.0112\n",
      "Epoch [8/8], Step [670/750], Loss: 0.0348\n",
      "Epoch [8/8], Step [671/750], Loss: 0.0021\n",
      "Epoch [8/8], Step [672/750], Loss: 0.0035\n",
      "Epoch [8/8], Step [673/750], Loss: 0.0089\n",
      "Epoch [8/8], Step [674/750], Loss: 0.0043\n",
      "Epoch [8/8], Step [675/750], Loss: 0.0253\n",
      "Epoch [8/8], Step [676/750], Loss: 0.0352\n",
      "Epoch [8/8], Step [677/750], Loss: 0.0341\n",
      "Epoch [8/8], Step [678/750], Loss: 0.0050\n",
      "Epoch [8/8], Step [679/750], Loss: 0.0168\n",
      "Epoch [8/8], Step [680/750], Loss: 0.0165\n",
      "Epoch [8/8], Step [681/750], Loss: 0.0010\n",
      "Epoch [8/8], Step [682/750], Loss: 0.0254\n",
      "Epoch [8/8], Step [683/750], Loss: 0.0047\n",
      "Epoch [8/8], Step [684/750], Loss: 0.0899\n",
      "Epoch [8/8], Step [685/750], Loss: 0.0032\n",
      "Epoch [8/8], Step [686/750], Loss: 0.0470\n",
      "Epoch [8/8], Step [687/750], Loss: 0.0185\n",
      "Epoch [8/8], Step [688/750], Loss: 0.0093\n",
      "Epoch [8/8], Step [689/750], Loss: 0.0915\n",
      "Epoch [8/8], Step [690/750], Loss: 0.0532\n",
      "Epoch [8/8], Step [691/750], Loss: 0.1987\n",
      "Epoch [8/8], Step [692/750], Loss: 0.0832\n",
      "Epoch [8/8], Step [693/750], Loss: 0.0018\n",
      "Epoch [8/8], Step [694/750], Loss: 0.0026\n",
      "Epoch [8/8], Step [695/750], Loss: 0.0027\n",
      "Epoch [8/8], Step [696/750], Loss: 0.0189\n",
      "Epoch [8/8], Step [697/750], Loss: 0.1299\n",
      "Epoch [8/8], Step [698/750], Loss: 0.0008\n",
      "Epoch [8/8], Step [699/750], Loss: 0.1029\n",
      "Epoch [8/8], Step [700/750], Loss: 0.0115\n",
      "Epoch [8/8], Step [701/750], Loss: 0.0121\n",
      "Epoch [8/8], Step [702/750], Loss: 0.0020\n",
      "Epoch [8/8], Step [703/750], Loss: 0.0340\n",
      "Epoch [8/8], Step [704/750], Loss: 0.0658\n",
      "Epoch [8/8], Step [705/750], Loss: 0.0115\n",
      "Epoch [8/8], Step [706/750], Loss: 0.0189\n",
      "Epoch [8/8], Step [707/750], Loss: 0.0273\n",
      "Epoch [8/8], Step [708/750], Loss: 0.0486\n",
      "Epoch [8/8], Step [709/750], Loss: 0.0880\n",
      "Epoch [8/8], Step [710/750], Loss: 0.0111\n",
      "Epoch [8/8], Step [711/750], Loss: 0.0080\n",
      "Epoch [8/8], Step [712/750], Loss: 0.0487\n",
      "Epoch [8/8], Step [713/750], Loss: 0.0914\n",
      "Epoch [8/8], Step [714/750], Loss: 0.0075\n",
      "Epoch [8/8], Step [715/750], Loss: 0.0012\n",
      "Epoch [8/8], Step [716/750], Loss: 0.0045\n",
      "Epoch [8/8], Step [717/750], Loss: 0.0290\n",
      "Epoch [8/8], Step [718/750], Loss: 0.0071\n",
      "Epoch [8/8], Step [719/750], Loss: 0.0880\n",
      "Epoch [8/8], Step [720/750], Loss: 0.0228\n",
      "Epoch [8/8], Step [721/750], Loss: 0.0059\n",
      "Epoch [8/8], Step [722/750], Loss: 0.0463\n",
      "Epoch [8/8], Step [723/750], Loss: 0.0243\n",
      "Epoch [8/8], Step [724/750], Loss: 0.0141\n",
      "Epoch [8/8], Step [725/750], Loss: 0.0267\n",
      "Epoch [8/8], Step [726/750], Loss: 0.0177\n",
      "Epoch [8/8], Step [727/750], Loss: 0.0078\n",
      "Epoch [8/8], Step [728/750], Loss: 0.0374\n",
      "Epoch [8/8], Step [729/750], Loss: 0.0104\n",
      "Epoch [8/8], Step [730/750], Loss: 0.0334\n",
      "Epoch [8/8], Step [731/750], Loss: 0.0038\n",
      "Epoch [8/8], Step [732/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [733/750], Loss: 0.0149\n",
      "Epoch [8/8], Step [734/750], Loss: 0.0034\n",
      "Epoch [8/8], Step [735/750], Loss: 0.0020\n",
      "Epoch [8/8], Step [736/750], Loss: 0.0019\n",
      "Epoch [8/8], Step [737/750], Loss: 0.0417\n",
      "Epoch [8/8], Step [738/750], Loss: 0.0544\n",
      "Epoch [8/8], Step [739/750], Loss: 0.0042\n",
      "Epoch [8/8], Step [740/750], Loss: 0.0190\n",
      "Epoch [8/8], Step [741/750], Loss: 0.0084\n",
      "Epoch [8/8], Step [742/750], Loss: 0.0052\n",
      "Epoch [8/8], Step [743/750], Loss: 0.0233\n",
      "Epoch [8/8], Step [744/750], Loss: 0.0312\n",
      "Epoch [8/8], Step [745/750], Loss: 0.0083\n",
      "Epoch [8/8], Step [746/750], Loss: 0.1237\n",
      "Epoch [8/8], Step [747/750], Loss: 0.0040\n",
      "Epoch [8/8], Step [748/750], Loss: 0.0070\n",
      "Epoch [8/8], Step [749/750], Loss: 0.0016\n",
      "Epoch [8/8], Step [750/750], Loss: 0.0324\n",
      "Epoch [8/8], Tr. loss: 0.4314. Test loss: 0.3808\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "\n",
    "history = train(num_epochs, hnn, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1ed953a",
   "metadata": {
    "id": "f1ed953a"
   },
   "outputs": [],
   "source": [
    "torch.save(hnn.state_dict(), 'trained_model_24qubits_3_layers_with_strong_entagling5qu.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b70614ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data5qu.txt', 'w') as outfile:\n",
    "    json.dump(history, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7a0c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_plot(history):\n",
    "    loss, val_loss = zip(*history)\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    for i in range(len(loss)):\n",
    "        losses.append(loss[i])\n",
    "        val_losses.append(val_loss[i])\n",
    "        for j in range(i):\n",
    "            losses[i] -= losses[j]\n",
    "            val_losses[i] -= val_losses[j]\n",
    "            \n",
    "    plt.figure(figsize=(15, 9))\n",
    "    plt.plot(losses, label=\"train_loss\")\n",
    "    plt.plot(val_losses, label=\"val_loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d12fa2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdd85606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        accuracy = 0\n",
    "        for i, (images, labels) in enumerate(loaders['test']):\n",
    "            print(i)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            test_output = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1]\n",
    "            accuracy += (pred_y == labels).sum().item() \n",
    "        accuracy /= len(loaders['test'].dataset)\n",
    "        print('Test Accuracy of the model on the 10000 test images: {:.2f} %'.format(accuracy*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d79b0354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Test Accuracy of the model on the 10000 test images: 99.11 %\n"
     ]
    }
   ],
   "source": [
    "test(hnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7518b56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction number: [8 4 3 9 4 6 2 4 2 0]\n",
      "Actual number: [8 4 3 9 4 6 2 4 2 0]\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(loaders['test']))\n",
    "images, labels = sample\n",
    "actual_number = labels[:10].numpy()\n",
    "actual_number\n",
    "\n",
    "test_output = hnn(images[:10].to(device))\n",
    "pred_y = torch.max(test_output, 1)[1].cpu().numpy()\n",
    "print(f'Prediction number: {pred_y}')\n",
    "print(f'Actual number: {actual_number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "968b420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-429642a251d7>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(test_output)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[6.7347e-09, 3.3360e-10, 2.2880e-10, 2.4755e-06, 6.5204e-07, 9.9999e-01,\n",
       "         1.7102e-06, 2.0303e-09, 1.7067e-07, 5.2945e-06],\n",
       "        [9.0944e-05, 6.7658e-03, 1.8291e-02, 8.8062e-03, 3.7265e-04, 3.6177e-05,\n",
       "         1.9353e-05, 9.6331e-01, 1.5673e-03, 7.4037e-04],\n",
       "        [2.0128e-06, 2.3713e-08, 1.2734e-05, 5.5169e-06, 6.0020e-05, 4.4002e-05,\n",
       "         3.7533e-07, 1.2752e-06, 1.1731e-06, 9.9987e-01],\n",
       "        [9.8988e-08, 9.9977e-01, 4.2399e-06, 4.1360e-08, 1.4053e-04, 3.9074e-05,\n",
       "         5.0901e-06, 3.5419e-05, 3.0632e-06, 3.1663e-06],\n",
       "        [9.9995e-01, 2.3188e-08, 3.1219e-06, 2.4693e-07, 8.0029e-07, 1.2359e-06,\n",
       "         1.8518e-05, 1.5103e-06, 1.8404e-06, 2.4621e-05],\n",
       "        [1.7271e-06, 9.9946e-01, 1.6503e-05, 5.6507e-08, 3.0275e-04, 1.2444e-04,\n",
       "         1.1543e-05, 6.7242e-05, 3.4002e-06, 8.7738e-06],\n",
       "        [9.9890e-01, 1.5705e-07, 1.1175e-05, 6.9385e-06, 6.5066e-05, 5.8295e-06,\n",
       "         5.5960e-05, 1.1560e-06, 5.1467e-06, 9.5117e-04],\n",
       "        [1.8027e-07, 2.8850e-06, 7.6717e-06, 1.1336e-05, 6.3133e-09, 2.5536e-06,\n",
       "         4.9802e-06, 1.0680e-06, 9.9997e-01, 3.7389e-06],\n",
       "        [3.7289e-06, 5.9976e-05, 5.0709e-04, 6.8511e-06, 4.6123e-05, 8.0636e-06,\n",
       "         6.9855e-07, 9.9921e-01, 4.9201e-06, 1.5578e-04],\n",
       "        [4.5513e-05, 7.2612e-08, 3.3442e-08, 6.9119e-08, 5.6977e-06, 1.5923e-06,\n",
       "         1.7610e-07, 1.1211e-06, 3.3129e-07, 9.9995e-01]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579ebba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fully_connected_with_qnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00cd50cc19624ebf946c288dcf96ca77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a0c3aa1494d454b85ce6083f2c98234": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ca85bfb648a4586ab15f601f45bd7c2",
      "placeholder": "",
      "style": "IPY_MODEL_979724284c094071b6c3143666a2a8d5",
      "value": " 0/10 [00:22&lt;?, ?it/s]"
     }
    },
    "0aa6d7245fc747b58457668a840cd66b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0de84b8c5dde45b68b6fe495e3978524": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1784bfcf80e54eb1a3b7873cd36494b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fb4ea8439054fbeaab29cbbdd8d8c62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2454c06e04cc44a59af6a54edf771dcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d31e57f44fef4cd99cbda543609a5363",
      "placeholder": "",
      "style": "IPY_MODEL_f1cb6810b0f0425c925092fd2c564460",
      "value": " 29696/? [00:00&lt;00:00, 779622.51it/s]"
     }
    },
    "2bae88a771624fce8a7195ab2d3790e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3629620f44dc4eec928e8285f4e292c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c55b6cc69fe4e71bfefdfafe172e1f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42c3a0dfb96d4d0a9cbcfbefdb88634a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b7381bd873134277869b9bf86f7f6cde",
       "IPY_MODEL_92a84a7a4c1a49fca9c649540174de1a",
       "IPY_MODEL_973a9057772b4a33bebe1c71b2548322"
      ],
      "layout": "IPY_MODEL_fb3c5d28cc434035936450f03eaab7d4"
     }
    },
    "4472460dcaf349e282e8505097ac56db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b9f9045a6a64724b75bed3bdf4825e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d29ed3d913f4c3dadd8aa28ffdf60ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fb4ea8439054fbeaab29cbbdd8d8c62",
      "placeholder": "",
      "style": "IPY_MODEL_7598f22b00ca4188abd0e988d87c8717",
      "value": ""
     }
    },
    "4ee572a2f86844b38f1475756d831d7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53264b907c324fd2903f6a33df4024eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3629620f44dc4eec928e8285f4e292c7",
      "placeholder": "",
      "style": "IPY_MODEL_ca79710ed42844eda53edd54ad1cce9e",
      "value": ""
     }
    },
    "585d33ea2f6c4c6f866ab5eaf500a486": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4472460dcaf349e282e8505097ac56db",
      "placeholder": "",
      "style": "IPY_MODEL_00cd50cc19624ebf946c288dcf96ca77",
      "value": " 5120/? [00:00&lt;00:00, 149206.45it/s]"
     }
    },
    "5f557bc2ab954fcdbb3145c5ae1cd1fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_831588c036b84ba5919552e1c0ffaf95",
      "placeholder": "",
      "style": "IPY_MODEL_4b9f9045a6a64724b75bed3bdf4825e5",
      "value": " 9913344/? [00:00&lt;00:00, 18038716.63it/s]"
     }
    },
    "611e4230f7284f56a36e03b6cab128bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "656d789dd50d4d7c95121ee911a0315e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "664e26cb5e91441d8360e652dc6f3f13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbeac195f2cf4776b6c95dedda39513c",
      "placeholder": "",
      "style": "IPY_MODEL_611e4230f7284f56a36e03b6cab128bb",
      "value": ""
     }
    },
    "70629c93466c490cb4d10c7ba2e41d62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7081d0216eb6473fae87f7331becfb53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7192858941cb47649969779dc0e63c91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_664e26cb5e91441d8360e652dc6f3f13",
       "IPY_MODEL_ba90ae57f0bf4101b5306b32401f5ca6",
       "IPY_MODEL_585d33ea2f6c4c6f866ab5eaf500a486"
      ],
      "layout": "IPY_MODEL_0de84b8c5dde45b68b6fe495e3978524"
     }
    },
    "739c6da6cd64446686e87fd499fc4e8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1784bfcf80e54eb1a3b7873cd36494b1",
      "max": 28881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae7d8308ff8541d082c143732f92bce6",
      "value": 28881
     }
    },
    "7598f22b00ca4188abd0e988d87c8717": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ca85bfb648a4586ab15f601f45bd7c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "831588c036b84ba5919552e1c0ffaf95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b99aefdd9a247678de2cefd312207bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7081d0216eb6473fae87f7331becfb53",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70629c93466c490cb4d10c7ba2e41d62",
      "value": 0
     }
    },
    "9002f4b1af6d4c03b8f53fecdffaa3b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92a84a7a4c1a49fca9c649540174de1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bae88a771624fce8a7195ab2d3790e1",
      "max": 1648877,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a5767d4185754f1f82e22dc55b9b659e",
      "value": 1648877
     }
    },
    "973a9057772b4a33bebe1c71b2548322": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bcaf3aab2df24777bb896394049144d2",
      "placeholder": "",
      "style": "IPY_MODEL_656d789dd50d4d7c95121ee911a0315e",
      "value": " 1649664/? [00:00&lt;00:00, 5358482.30it/s]"
     }
    },
    "979724284c094071b6c3143666a2a8d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a0d229daebc428d8d0bea6db980845f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2b522fb50d54cecaead8906ecc2c88f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2db74a18ce8412399aba11b3bab930b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa56b1689edb4a75aa4af965407467e6",
       "IPY_MODEL_8b99aefdd9a247678de2cefd312207bb",
       "IPY_MODEL_0a0c3aa1494d454b85ce6083f2c98234"
      ],
      "layout": "IPY_MODEL_9002f4b1af6d4c03b8f53fecdffaa3b0"
     }
    },
    "a5767d4185754f1f82e22dc55b9b659e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aa56b1689edb4a75aa4af965407467e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df312d8125694913844f2856ee295e67",
      "placeholder": "",
      "style": "IPY_MODEL_0aa6d7245fc747b58457668a840cd66b",
      "value": "  0%"
     }
    },
    "ae7d8308ff8541d082c143732f92bce6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b7381bd873134277869b9bf86f7f6cde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ee572a2f86844b38f1475756d831d7e",
      "placeholder": "",
      "style": "IPY_MODEL_a2b522fb50d54cecaead8906ecc2c88f",
      "value": ""
     }
    },
    "ba90ae57f0bf4101b5306b32401f5ca6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a0d229daebc428d8d0bea6db980845f",
      "max": 4542,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dc04c3799392479f84746241eac0c9bc",
      "value": 4542
     }
    },
    "bcaf3aab2df24777bb896394049144d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7dfb28ce1ec490bbbc71bbf8f8bc7bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d29ed3d913f4c3dadd8aa28ffdf60ab",
       "IPY_MODEL_739c6da6cd64446686e87fd499fc4e8c",
       "IPY_MODEL_2454c06e04cc44a59af6a54edf771dcf"
      ],
      "layout": "IPY_MODEL_fc29fb7181a64867aa764553a94327cf"
     }
    },
    "ca79710ed42844eda53edd54ad1cce9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d31e57f44fef4cd99cbda543609a5363": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8d21603da7146628c7025db753adeb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dbeac195f2cf4776b6c95dedda39513c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc04c3799392479f84746241eac0c9bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df312d8125694913844f2856ee295e67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df69f8986a7b46df869d78996f9a92bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_53264b907c324fd2903f6a33df4024eb",
       "IPY_MODEL_fe3e249b5f1947ec9fc496ab71992860",
       "IPY_MODEL_5f557bc2ab954fcdbb3145c5ae1cd1fe"
      ],
      "layout": "IPY_MODEL_3c55b6cc69fe4e71bfefdfafe172e1f4"
     }
    },
    "f1cb6810b0f0425c925092fd2c564460": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4bb69ed5dcf45cabca0294b04062d68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb3c5d28cc434035936450f03eaab7d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc29fb7181a64867aa764553a94327cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe3e249b5f1947ec9fc496ab71992860": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4bb69ed5dcf45cabca0294b04062d68",
      "max": 9912422,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d8d21603da7146628c7025db753adeb3",
      "value": 9912422
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
